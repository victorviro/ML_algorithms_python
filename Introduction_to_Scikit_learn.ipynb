{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Scikit-learn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3/ZJ+zTVXgYa8gWr3xg5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Machine-Learning-Python/blob/master/Introduction_to_Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UeEdsHCTv3k"
      },
      "source": [
        "# Introduction to Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SRVIV3LTwoD"
      },
      "source": [
        "[Scikit-learn](https://github.com/scikit-learn/scikit-learn) is a Python library that provides a standard interface for implementing state-of-the-art machine learning algorithms, as well as comprehensive [documentation](https://scikit-learn.org/stable/index.html) about each algorithm. It also includes other auxiliary functions that are integral to the machine learning pipeline such as data preprocessing steps, data resampling techniques, evaluation parameters, and search interfaces for tuning/optimizing an algorithm’s performance.\n",
        "\n",
        "This section will go through the functions for implementing a typical machine learning pipeline with Scikit-learn. Since Scikit-learn has a variety of packages and modules that are called depending on the use case, we’ll import a module directly from a package if and when needed using the `from` keyword. Again the goal of this notebook is to provide the foundation to be able to comb through the exhaustive Scikit-learn library and be able to use the right tool or function to get the job done.\n",
        "\n",
        "Scikit-learn comes with a set of small standard datasets that are ideal for learning purposes. However, these datasets are small and well-curated, they do not represent real-world scenarios.\n",
        "\n",
        "The [Iris plants dataset](https://scikit-learn.org/stable/datasets/index.html#iris-dataset) consists of 3 different types of plants (Setosa, Versicolour, and Virginica) and four features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFkh0c5OVDRp"
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBGBI1y8VHLn",
        "outputId": "69895f78-21fd-4a5b-e701-29c4ec6711f7"
      },
      "source": [
        "# Load iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "print(f'Keys of iris: {iris.keys()}')\n",
        "print(f'Iris dataset shape: {iris.data.shape}')\n",
        "print(f'Predictive features: {iris.feature_names}') "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys of iris: dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
            "Iris dataset shape: (150, 4)\n",
            "Predictive features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU9MkdRK1997"
      },
      "source": [
        "The data itself is contained in the `target` and `data` fields. `data` contains the numeric measurements of sepal length, sepal width, petal length, and petal width in a NumPy array. The `target` array contains the species of each of the flowers that were measured, also as a NumPy array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wex5DBpETxGm"
      },
      "source": [
        "## Splitting the dataset into training and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e778Pi6U5u6"
      },
      "source": [
        "We want to build a machine learning model from this data that can predict the species of iris for a new set of measurements. But before we can apply our model to new measurements, we need to know whether it actually works )whether we should trust its predictions). We cannot use the data we used to build the model to evaluate it because our model can always simply remember the whole training set, and will therefore always predict the correct label for any point in the training set. This *remembering* does not indicate to us whether our model will generalize well (whether it will also perform well on new data).\n",
        "\n",
        "To assess the model’s performance, we show it new data (data that it hasn’t seen before) for which we have labels. This is usually done by splitting the labeled data we have collected (here, our 150 flower measurements) into two parts. One part of the\n",
        "data, the training data, is used to build our machine learning model, and the rest of the data, the test data or hold-out set,  will be used to assess how well the model works.\n",
        "\n",
        "Scikit-learn has a convenient method to assist in that process called [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)`(X, y, test_size=0.25)`, where `X` is the design matrix or dataset of predictors and `y` is the target variable. The split size is controlled using the attribute `test_size`. By default, `test_size` is set to 25% of the dataset size. It is standard practice to shuffle the dataset before splitting by setting the attribute `shuffle=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-7zLlwwU-Vq",
        "outputId": "b0c86ad3-d2a0-4f15-c318-ba149a22e946"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, shuffle=True)\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (112, 4)\n",
            "X_test shape: (38, 4)\n",
            "y_train shape: (112,)\n",
            "y_test shape: (38,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DrXo-xF4PZc"
      },
      "source": [
        "Before making the split, the `train_test_split` function shuffles the dataset using a pseudorandom number generator. If we just took the last 25% of the data as a test set, all the data points would have the label 2 , as the data points are sorted by the label (see the output for `iris.target`). Using a test set containing only one of the three classes would not tell us much about how well our model generalizes,\n",
        "so we shuffle our data to make sure the test data contains data from all classes.\n",
        "\n",
        "To make sure that we will get the same output if we run the same function several times, we can provide the pseudorandom number generator with a fixed seed using the `random_state` parameter. This will make the outcome deterministic, so this line will always have the same outcome. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HXQ5dRwVLHQ"
      },
      "source": [
        "## Preprocessing the data for model fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vABSFPpZVM-4"
      },
      "source": [
        "Before a dataset is trained or fitted with a machine learning model, it necessarily undergoes some vital transformations. These transformations have a huge effect on the performance of the learning model. Transformations in Scikit-learn have a `fit()` and `transform()` method, or a `fit_transform()` method.\n",
        "\n",
        "Depending on the use case, the `fit()` method can be used to learn the parameters of the dataset, while the `transform()` method applies the data transform based on the learned parameters to the same dataset and also to the test or validation datasets before modeling. Also, the `fit_transform()` method can be used to learn and apply the transformation to the same dataset in a one-off fashion. Data transformation packages are found in the [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) package.\n",
        "\n",
        "This section will cover some critical transformation for numeric and categorical variables. They include:\n",
        "\n",
        "- Data rescaling\n",
        "\n",
        "- Encoding categorical variables\n",
        "\n",
        "- Input missing data\n",
        "\n",
        "- Generating higher-order polynomial features\n",
        "\n",
        "- Binning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tI5Cv1DWP1C"
      },
      "source": [
        "### Data rescaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaSh22hoWR_r"
      },
      "source": [
        "It is often the case that the features of the dataset contain data with different scales. In other words, the data in column A can be in the range of 1-5, while the data in column B is in the range of 1000-9000. This different scale for units of observations in the same dataset can have an adverse effect for certain machine learning models, especially when minimizing the cost function of the algorithm because it shrinks the function space and makes it difficult for an optimization algorithm like gradient descent to find the global minimum (see notebook [Introduction to gradient descent algorithm](https://github.com/victorviro/Machine-Learning-Python/blob/master/Introduction_gradient_descent_algorithm.ipynb)). Note that scaling the target values is generally not\n",
        "required.\n",
        "\n",
        "There are common ways to get all attributes to have the same scale: *min-max scaling*, *standardization* or *normalization*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sy9C5xlVRSX"
      },
      "source": [
        "#### Min-max scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkwQDAHVTEi"
      },
      "source": [
        "Min-max scaling is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. It is implemented in Scikit-learn using the [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) module. It has a `feature_range` hyperparameter that lets us change the range if we don’t want 0-1 for some reason. Let’s see an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvrF_BvTVVKQ",
        "outputId": "f9ef1c60-66be-474d-9a1f-61da5ba3b9bb"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "data = datasets.load_iris()\n",
        "\n",
        "# Separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Print first 5 rows of X before rescaling\n",
        "print(X[0:5,:])\n",
        "\n",
        "# Rescale X\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled_X = scaler.fit_transform(X)\n",
        "\n",
        "# Print first 5 rows of X after rescaling\n",
        "print(rescaled_X[0:5,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "[[0.22222222 0.625      0.06779661 0.04166667]\n",
            " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
            " [0.11111111 0.5        0.05084746 0.04166667]\n",
            " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
            " [0.19444444 0.66666667 0.06779661 0.04166667]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hSzUFk1VZ9t"
      },
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tig5_jwRTxDC"
      },
      "source": [
        "Standardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. For example, suppose a plant had a sepal lenght equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0-5 down to 0-0.05, whereas standardization would not be much affected. Scikit-Learn provides a transformer called [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for standardization. Let’s look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joEMerKvVeBH",
        "outputId": "e39b53c6-8648-4f02-f235-6f0de5031ec6"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load dataset\n",
        "data = datasets.load_iris()\n",
        "# Separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Print first 5 rows of X before standardization\n",
        "print(X[0:5,:])\n",
        "\n",
        "# Standardize X\n",
        "scaler = StandardScaler().fit(X)\n",
        "standardize_X = scaler.transform(X)\n",
        "\n",
        "# Print first 5 rows of X after standardization\n",
        "print(standardize_X[0:5,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
            " [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
            " [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
            " [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
            " [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzsGOKVfVg_m"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj_19dqoTxAR"
      },
      "source": [
        "Data normalization involves transforming the observations in the dataset so that it has a unit norm or has magnitude or length of 1. The length of a vector is the square root of the sum of squares of the vector elements. A unit vector (or unit norm) is obtained by dividing the vector by its length. Normalizing the dataset is particularly useful in scenarios where the dataset is sparse (i.e., a large number of observations are zeros) and also has differing scales. Normalization in Scikit-learn is implemented in the [`Normalizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXToIHEEVljr",
        "outputId": "d083ede6-1ba0-40ae-8f26-1effb094f296"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "# Load dataset\n",
        "data = datasets.load_iris()\n",
        "# Separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Print first 5 rows of X before standardization\n",
        "print(X[0:5,:])\n",
        "\n",
        "# Normalize X\n",
        "scaler = Normalizer().fit(X)\n",
        "normalize_X = scaler.transform(X)\n",
        "\n",
        "# Print first 5 rows of X after normalization\n",
        "print(normalize_X[0:5,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "[[0.80377277 0.55160877 0.22064351 0.0315205 ]\n",
            " [0.82813287 0.50702013 0.23660939 0.03380134]\n",
            " [0.80533308 0.54831188 0.2227517  0.03426949]\n",
            " [0.80003025 0.53915082 0.26087943 0.03478392]\n",
            " [0.790965   0.5694948  0.2214702  0.0316386 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIVjpXxgVuRk"
      },
      "source": [
        "### Encoding categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powD9FdUTw9p"
      },
      "source": [
        "Most machine learning algorithms do not compute with non-numerical or categorical variables. Hence, encoding categorical variables is the technique for converting non-numerical features with labels into a numerical representation for use in machine learning modeling (for more information about why we need to convert non-numerical features with labels into a numerical features check this [notebook](https://nbviewer.jupyter.org/github/victorviro/Deep_learning_python/blob/master/Text_Vectorization_NLP.ipynb)). Scikit-learn provides modules for encoding categorical variables including the [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) for encoding labels as integers, [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for converting categorical features into a matrix of integers, and [`LabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) for creating a one-hot encoding of target labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYKQyN-2X2d9"
      },
      "source": [
        "#### Label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlB2qkNOX499"
      },
      "source": [
        "`LabelEncoder` is typically used on the target variable to transform a vector of categories (or labels) into an integer representation by encoding label with values between 0 and the number of categories minus 1.\n",
        "\n",
        "Let’s see an example of `LabelEncoder`. (`OrdinalEncoder`?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smY6qVYiV0eP",
        "outputId": "952a3bb6-ea12-4d78-fbaa-e04c1702e280"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Create dataset\n",
        "data = np.array([[5,8,\"calabar\"],[9,3,\"uyo\"],[8,6,\"owerri\"],\n",
        "                 [0,5,\"uyo\"],[2,3,\"calabar\"],[0,8,\"calabar\"],\n",
        "                 [1,8,\"owerri\"]])\n",
        "print(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = data[:,:2]\n",
        "y = data[:,-1]\n",
        "\n",
        "# Encode y\n",
        "encoder = LabelEncoder()\n",
        "encode_y = encoder.fit_transform(y)\n",
        "\n",
        "# adjust dataset with encoded targets\n",
        "data[:,-1] = encode_y\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['5' '8' 'calabar']\n",
            " ['9' '3' 'uyo']\n",
            " ['8' '6' 'owerri']\n",
            " ['0' '5' 'uyo']\n",
            " ['2' '3' 'calabar']\n",
            " ['0' '8' 'calabar']\n",
            " ['1' '8' 'owerri']]\n",
            "[['5' '8' '0']\n",
            " ['9' '3' '2']\n",
            " ['8' '6' '1']\n",
            " ['0' '5' '2']\n",
            " ['2' '3' '0']\n",
            " ['0' '8' '0']\n",
            " ['1' '8' '1']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2GhdsK8YAa-"
      },
      "source": [
        "#### One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsXMIAc2V3LY"
      },
      "source": [
        "`OneHotEncoder` is used to transform a categorical feature variable in a matrix of integers. This matrix is a sparse matrix with each column corresponding to one possible value of a category. The new attributes are sometimes called *dummy* attributes.\n",
        "\n",
        "Let’s see an example of `OneHotEncoder`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQBx_VA2V7g0",
        "outputId": "df8436eb-6f60-4cfb-a2af-c0d888e1467b"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Create dataset\n",
        "data = np.array([[5,\"efik\", 8,\"calabar\"],\n",
        "                 [9,\"ibibio\",3,\"uyo\"],\n",
        "                 [8,\"igbo\",6,\"owerri\"],\n",
        "                 [0,\"ibibio\",5,\"uyo\"],\n",
        "                 [2,\"efik\",3,\"calabar\"],\n",
        "                 [0,\"efik\",8,\"calabar\"],\n",
        "                 [1,\"igbo\",8,\"owerri\"]])\n",
        "# Separate features and target\n",
        "X = data[:,:3]\n",
        "y = data[:,-1]\n",
        "# Print the feature or design matrix X\n",
        "print(X)\n",
        "\n",
        "# One_hot_encode X\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "encode_categorical = X[:,1].reshape(len(X[:,1]), 1)\n",
        "one_hot_encode_X = one_hot_encoder.fit_transform(encode_categorical)\n",
        "# Print one_hot encoded matrix (the output is a SciPy sparse matrix)\n",
        "# Call the toarray() method\n",
        "print(one_hot_encode_X.toarray())\n",
        "\n",
        "# Remove categorical label\n",
        "X = np.delete(X, 1, axis=1)\n",
        "# Append encoded matrix\n",
        "X = np.append(X, one_hot_encode_X.toarray(), axis=1)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['5' 'efik' '8']\n",
            " ['9' 'ibibio' '3']\n",
            " ['8' 'igbo' '6']\n",
            " ['0' 'ibibio' '5']\n",
            " ['2' 'efik' '3']\n",
            " ['0' 'efik' '8']\n",
            " ['1' 'igbo' '8']]\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "[['5' '8' '1.0' '0.0' '0.0']\n",
            " ['9' '3' '0.0' '1.0' '0.0']\n",
            " ['8' '6' '0.0' '0.0' '1.0']\n",
            " ['0' '5' '0.0' '1.0' '0.0']\n",
            " ['2' '3' '1.0' '0.0' '0.0']\n",
            " ['0' '8' '1.0' '0.0' '0.0']\n",
            " ['1' '8' '0.0' '0.0' '1.0']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLxXTSi36g5T"
      },
      "source": [
        "Alternatively, we can use the `get_dummies` method implemented by the Pandas package.\n",
        "\n",
        "**Note**: In this example, we get the dummy variables on a dataset containing both the training and the test data. This is important to ensure categorical values are represented in the same way in the training set and the test set.\n",
        "\n",
        "**Note**: Often, whether for ease of storage or because of the way the data is collected, categorical variables are encoded as integers. That they are numbers doesn’t mean that they should necessarily be treated as\n",
        "continuous features. It is not always clear whether an integer feature should be treated as continuous or discrete (and one-hot encoded). If there is no ordering between the semantics that are encoded, the feature must be treated as discrete. The `get_dummies` function in pandas treats all numbers as continuous and will not\n",
        "create dummy variables for them. To get around this, we can either use Scikit-learn ’s `OneHotEncoder`, for which we can specify which variables are continuous and which are discrete, or convert numeric columns in the DataFrame to strings (using `as_type('str')`) and use `get_dummies`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxKyJkluV_3O"
      },
      "source": [
        "### Input missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2YyU7zWV66v"
      },
      "source": [
        "It is often the case that a dataset contains several missing observations. We can accomplish treat easily using DataFrame’s `dropna()`, `drop()`, and `fillna()`. If we choose the last option using the mean, we should compute the mean value on the training set, and use it to fill the missing values in the training set, but also we don’t forget to save the mean value that we have computed since we will need it later to replace missing values in the test set when we want to evaluate our system, and also once the system goes live to replace missing values in new data.\n",
        "\n",
        "Scikit-learn implements the [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) method for completing missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldp1CHMVWDbh",
        "outputId": "19ab60fd-74cd-4eb4-cccc-fba6074848bc"
      },
      "source": [
        "from sklearn. impute import SimpleImputer\n",
        "# Create dataset\n",
        "data = np.array([[5,np.nan,8],[9,3,5],[8,6,4],\n",
        "                 [np.nan,5,2],[2,3,9],[np.nan,8,7],\n",
        "                 [1,np.nan,5]])\n",
        "print(data)\n",
        "\n",
        "# Impute missing values - axis=0: impute along columns\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
        "print(imputer.fit_transform(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5. nan  8.]\n",
            " [ 9.  3.  5.]\n",
            " [ 8.  6.  4.]\n",
            " [nan  5.  2.]\n",
            " [ 2.  3.  9.]\n",
            " [nan  8.  7.]\n",
            " [ 1. nan  5.]]\n",
            "[[5. 5. 8.]\n",
            " [9. 3. 5.]\n",
            " [8. 6. 4.]\n",
            " [5. 5. 2.]\n",
            " [2. 3. 9.]\n",
            " [5. 8. 7.]\n",
            " [1. 5. 5.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkf7OI-prprR"
      },
      "source": [
        "The `imputer` has simply computed the mean of each attribute and stored the result in its `statistics_` instance variable. Only the two attributes had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the `imputer` to all the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF16NOFZr7Zf",
        "outputId": "3e8858bf-4b17-4ccc-c5b2-89298c7f2b48"
      },
      "source": [
        "print(imputer.statistics_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.         5.         5.71428571]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n6zvN8JZOhh"
      },
      "source": [
        "**Note**: The `SimpleImputer` approach is a univariate imputation algorithm, that is, it imputes values in the $i^{\\text{th}}$ feature dimension using only non-missing values in that feature dimension. We can opt for multivariate imputation algorithms that use the entire set of available feature dimensions to estimate the missing values (e.g. [`IterativeImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)).\n",
        "\n",
        "\n",
        "We can use another strategy for filling in missing values. For example, the [`KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer) class provides imputation for filling in missing values using the k-Nearest Neighbors approach.\n",
        "\n",
        "See Scikit-learn guide [Imputation of missing values](https://scikit-learn.org/stable/modules/impute.html) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD140qBdWGfl"
      },
      "source": [
        "### Generating higher-order polynomial features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teu3DuCHTw6x"
      },
      "source": [
        "Scikit-learn has a module called [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) for generating a new dataset containing high-order polynomial and interaction features based off the features in the original dataset. For example, if the original dataset has two dimensions $[a, b]$, the second-degree polynomial transformation of the features will result in $[1, a, b, a^2, ab, b^2]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2x87MHsWKH2",
        "outputId": "3728363f-5ed6-4b03-ef5a-ad651e1118ea"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "# Create dataset\n",
        "data = np.array([[5,8],[9,3],[8,6],\n",
        "                 [5,2],[3,9],[8,7],\n",
        "                 [1,5]])\n",
        "print(data)\n",
        "\n",
        "# Create polynomial features\n",
        "polynomial_features = PolynomialFeatures(2)\n",
        "data = polynomial_features.fit_transform(data)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 8]\n",
            " [9 3]\n",
            " [8 6]\n",
            " [5 2]\n",
            " [3 9]\n",
            " [8 7]\n",
            " [1 5]]\n",
            "[[ 1.  5.  8. 25. 40. 64.]\n",
            " [ 1.  9.  3. 81. 27.  9.]\n",
            " [ 1.  8.  6. 64. 48. 36.]\n",
            " [ 1.  5.  2. 25. 10.  4.]\n",
            " [ 1.  3.  9.  9. 27. 81.]\n",
            " [ 1.  8.  7. 64. 56. 49.]\n",
            " [ 1.  1.  5.  1.  5. 25.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBbqUzkUbxYG"
      },
      "source": [
        "Adding polynomial features of the original data is a way to enrich a feature representation, particularly for linear models. \n",
        "\n",
        "This kind of feature engineering, as weel as adding interaction features, is often used in statistical modeling, but it’s also common in many practical machine learning applications. \n",
        "\n",
        "Using polynomial features together with a linear regression model yields the classical model of polynomial regression (check this [notebook](https://github.com/victorviro/Machine-Learning-Python/blob/master/Introduction_linear_regression_and_regularized_linear_models.ipynb) to see how apply polynomial regression). Adding interactions and polynomials can decrease performance slightly for other sort of machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03U_eXcI6U8g"
      },
      "source": [
        "### Binning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGQwpbRy6Zvg"
      },
      "source": [
        "One way to make linear models more powerful on continuous data is to use [*binning*](https://en.wikipedia.org/wiki/Data_binning) (also known as *discretization* or *bucketing*) of the feature to split it up into multiple features. We imagine a partition of the input range for the feature (for example, the numbers of a variable from -3 to 3) into a fixed number of bins (say, 10). A data point will then be represented by which bin it falls into. To determine this, we first have to define the bins. In this case, we’ll define 10 bins equally spaced between -3 and 3. We use the `np.linspace` function for this, creating 11 entries, which will create 10 bins (the spaces in between two consecutive boundaries):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTriOoiB6i77",
        "outputId": "01aee1d8-d808-4b13-b5ad-21b9b82f10b6"
      },
      "source": [
        "bins = np.linspace(-3, 3, 11)\n",
        "print(\"bins: {}\".format(bins))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bins: [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWbfhXXX6i7-"
      },
      "source": [
        "Here, the first bin contains all data points with feature values -3 to -2.4, the second bin contains all points with feature values from -2.4 to -1.8, and so on.\n",
        "\n",
        "Next, we record for each data point which bin it falls into. This can be easily computed using the `np.digitize` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTzKrc_m6i7_",
        "outputId": "46f53869-1c8e-4a9e-dd05-0fc959511165"
      },
      "source": [
        "X = np.array([-2.9,2.1,0.2,1,5,-2.1,1.88,2.55]).reshape((8, 1))\n",
        "print('X shape:', X.shape)\n",
        "which_bin = np.digitize(X, bins=bins)\n",
        "print(\"\\nData points:\\n\", X)\n",
        "print(\"\\nBin membership for data points:\\n\", which_bin)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (8, 1)\n",
            "\n",
            "Data points:\n",
            " [[-2.9 ]\n",
            " [ 2.1 ]\n",
            " [ 0.2 ]\n",
            " [ 1.  ]\n",
            " [ 5.  ]\n",
            " [-2.1 ]\n",
            " [ 1.88]\n",
            " [ 2.55]]\n",
            "\n",
            "Bin membership for data points:\n",
            " [[ 1]\n",
            " [ 9]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [11]\n",
            " [ 2]\n",
            " [ 9]\n",
            " [10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECb2VfhQ6i7_"
      },
      "source": [
        "What we did here is transform the single continuous input feature into a categorical feature that encodes which bin a data point is in. We can then transform this discrete feature to a one-hot encoding using the `OneHotEncoder` from the preprocessing module. Or we can use directly the [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html) class and set the `encode` attribute to `onehot`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ES1T6Bk6i7_",
        "outputId": "efd73c2d-fbc5-45d0-b9ba-8b3ba1ce9da8"
      },
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "est = KBinsDiscretizer(n_bins=10, encode='onehot', strategy='uniform')\n",
        "est.fit(X)\n",
        "X_binned = est.transform(X)\n",
        "X_binned.toarray()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DJicwwn6i7_"
      },
      "source": [
        "Because we specified 10 bins, the transformed dataset `X_binned` now is made up of 10 features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hvy2LT3maa7"
      },
      "source": [
        "### Other preprocessing techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9atg6bakWfFk"
      },
      "source": [
        "- *Binarization* is a transformation technique for converting a dataset into binary values by setting a cutoff or threshold. All values above the threshold are set to 1, while those below are set to 0. This technique is useful for converting a dataset of probabilities into integer values or in transforming a feature to reflect some categorization. Scikit-learn implements binarization with the [`Binarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html) module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw6vIk9tupNI"
      },
      "source": [
        "### Custom transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GvjE-ugjaw"
      },
      "source": [
        "Although Scikit-Learn provides many useful transformers, we may need to write our own for tasks such as custom cleanup operations or combining specific attributes. We will want our transformer to work seamlessly with Scikit-learn functionalities (such as `pipelines`), and since Scikit-learn relies on duck typing (not inheritance), all we need is to create a class and implement three methods: `fit()` (returning `self`), `transform()`, and `fit_transform()`. We can get the last one for free by simply adding `TransformerMixin` as a base class. Also, if we add `BaseEstimator` as a base class (and avoid `*args` and `**kargs` in our constructor) we will get two extra methods (`get_params()` and `set_params()`) that will be useful for automatic hyperparameter tuning. For example, here is a small transformer class that adds a combined attribute in the iris dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vZoruQmHVCm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "d442ffaf-a98a-4016-ace5-ec107404becf"
      },
      "source": [
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "X.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0                5.1               3.5                1.4               0.2\n",
              "1                4.9               3.0                1.4               0.2\n",
              "2                4.7               3.2                1.3               0.2\n",
              "3                4.6               3.1                1.5               0.2\n",
              "4                5.0               3.6                1.4               0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cNzKCBHhrTE"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Add a new variable: sepal length divided by sepal width\n",
        "class VariableRatioSepalAdder(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    # Define the transformation\n",
        "    def transform(self, X):\n",
        "        X_new = X.copy()\n",
        "        # Add a new variable\n",
        "        X_new[\"ratio_sepal_lengh_width\"] = X[\"sepal length (cm)\"]/X[\"sepal width (cm)\"]\n",
        "        return X_new"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf5K8VaAWNar"
      },
      "source": [
        "## Machine learning algorithms\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBKiar0QWPlO"
      },
      "source": [
        "A series of notebooks that show how machine learning algorithms work using the Scikit-learn library:\n",
        "\n",
        "- Supervised learning\n",
        " - [Introduction to linear regression and regularized linear models](https://github.com/victorviro/ML_algorithms_python/blob/master/Introduction_linear_regression_and_regularized_linear_models.ipynb)\n",
        " - [Logistic regression](https://github.com/victorviro/ML_algorithms_python/blob/master/Logistic_regression.ipynb)\n",
        " - [Discriminant analysis and Naive bayes](https://github.com/victorviro/ML_algorithms_python/blob/master/Gaussian_discriminant_analysis_and_Naive_bayes.ipynb)\n",
        " - [Decision Trees](https://github.com/victorviro/ML_algorithms_python/blob/master/Decision_Trees.ipynb)\n",
        " - [Ensemble learning](https://github.com/victorviro/ML_algorithms_python/blob/master/Ensemble_learning.ipynb)\n",
        " - [Support Vector Machines (SVM)](https://github.com/victorviro/ML_algorithms_python/blob/master/Support_Vector_Machines_explained.ipynb)\n",
        "\n",
        "- Unsupervised learning\n",
        " - [Dimesionality reduction (PCA)](https://github.com/victorviro/ML_algorithms_python/blob/master/Dimensionality_reduction_algorithms.ipynb)\n",
        " - [Clustering](https://github.com/victorviro/ML_algorithms_python/blob/master/Unsupervised_Learning_Techniques_Clustering.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DprGOSStTw1U"
      },
      "source": [
        "## Scikit-learn design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agnfEatXsY9d"
      },
      "source": [
        "Scikit-Learn’s API is remarkably well designed. The [main design principles](https://arxiv.org/abs/1309.0238) are:\n",
        "\n",
        "- **Consistency**. All objects share a consistent and simple interface:\n",
        " - *Estimators*. Any object that can estimate some parameters based on a dataset is called an *estimator* (e.g., an `imputer` is an estimator). The estimation itself is performed by the `fit()` method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as an `imputer strategy` ), and it must be set as an instance variable (generally via a constructor parameter).\n",
        "\n",
        " - *Transformers*. Some estimators (such as an `imputer`) can also transform a dataset; these are called *transformers*. Once again, the API is quite simple: the transformation is performed by the `transform()` method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for an `imputer` . All transformers also have a convenience method called `fit_transform()` that is equivalent to calling `fit()` and then `transform()` (but sometimes `fit_transform()` is optimized and runs much faster).\n",
        "\n",
        " - *Predictors*. Finally, some estimators are capable of making predictions given a dataset; they are called *predictors*. For example, the `LinearRegression` model. A predictor has a `predict()` method that takes a\n",
        "dataset of new instances and returns a dataset of corresponding predictions. It also has a `score()` method that measures the quality of the predictions given a test set (and the corresponding labels in the case of supervised learning algorithms).\n",
        "\n",
        "- **Inspection**. All the estimator’s hyperparameters are accessible directly via public instance variables (e.g., `imputer.strategy`), and all the estimator’s learned parameters are also accessible via public instance variables with an underscore suffix (e.g., `imputer.statistics_`).\n",
        "\n",
        "- **Nonproliferation of classes**. Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers.\n",
        "\n",
        "- **Composition**. Existing building blocks are reused as much as possible. For example, it is easy to create a `Pipeline` estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see.\n",
        "\n",
        "- **Sensible defaults**. Scikit-Learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK3fgDmsTwyk"
      },
      "source": [
        "## Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aefSnFA9TwwG"
      },
      "source": [
        "The machine learning process often combines a series of transformers on raw data, transforming the dataset each step of the way until it is passed to the fit method of a final estimator. But if we don’t transform our data in the same exact manner, we will end up with wrong or, at the very least, unintelligible results. The Scikit-Learn [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) object is the solution to this dilemma.\n",
        "\n",
        "\n",
        "Let’s look at how we can use the `Pipeline` class to express the workflow for training a SVC after scaling the data with MinMaxScaler.\n",
        "First, we build a pipeline object by providing it with a list of steps. Each step is a tuple containing a name (any string of our choosing) and an instance of an estimator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGgnB8uyH9dL"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Split in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, shuffle=True)\n",
        "\n",
        "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZP6FqBZIZac"
      },
      "source": [
        "Here, we created two steps: the first, called `\"scaler\"`, is an instance of `MinMaxScaler`, and the second, called `\"svm\"`, is an instance of `SVC`. Now, we can fit the pipeline, like any other scikit-learn estimator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwNn0BhwIH6Y",
        "outputId": "3a360767-4f7c-4a75-b963-6ff5f2371357"
      },
      "source": [
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
              "                ('svm',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPGPejlI1fq"
      },
      "source": [
        "Here, `pipe.fit` first calls `fit` on the first step (the scaler), then transforms the training data using the scaler, and finally fits the SVM with the scaled data. To evaluate on the test data, we simply call `pipe.score`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njjS9NPeJBdO",
        "outputId": "8ec2b15e-16cc-435d-da26-ef7f45b5fb00"
      },
      "source": [
        "print(f'Test score: {pipe.score(X_test, y_test)}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.9473684210526315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFQdwwfwJ7Qf"
      },
      "source": [
        "Calling the `score` method on the pipeline first transforms the test data using the scaler, and then calls the `score` method on the SVM using the scaled test data. Using the pipeline, we reduced the\n",
        "code needed for our \"preprocessing + classification\" process. The main benefit of using the pipeline, however, is that we can now use this single estimator in `cross_val_score` or `GridSearchCV`.\n",
        "\n",
        "The `Pipeline` class is not restricted to preprocessing and classification, but can in fact join any number of estimators together. For example, you could build a pipeline containing feature extraction, feature selection, scaling, and classification, for a total of four steps. Similarly, the last step could be regression or clustering instead of classification.\n",
        "\n",
        "The only requirement for estimators in a pipeline is that all but the last step need to have a `transform` method, so they can produce a new representation of the data that can be used in the next step.\n",
        "\n",
        "Internally, during the call to `Pipeline.fit`, the pipeline calls `fit` and then `transform` on each step in turn (or just `fit_transform`), with the input given by the output of the `transform method` of the previous step. For the last step in the pipeline, just `fit` is called.\n",
        "\n",
        "When predicting using `Pipeline`, we similarly transform the data using all but the last step and then call `predict` on the last step.\n",
        "\n",
        "The process is illustrated in Figure 6-3 for two transformers, `T1` and `T2` , and a classifier (called `Classifier`).\n",
        "\n",
        "![](https://i.ibb.co/wR13LzF/pipeline-training-and-prediction.png)\n",
        "\n",
        "The pipeline is actually even more general than this. There is no requirement for the last step in a pipeline to have a predict function, and we could create a pipeline just containing, for example, a scaler and `PCA`. Then, because the last step (`PCA`) has a `transform` method, we could call `transform` on the pipeline to get the output of\n",
        "`PCA.transform` applied to the data that was processed by the previous step. The last step of a pipeline is only required to have a `fit` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xox-eRhWulmc"
      },
      "source": [
        "**Accessing step attributes**: Often we will want to inspect attributes of one of the steps of the pipeline (the coefficients of a linear model or the components extracted by PCA). The easiest way to access the steps in a pipeline is via the `named_steps` attribute, which is a dictionary from the step names to the estimators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUwUPOzQuqQb",
        "outputId": "ea53d535-a18f-415b-921a-54d5865e1765"
      },
      "source": [
        "print(pipe.named_steps)\n",
        "\n",
        "# Extract the Regularization parameter from the \"svm\" step\n",
        "c = pipe.named_steps[\"svm\"].C\n",
        "print(f'\\nRegularization parameter: {c}')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'scaler': MinMaxScaler(copy=True, feature_range=(0, 1)), 'svm': SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)}\n",
            "\n",
            "Regularization parameter: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L75AIaaLMIE"
      },
      "source": [
        "### Convenient pipeline creation with `make_pipeline`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my_RYYHILNnM"
      },
      "source": [
        "Creating a pipeline using the syntax described earlier is sometimes a bit cumbersome, and we often don’t need user-specified names for each step. There is a convenience function, `make_pipeline`, that will create a pipeline for us and automatically name each step based on its class. The syntax for `make_pipeline` is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZHVGAVTLbUd"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "# Standard syntax\n",
        "pipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
        "# Abbreviated syntax\n",
        "pipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf_QqeQnLftm"
      },
      "source": [
        "The pipeline objects `pipe_long` and `pipe_short` do exactly the same thing, but `pipe_short` has steps that were automatically named. We can see the names of the steps by looking at the `steps` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM0aPRJFLjjB",
        "outputId": "c1be9ec4-1d7e-40d6-abe9-2247941eb7d0"
      },
      "source": [
        "print(f'Pipeline steps:\\n{pipe_short.steps}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline steps:\n",
            "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svc', SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_4Rq_jZLvVQ"
      },
      "source": [
        "The steps are named `minmaxscaler` and `svc`. In general, the step names are just lowercase versions of the class names. If multiple steps have the same class, a number is appended:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WYQbx26L5KB",
        "outputId": "47d2fe11-a75c-40a6-c27d-12630b1156d0"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "pipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\n",
        "print(f'Pipeline steps:\\n{pipe.steps}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline steps:\n",
            "[('standardscaler-1', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False)), ('standardscaler-2', StandardScaler(copy=True, with_mean=True, with_std=True))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7nXU8wnL_R5"
      },
      "source": [
        "However, in such settings it might be better to use the `Pipeline` construction with explicit names, to give more semantic names to each\n",
        "step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v14oyJIQTwlg"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd2Q2IHAGqfe"
      },
      "source": [
        "- [Scikit-learn documentation](https://scikit-learn.org/stable/index.html)\n",
        "\n",
        "- [Preprocessing data with Scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "\n",
        "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
        "\n",
        "- [Introduction to Machine Learning with Python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/)"
      ]
    }
  ]
}