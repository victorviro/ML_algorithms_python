{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5E0lKmMKc9v_",
        "Uz4XOQnlIJ_O",
        "dxQZfL2YS9u0",
        "h0kAWJ-JlYOq",
        "O6Cd0TldwKkQ",
        "nu2cJ13l3_Sm",
        "pkWDRz9l-in4",
        "MtzNKOUFLjYT"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFc33rvs8h3ZIHTmn5PM1E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Machine-Learning-Python/blob/master/Hyperparameter_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41uUPq-DSg13"
      },
      "source": [
        "# Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E0lKmMKc9v_"
      },
      "source": [
        "# Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOunBijSo16s"
      },
      "source": [
        "1. [Introduction](#1)\n",
        "2. [Hyperparameters are important](#2)\n",
        "3. [Tuning approaches](#3)\n",
        "    1. [Manual tuning](#3.1)\n",
        "    2. [Grid search](#3.2)\n",
        "    3. [Random search](#3.3)\n",
        "    4. [Model-based optimization](#3.4)\n",
        "    5. [More advanced techniques](#3.5)\n",
        "4. [Example](#4)\n",
        "    1. [Prepare the dataset](#4.1)\n",
        "    2. [Define function to minimize](#4.2)\n",
        "    3. [Define search space](#4.3)\n",
        "    4. [Select search algorithm](#4.4)\n",
        "    5. [Run the tuning algorithm](#4.5)\n",
        "    5. [Tuning with Hyperopt-sklearn](#4.6)\n",
        "5. [References](#5)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz4XOQnlIJ_O"
      },
      "source": [
        "# Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOFlSeNQSg5o"
      },
      "source": [
        "In machine learning, **hyperparameter optimization or tuning** is the problem of **choosing a set of optimal hyperparameters** for a learning algorithm. A hyperparameter is a parameter whose value is used to **control the learning process**. By contrast, the values of other parameters (typically node weights) are learned.\n",
        "\n",
        "In this notebook, we will see why the choice of the values for the hyperparameters is important, what options we have for tuning a model, and finally, we will see an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxQZfL2YS9u0"
      },
      "source": [
        "# Hyperparameters are important <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhKOtFNjTCVL"
      },
      "source": [
        "With all of the advanced feature engineering, ETL, visualizations work, and coding effort that is involved in building a ML solution, seeing an **untuned model using defaults** is like buying a high-performance sports car and filling it with regular gas. **It will run, but it will not perform well**. Not only will it under-perform, but the chances of it breaking are very high once we take it out into the \"real world\".\n",
        "\n",
        "While some algorithms have no hyperparameters, the majority have from a single to dozens of parameters that influence not only the core functionality of the algorithm's optimizer (e.g. the `family` parameter in generalized linear regression will directly influence the predictive performance of such a model) but how the optimizer executes its search to find the minimum objective function. The **most of these hyperparameters influence how the algorithm will \"learn\" an optimal fit to the data**, which is exceptionally important.\n",
        "\n",
        "The effect on models for different values of the hyperparameters is not only dependent on the algorithm type being used, but also in the nature of the data contained in the feature vector and the attributes of the target variable. This is why **every model needs to be tuned**.\n",
        "\n",
        "The next figure shows some simplified examples of two such\n",
        "critical hyperparameters for linear regression models.\n",
        "\n",
        "![](https://i.ibb.co/Sfv4wLQ/hyperparameter-opt.png)\n",
        "\n",
        "The effects of using defaults (left) or selecting a poor hyperparameter (using an aggressive learning rate, right) on a linear regressor that uses stochastic gradient descent as an optimizer for the solver. **Without properly tuning the model, the final result could be a model that is overfitting to a local minima or can be a model that fails to converge at all**. Either way, the predictions are going to be less than optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM1UP0_QTBEk"
      },
      "source": [
        "# Tuning approaches <a name=\"3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdd1Koled6SI"
      },
      "source": [
        "In the earlier stage of a experimentation phase, a manual process can be used to adjust and tune the model to get an idea about its predictive performance. However, depending on the use case, the difference between \"ok\" and \"very good\" predictions could translate to millions of dollars. **Spending time manually tuning** by just trying a bunch of different hyperparameters simply **won’t scale for both predictive performance and for timeliness of delivery**.\n",
        "\n",
        "If we want to come up with a better approach for tuning the model, we need to look at **what options there are**. The figure below shows the different approaches to tune models.\n",
        "\n",
        "![](https://i.ibb.co/sQh2hWd/model-tuning-approaches.png)\n",
        "\n",
        "As we can see, there are several options to pursue for arriving at the most optimal set of hyperparameters. The top section is typically how prototypes are built. Manually testing values of hyperparameters, when doing rapid testing, is an understandable approach. The goal of the prototype is getting an approximation of the tuneability of a solution. At the stage of moving towards a production solution, however, more maintainable and powerful solutions need to be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0kAWJ-JlYOq"
      },
      "source": [
        "## Manual tuning <a name=\"3.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtTvFdI9ldT2"
      },
      "source": [
        "We will see later, when going through the process of applying hyperopt to an example, **how difficult it would be to arrive at the optimal hyperparameters for the model**. **It’s very unlikely to get even close to optimal parameters with a manual process**. The amount of time required to arrive at a set of hyperparameters that gives an acceptably good answer could be weeks or months.\n",
        "\n",
        "**Another issue** with this method is in **tracking what has been tested**. Even if there was a system in place to record and ensure that the same values haven’t been tried before, the amount of work required is overwhelming, and prone to errors. Project work, after the rapid prototyping phase, should always abandon this approach to tuning as soon as is practicable. We have many better things to do with our time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Cd0TldwKkQ"
      },
      "source": [
        "## Grid Search <a name=\"3.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZpFoy9HwNIx"
      },
      "source": [
        "The **brute-force-search approach** of grid-based testing of hyperparameters has been around for quite some time. The process requires **selecting a set of values to test for each of the hyperparameters**. The grid search will then assemble **collections of hyperparameters to test by creating permutations of each of the values from each group** that has been specified.\n",
        "\n",
        "The **problem** with this approach is **when there is a large collection of hyperparameters to search through** (**and a large space** to search through mainly for those that are continuously distributed). The count of permutations that need to be tested can become overwhelming rather quickly. The trade-off is between the time required to run all of the permutations and the search capability of the optimization. If we want to explore more of the hyperparameter response surface, we’re going to have to run more iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu2cJ13l3_Sm"
      },
      "source": [
        "## Random Search <a name=\"3.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYPRewuD49zQ"
      },
      "source": [
        "**An alternative to Grid Search**, to simultaneously test the influencing effects of different hyperparameters at the same time (rather than relying on explicit permutations to determine the optimal values), **is using random sampling of each of the hyperparameter groups**. Instead of specifying exact conditions to test through permutation collections (as in Grid Search), this algorithm can **select candidates from each hyperparameter space and assemble, at random, candidate hyperparameter collections to test**. This approach, **controlled via a number of iterations**, can ensure that **large spaces are tested in a much shorter period of time**. The trade-off, however, is in the coverage of the hyperparameter space.\n",
        "\n",
        "In a paper by James Bergstra and Yoshua Bengio, [Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf), they concluded that grid search is flawed as an approach (since **some hyperparameters are far more influential in the overall quality of a particular trained model, those with greater effect get the same amount of \"coverage\" as those with negligible influence**, limiting the effective search due to computation time and cost of more expansive\n",
        "testing). Random Search is, in their opinion, a better approach than Grid Search, but it isn’t the most effective approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkWDRz9l-in4"
      },
      "source": [
        "## Model-based optimization. TPE (hyperopt) <a name=\"3.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRX0p-TL-kee"
      },
      "source": [
        "With a complex search space for hyperparameters, the approaches mentioned above are either too time consuming, expensive, or difficult to achieve adequate fit characteristics for validation against hold out data (all of them).\n",
        "\n",
        "The same team that brought the paper arguing that random search is a superior methodology to grid search also arrived at a **process for selecting an optimized hyperparameter response surface through the use of Bayesian techniques in a model-based optimization** relying on either Gaussian Processes or Tree of Parzen Estimators. The results of their research are provided in the open source python package [hyperopt](https://github.com/hyperopt/hyperopt).\n",
        "\n",
        "This technique is remarkably **capable of exploring complex hyperparameter spaces, but it can do so in far fewer iterations** than other methodologies. For further\n",
        "reading on this topic, the original paper is available,\n",
        "[Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtzNKOUFLjYT"
      },
      "source": [
        "## More advanced/complex techniques <a name=\"3.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7aZKWgbLkX8"
      },
      "source": [
        "More advanced techniques typically means paying a company that offers an **automated-ML (autoML)** solution or build your own.\n",
        "\n",
        "**Building a custom tuner** solution, we might look into a\n",
        "mixture of genetic algorithms with Bayesian prior search optimization to create search candidates within the hyperparameter space that have the highest likelihood of giving a good result, leveraging the selective optimization that genetic algorithms are known for.\n",
        "\n",
        "However, going down this path **is not usually recommended** unless we’re building out a custom framework for hundreds different projects and have a distinct need for a high-performance and lower cost optimization tool. **The effort otherwise is simply not worth it**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWatfifuSH7Q"
      },
      "source": [
        "# Example <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXTw72fG9Fp9"
      },
      "source": [
        "As we mentioned previously, [Hyperopt](https://github.com/hyperopt/hyperopt) is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions. It is designed for large-scale optimization for models with hundreds of parameters and allows the optimization procedure to be scaled across multiple cores and multiple machines.\n",
        "\n",
        "In this section, we are going to see an example using this library to find the best hyperparameters for a regression problem. We are going to use the [Housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html), which contains data representing characteristics of houses. The target variable is the median value of homes in $1000’s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LehJrleiKXH6"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B229J39gADXT"
      },
      "source": [
        "## Prepare the dataset <a name=\"4.1\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-skCdTIc-NdW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "308b123f-cb5a-4c60-ef7b-ab48daaacd9b"
      },
      "source": [
        "X, y = load_boston(return_X_y=True)\n",
        "X = pd.DataFrame(X)\n",
        "print(X.shape)\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2    3      4   ...   8      9     10      11    12\n",
              "0  0.00632  18.0  2.31  0.0  0.538  ...  1.0  296.0  15.3  396.90  4.98\n",
              "1  0.02731   0.0  7.07  0.0  0.469  ...  2.0  242.0  17.8  396.90  9.14\n",
              "2  0.02729   0.0  7.07  0.0  0.469  ...  2.0  242.0  17.8  392.83  4.03\n",
              "3  0.03237   0.0  2.18  0.0  0.458  ...  3.0  222.0  18.7  394.63  2.94\n",
              "4  0.06905   0.0  2.18  0.0  0.458  ...  3.0  222.0  18.7  396.90  5.33\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4IERwot_3Io"
      },
      "source": [
        "We split the data into into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxOVqez0_YWN"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LEJil91_1Ek"
      },
      "source": [
        "## Define function to minimize <a name=\"4.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK6aNCOLAFxC"
      },
      "source": [
        "In this example, we want to search for a regression linear model. We define a parameter `params['type']` as the model name. We define a function to run the training and return the mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp-xqWUMAP-2"
      },
      "source": [
        "def objective(params):\n",
        "    regressors_type = params['type']\n",
        "    del params['type']\n",
        "    if regressors_type == 'ElasticNet':\n",
        "        pipeline = Pipeline([('scaler', StandardScaler()),\n",
        "                             ('ElasticNet', ElasticNet(**params))]\n",
        "        )\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred_test = pipeline.predict(X_test)\n",
        "    else:\n",
        "        return 0    \n",
        "    loss = mean_squared_error(y_test, y_pred_test)\n",
        "    \n",
        "    return {'loss': loss, 'status': STATUS_OK}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J5PnHa8OhZw"
      },
      "source": [
        "## Define search space <a name=\"4.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlLFrHVOmWa"
      },
      "source": [
        "Details on defining a search space and parameter expressions are available in the [Hyperopt docs](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S3RmW7pLfAV"
      },
      "source": [
        "search_space = hp.choice('regressor_type', [\n",
        "    {\n",
        "        'type': 'ElasticNet',\n",
        "        'alpha': hp.lognormal('alpha', 0, 1.0),\n",
        "        'l1_ratio': hp.lognormal('l1_ratio', 0, 1.0)\n",
        "    },\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYd4WOsHOtLC"
      },
      "source": [
        "## Select search algorithm <a name=\"4.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqnC4I0ZOuPa"
      },
      "source": [
        "The two main choices are:\n",
        "\n",
        "- `hyperopt.tpe.suggest`: Tree of Parzen Estimators, a Bayesian approach which iteratively and adaptively selects new hyperparameter settings to explore based on past results.\n",
        "\n",
        "- `hyperopt.rand.suggest`: Random search, a non-adaptive approach which samples over the search space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0neKswgO-UL"
      },
      "source": [
        "algorithm = tpe.suggest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCw423ngO8Ps"
      },
      "source": [
        "## Run the tuning algorithm <a name=\"4.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ09g1sDPHdQ"
      },
      "source": [
        "We set `max_evals` to the maximum number of points in hyperparameter space to test, that is, the maximum number of models to fit and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCLoPKjRLurT"
      },
      "source": [
        "best_result = fmin(\n",
        "    fn=objective, \n",
        "    space=search_space,\n",
        "    algo=algorithm,\n",
        "    max_evals=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvmDH-yhL0VV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc639943-e409-4a27-ca9b-f4e7439f899e"
      },
      "source": [
        "print(f'best parameters: {best_result}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best parameters: {'alpha': 0.038833880217567576, 'l1_ratio': 1.7404673537285114, 'regressor_type': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIPdgG21Sv1h"
      },
      "source": [
        "**Note**: Our results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. We can run the example a few times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRs5-dOrQzMu"
      },
      "source": [
        "## Tuning with hyperopt-sklearn <a name=\"4.6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5d_0rG6Q5qr"
      },
      "source": [
        "[Hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn) is Hyperopt-based model selection among ML algorithms in Scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkTQSNt-RffE"
      },
      "source": [
        "!pip install git+https://github.com/hyperopt/hyperopt-sklearn.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwUXvd97Rcny"
      },
      "source": [
        "from hpsklearn import (HyperoptEstimator, any_regressor, any_preprocessing) \n",
        "from hyperopt import tpe\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzW5hEwWR4oS"
      },
      "source": [
        "We define the search procedure. We will explore all regressor algorithms and all data transforms available to the library and use the Tree of Parzen Estimators search algorithm.\n",
        "\n",
        "The search will evaluate 50 pipelines and limit each evaluation to 30 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6o1Gh28RaWi"
      },
      "source": [
        "# Define search\n",
        "hyperopt_estimator = HyperoptEstimator(regressor=any_regressor('reg'),\n",
        "                          preprocessing=any_preprocessing('pre'),\n",
        "                          loss_fn=mean_squared_error,\n",
        "                          algo=tpe.suggest, max_evals=50, trial_timeout=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtc0bOFDSgnh"
      },
      "source": [
        "We then start the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2RM6BFPShdJ"
      },
      "source": [
        "hyperopt_estimator.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igjn3hACRZrO"
      },
      "source": [
        "We can report the performance of the model on the holdout dataset and summarize the best performing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83t-L4gwTEle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecc83ee-efc2-420f-f255-1834bef662c5"
      },
      "source": [
        "y_test_predicted = hyperopt_estimator.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_test_predicted)\n",
        "print(\"MSE: %.3f\" % mse)\n",
        "# Summarize the best model\n",
        "print(f'Best model: {hyperopt_estimator.best_model()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: 9.826\n",
            "Best model: {'learner': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=41, n_jobs=1, oob_score=False,\n",
            "                      random_state=2, verbose=False, warm_start=False), 'preprocs': (), 'ex_preprocs': ()}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mReARVkEQSS8"
      },
      "source": [
        "# References <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHzlJfcrQVp_"
      },
      "source": [
        "- [Machine learning engineering in action](https://livebook.manning.com/book/machine-learning-engineering/)\n",
        "\n",
        "- [Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
        "\n",
        "- [Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)\n",
        "\n",
        "- [Hyperopt](https://github.com/hyperopt/hyperopt)\n",
        "\n",
        "- [Hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn)\n",
        "\n",
        "- [Hyperopt-Sklearn paper](https://www.ml4aad.org/wp-content/uploads/2018/07/automl_book_draft_hyperopt-sklearn.pdf)"
      ]
    }
  ]
}