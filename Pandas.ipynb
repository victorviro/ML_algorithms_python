{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pandas.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EYPtrdsJB_Jb",
        "My3ypIjS2qqL",
        "eSEGHQhvwGj3",
        "H_PLn-BIwGkF",
        "DuAUesO3wGkK",
        "i29dH0sdwGkO",
        "jVAoa7npwGkO",
        "wG7eLhGXwGkP",
        "OTCYd9SS279A",
        "JKTpDIOlI_VX",
        "2_K5uPeFJLDJ",
        "RLmSDI1WJf6v",
        "efxl5El1Jm9M",
        "RX86_5ABAZyE",
        "TeNBPD413cvb",
        "N8Idfofn3kr4",
        "01b7PQp-3sdX",
        "FBDjRY5gHyoI",
        "wrOQwe9dJp2h",
        "2cTeeDVtKAot",
        "vJJ4pVJz487H",
        "-6xqJkhf4baz",
        "1PYioOXO5RDw",
        "x8DPW6-P5kF_",
        "vpfNOGrhNSjc",
        "qWsaLCWb5uuB",
        "34aheEGqbLDS",
        "OTpMmLhebSM_",
        "4y67WO5Ybbnf",
        "Nf5A3JS3bgKH",
        "eYxSoc-DaRdG",
        "35JBLT5yadaU",
        "ov6Eu8J0YG2q",
        "3IHiZuJ2cTOE",
        "13nFVmdJczAj",
        "SE8ZV4B1ebFS",
        "15Xj1ZK8kSL9",
        "-qb-NbU2kYhN",
        "8aqxOzdSkeQS",
        "ZBBgnMPbkimC",
        "WXWiyoZYK0U5",
        "OrwYT-yKLV2s",
        "d7DCTU1Rqwct",
        "nYfIy2NTO0VF",
        "KqUcY7JbO0VP",
        "ywEVKdcUO0VS",
        "j9Jm0kKVO0VV",
        "6z6CvDotmaAy",
        "kszar-5aTg_H",
        "x08le47gTs_d",
        "1arV5dT5ViXg",
        "-b8ZxHJHWvuj"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPRLo1paqqSfuN1CD9n9+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Machine-Learning-Python/blob/master/Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofUSMCSt1vh6"
      },
      "source": [
        "# Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_erhQefI2df"
      },
      "source": [
        "1. [Introduction](#1)\n",
        "2. [Pandas data structures](#2)\n",
        "    1. [Series](#2.1)\n",
        "        1. [Populating a Series](#2.1.1)\n",
        "        2. [Attributes and methods](#2.1.2)\n",
        "        3. [Sorting a Series](#2.1.3)\n",
        "        4. [Counting values](#2.1.4)\n",
        "        5. [Invoking a function on a Series](#2.1.5)\n",
        "    2. [DataFrames](#2.2)\n",
        "        1. [Populating a DataFrame](#2.2.1)\n",
        "        2. [Attributes and methods](#2.2.2)\n",
        "        3. [Sorting a DataFrame](#2.2.3)\n",
        "        4. [Setting new index](#2.2.4)\n",
        "        5. [Rename/add/drop columns and rows](#2.2.5)\n",
        "3. [Data indexing](#3)\n",
        "    1. [Selecting columns from a DataFrame](#3.1)\n",
        "    2. [Selecting rows from a DataFrame](#3.2)\n",
        "4. [Filtering a DataFrame](#4)\n",
        "    1. [Filtering by a single condition](#4.1)\n",
        "    2. [Filtering by multiple conditions](#4.2)\n",
        "5. [Handling missing and duplicate data](#5)\n",
        "    1. [Identifying missing values](#5.1)\n",
        "    2. [Removing missing values](#5.2)\n",
        "    3. [Imputing missing values](#5.3)\n",
        "    4. [Dealing with duplicate data](#5.4)\n",
        "6. [Data aggregation (grouping)](#6)\n",
        "    1. [The GroupBy object](#6.1)\n",
        "    2. [Methods on a GroupBy object](#6.2)\n",
        "    3. [Applying custom operation to all groups](#6.3)\n",
        "    4. [Grouping by multiple columns](#6.4)\n",
        "7. [Working with dates and times](#7)\n",
        "    1. [Python's datetime](#7.1)\n",
        "    2. [Timestamp and Timedelta object](#7.2)\n",
        "    3. [The DatetimeProperties object](#7.3)\n",
        "    4. [Adding/Substracting durations of time](#7.4)\n",
        "8. [Working with text data](#8)\n",
        "    1. [String casing](#8.1)\n",
        "    2. [String slicing](#8.2)\n",
        "    3. [Boolean methods](#8.3)\n",
        "    4. [Splitting strings](#8.4)\n",
        "9. [Merging, joining and concatenating](#9)\n",
        "    1. [Concatenating DataFrames](#9.1)\n",
        "    2. [Joining/Merging](#9.2)\n",
        "10. [Importing/Exporting data](#10)\n",
        "    1. [JSON files](#10.1)\n",
        "    2. [CSV files](#10.2)\n",
        "    3. [Excel workbooks](#10.3)\n",
        "11. [Optimizing a dataset fro memory usage](#11)\n",
        "12. [MultiIndex DataFrames](#12)\n",
        "    1. [MultiIndex object](#12.1)\n",
        "    2. [Indexing with a MultiIndex](#12.2)\n",
        "    3. [Manipulating the index](#12.3)\n",
        "13. [References and further reading](#13)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYPtrdsJB_Jb"
      },
      "source": [
        "# Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZri9qSN1wgk"
      },
      "source": [
        "[Pandas](https://github.com/pandas-dev/pandas) is an open-source **Python library for data analysis** that enables us to manipulate and analyze complex **structured data sets**. It boasts easy-to-use functionality for reading and writing data, **dealing with missing data, reshaping the dataset**, and massaging the data by **slicing, indexing, inserting, and deleting data** variables and records. Pandas also has an important **groupBy** functionality for **aggregating data** for defined conditions, useful for plotting and computing data summaries for exploration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIW_vWTX2pHt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My3ypIjS2qqL"
      },
      "source": [
        "# Pandas Data Structures <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJMiklhf2t20"
      },
      "source": [
        "Just like NumPy, Pandas can store and manipulate a multi-dimensional array of data. To handle this, Pandas has two primary data structures: the **Series** and **DataFrame**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSEGHQhvwGj3"
      },
      "source": [
        "## Series <a name=\"2.1\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrZNS23QwGkF"
      },
      "source": [
        "The [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) data structure stores **1-D arrays of homogenous data**. The term \"homogenous\" means that the values are of the same data type. A Series contains an index which must have the same length as data. \n",
        "\n",
        "- The **index provides an identifier for each element**, similar to an index position in a list or to a key in a dictionary (an identifier or name connected to each value). \n",
        "- One feature that distinguishes a Series index from a list index or dictionary keys is that it **permits duplicates**, that is, elements in a Series (or a DataFrame) can be assigned the same indices. \n",
        "\n",
        "- By default, Series are created with an ascending integer index starting at 0, although, as we'll see, we can specify our own index. \n",
        "\n",
        "- Index labels can consist of **any immutable data type**.\n",
        "\n",
        "Multidimensional pandas data structures like the DataFrame are composed of one or more Series objects joined together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_PLn-BIwGkF"
      },
      "source": [
        "### Populating a Series <a name=\"2.1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXeY1BDswGkG"
      },
      "source": [
        "The `data` parameter in the Series constructor accepts a variety of inputs, including native Python objects like `list`, `dict` or `tuple` and other objects like numpy `ndarray`. Let's consider an example of creating a **Series** data structure and explore some basic attributes. When looking at a Series, Pandas displays the index on the left side and the values on the right.\n",
        "\n",
        "- The `values` attribute returns the numpy `ndarray` object that stores the values. The `to_list` method returns the values in a list. \n",
        "\n",
        "- The `index` attribute returns the `Index` object that the Series stores internally. When creating a Series, we can set our own index with the `index` parameter. If we define a Series, say, with an index of string labels, each of the values is assigned to a string label, but also to an index position. We'll explore the numerous ways to access Series elements by row or by label in the section \"Data indexing\".\n",
        "\n",
        "- The `size` attribute counts the number of values in the Series. The `shape` attribute returns a tuple of the dimensions of any pandas data structure. For a 1-dimensional object like the Series, the tuple's only value will be its size.\n",
        "\n",
        "- The`dtype` attribute reflects the data type of the values in the Series. If an explicit value is not passed to the `dtype` parameter, pandas will infer an appropriate data type. Pandas shows `dtype: object` for string values. For other data types, we'll typically see more precise outputs. This information is also available when looking at the Series.\n",
        "\n",
        "- We can retrieve the first and last elements of the Series using the methods `head()` and `tail()`. The `head` method returns one or more rows from the top of the dataset. It accepts an argument $n$ that represents the number of rows to extract. `tail` is the sibling method to `head`. It returns one or more rows from the end of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeWuoQ31wGkG",
        "outputId": "5e829c85-ce36-459b-f7ff-0e40178bc974"
      },
      "source": [
        "my_series = pd.Series(data=[2, 4, 6, 8])\n",
        "print(f'The Series:\\n{my_series}\\n')\n",
        "print(f'The Series as numpy array: {my_series.values}') \n",
        "print(f'The Series as list: {my_series.to_list()}') \n",
        "print(f'Index of the series: {my_series.index}')\n",
        "print(f'Number of values in the series: {my_series.size}')\n",
        "print(f'Dimensions of the series: {my_series.shape}')\n",
        "print(f'Data type of the Series values: {my_series.dtypes}')\n",
        "print(f'The first two elements of the Series:\\n{my_series.head(2)}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Series:\n",
            "0    2\n",
            "1    4\n",
            "2    6\n",
            "3    8\n",
            "dtype: int64\n",
            "\n",
            "The Series as numpy array: [2 4 6 8]\n",
            "The Series as list: [2, 4, 6, 8]\n",
            "Index of the series: RangeIndex(start=0, stop=4, step=1)\n",
            "Number of values in the series: 4\n",
            "Dimensions of the series: (4,)\n",
            "Data type of the Series values: int64\n",
            "The first two elements of the Series:\n",
            "0    2\n",
            "1    4\n",
            "dtype: int64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjXCLBYvZdxk"
      },
      "source": [
        "**Note**: The Series is a one-dimensional data structure that only supports a single \"column\" of data. Thus, if we attempt to pass a multi-dimensional list or ndarray to the constructor, pandas will raise an Exception."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuAUesO3wGkK"
      },
      "source": [
        "### Atributes and methods <a name=\"2.1.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NWibk_IwGkL"
      },
      "source": [
        "The Series object includes a lot of **methods and attributes for mathematical and statistical operations**. Many of these deal with missing values.\n",
        "\n",
        "- The `sum` method returns the sum of all of the values. Missing values are excluded. Most methods include a `skipna` parameter that can be set to `False` to include missing values. Because a null value cannot be added to any other, the return value will a null value. The `min_count` parameter sets a minimum number of non-null values that must be present for the sum to be calculated. The `product` method multiplies the Series values together. Like sum, it accepts `skipna` and `min_count` parameters.\n",
        "\n",
        "- The `cumsum` (cumulative sum) method returns a new Series with a rolling sum of values. If the parameter`skipna=False` here, the returned Series will have a cumulative sum up to the index with the first missing value, then `NaN` for the remaining values.\n",
        "\n",
        "- The `pct_change` method returns the percentage differences from one Series value to the next. The mathematical formula is equal to adding the current value and previous value, and then diving the result by the current value. It defaults to a \"forward fill\" solution for missing values. We can use the `fill_method` parameter to customize the logic that the `pct_change` method uses to substitute NaN values. This parameter is available for many methods (it's explained in the section \"Handling missing data\" when imputing missing values).\n",
        "\n",
        "- The `max` and `min` methods retrieve the largest and smallest value from the Series. If the Series consists of strings, the values will be alphabetically sorted. The \"largest\" value will be the one closest to the end of the alphabet and the \"smallest\" the one closest to the start of the alphabet. Alternatively, the Series object can be passed to Python's built-in `max` and `min` functions to arrive at the same results.\n",
        "\n",
        "- The `describe` method returns a Series of common statistical evaluations including count, mean, standard deviation, and more.\n",
        "\n",
        "- The `round` method rounds each value in a Series to the given number of decimals.\n",
        "\n",
        "- The `hasnans` attribute informs us if there are missing values in the values of the series. The index of the series also has this attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I41xPehmwGkL",
        "outputId": "04d7a97c-49cf-4560-8050-321ac56d5b9d"
      },
      "source": [
        "my_series = pd.Series([2, 4, 6, 8])\n",
        "print(f'Sum of the values of the Series: {my_series.sum()}')\n",
        "print(f'\\nMean of the Series: {my_series.mean()}')\n",
        "print(f'Maximum of the Series: {my_series.max()}')\n",
        "print(f'Summary statistics of the Series:\\n{my_series.describe()}\\n')\n",
        "print(f'Random sample of size 2 of the Series:\\n{my_series.sample(2)}\\n')\n",
        "print(f'Unique elements in the Series: {my_series.unique()}')\n",
        "print(f'Number of unique elements in the Series: {my_series.nunique()}')\n",
        "print(f'Are all elements in the Series unique?: {my_series.is_unique}')\n",
        "print(f'Are all elements in the Series increasing?: {my_series.is_monotonic}')\n",
        "print(f'Are missing values in the Series?: {my_series.hasnans}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum of the values of the Series: 20\n",
            "\n",
            "Mean of the Series: 5.0\n",
            "Maximum of the Series: 8\n",
            "Summary statistics of the Series:\n",
            "count    4.000000\n",
            "mean     5.000000\n",
            "std      2.581989\n",
            "min      2.000000\n",
            "25%      3.500000\n",
            "50%      5.000000\n",
            "75%      6.500000\n",
            "max      8.000000\n",
            "dtype: float64\n",
            "\n",
            "Random sample of size 2 of the Series:\n",
            "1    4\n",
            "2    6\n",
            "dtype: int64\n",
            "\n",
            "Unique elements in the Series: [2 4 6 8]\n",
            "Number of unique elements in the Series: 4\n",
            "Are all elements in the Series unique?: True\n",
            "Are all elements in the Series increasing?: True\n",
            "Are missing values in the Series?: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8iK46lawGkM"
      },
      "source": [
        "Standard **arithmetic operations** like addition, subtraction, multiplication, and division can be applied to every value in a Series. Any `NaN` values remain unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4ZRqQrfwGkM",
        "outputId": "16aaae0b-ffc4-49e8-8a85-984754ec730a"
      },
      "source": [
        "my_series = pd.Series(data = [5, np.nan, 15], index = [\"A\", \"B\", \"C\"])\n",
        "print(f'The Series as numpy array: {my_series.values}') \n",
        "print(f'The Series plus 3 as numpy array: {(my_series + 3).values}') \n",
        "print(f'The Series times by 2 as numpy array: {(my_series * 2).values}') \n",
        "print(f'The Series divided by 2 as numpy array: {(my_series / 2).values}') \n",
        "print(f'The Series divided (floor) by 4: {(my_series // 4).values}') \n",
        "print(f'The Series modulo 3 as numpy array: {(my_series % 3).values}') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Series as numpy array: [ 5. nan 15.]\n",
            "The Series plus 3 as numpy array: [ 8. nan 18.]\n",
            "The Series times by 2 as numpy array: [10. nan 30.]\n",
            "The Series divided by 2 as numpy array: [2.5 nan 7.5]\n",
            "The Series divided (floor) by 4: [ 1. nan  3.]\n",
            "The Series modulo 3 as numpy array: [ 2. nan  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtyU0ZAvwGkM"
      },
      "source": [
        "**Broadcasting**: Mathematical operations and some [comparison operators](https://docs.python.org/3/library/stdtypes.html#comparisons) can be applied across multiple Series objects. Generally, pandas will seek to align data values with a shared index label. In the example below, both Series have the same index. In the numpy documentation, [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) is defined as \"how numpy treats arrays with different shapes during arithmetic operations\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLBJpI1VwGkN",
        "outputId": "0f1562f0-97b0-4a31-ad33-cf8691319984"
      },
      "source": [
        "my_series1 = pd.Series([1, 2, 6], index = [\"A\", \"B\", \"C\"])\n",
        "my_series2 = pd.Series([4, 5, 6], index = [\"A\", \"B\", \"C\"])\n",
        "print(f'The two Series added as numpy array: {(my_series1 + my_series2).values}') \n",
        "print(f'The two Series are equal?: {(my_series1 == my_series2).values}') \n",
        "print(f'The Series 1 is less than Series 2?: {(my_series1 < my_series2).values}') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The two Series added as numpy array: [ 5  7 12]\n",
            "The two Series are equal?: [False False  True]\n",
            "The Series 1 is less than Series 2?: [ True  True False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYhcEdC16XfJ"
      },
      "source": [
        "**Note**: Operations between Series become trickier when there are differences between indices. One index may have more or fewer values than the other, or there may be a mismatch between the values themselves. When index labels are not shared between the Series, pandas returns `NaN` values.\n",
        "\n",
        "**Passing the Series to python's built-in functions**: Pandas objects integrate well with some of Python's built-in functions.\n",
        "\n",
        "- The `len` function returns the number of rows in a Series. `NaN` values will be included in the count.\n",
        "\n",
        "- The `type` function returns the class that an object is constructed from.\n",
        "\n",
        "- The `dir` (directory) function returns a list of the attributes and methods on an object. The names are listed as strings.\n",
        "\n",
        "- The `dict` function/class converts the Series into a dictionary. The index labels are used as the dictionary keys.\n",
        "\n",
        "- The `max` and `min` functions return the greatest and smallest value from the Series. If the Series consists of strings, `max` returns the last alphabetically sorted value and `min` returns the first one.\n",
        "\n",
        "- The `in` keyword returns `True` if the given value is found **in the index** labels of the Series. To check for inclusion among the values of the Series, we use the `in` keyword with the `ndarray` object returned from the `values` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5qkzmvwI46C",
        "outputId": "2f126f0a-647d-4eea-b3eb-54fb384798e2"
      },
      "source": [
        "cities = pd.Series(data = [\"San Francisco\", \"Los Angeles\", \"Las Vegas\"])\n",
        "print(f'Number of rows in the Series: {len(cities)}')\n",
        "print(f'Type of the Series object: {type(cities)}'))\n",
        "print(f'The Series as a dictionary: {dict(cities)}')\n",
        "print(f'Greatest value in the Series: {max(cities)}')\n",
        "print(f'Does the Series contain an index 2?: {2 in cities}')\n",
        "print(f'Does the Series contain a value \"Las Vegas\"?: {\"Las Vegas\" in cities.values}')\n",
        "print(f'Does not the Series contain a value \"Paris\"?: {\"Paris\" not in cities.values}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in the Series: 3\n",
            "Type of the Series object: <class 'pandas.core.series.Series'>\n",
            "Name of attributes and methods on the Series object: ['T', '_AXIS_LEN', '_AXIS_NAMES', '_AXIS_NUMBERS', '_AXIS_ORDERS', '_AXIS_REVERSED', '_AXIS_TO_AXIS_NUMBER', '_HANDLED_TYPES', '__abs__', '__add__', '__and__', '__annotations__', '__array__', '__array_priority__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__divmod__', '__doc__', '__eq__', '__finalize__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__imod__', '__imul__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_accessors', '_add_numeric_operations', '_add_series_or_dataframe_operations', '_agg_by_level', '_agg_examples_doc', '_agg_see_also_doc', '_aggregate', '_aggregate_multiple_funcs', '_align_frame', '_align_series', '_binop', '_builtin_table', '_can_hold_na', '_check_inplace_setting', '_check_is_chained_assignment_possible', '_check_label_or_level_ambiguity', '_check_setitem_copy', '_clear_item_cache', '_clip_with_one_bound', '_clip_with_scalar', '_consolidate', '_consolidate_inplace', '_construct_axes_dict', '_construct_axes_from_arguments', '_construct_result', '_constructor', '_constructor_expanddim', '_constructor_sliced', '_convert', '_convert_dtypes', '_cython_table', '_data', '_deprecations', '_dir_additions', '_dir_deletions', '_drop_axis', '_drop_labels_or_levels', '_find_valid_index', '_get_axis', '_get_axis_name', '_get_axis_number', '_get_axis_resolvers', '_get_block_manager_axis', '_get_bool_data', '_get_cacher', '_get_cleaned_column_resolvers', '_get_cython_func', '_get_index_resolvers', '_get_item_cache', '_get_label_or_level_values', '_get_numeric_data', '_get_value', '_get_values', '_get_values_tuple', '_get_with', '_gotitem', '_index', '_indexed_same', '_info_axis', '_info_axis_name', '_info_axis_number', '_init_dict', '_init_mgr', '_internal_names', '_internal_names_set', '_is_builtin_func', '_is_cached', '_is_copy', '_is_label_or_level_reference', '_is_label_reference', '_is_level_reference', '_is_mixed_type', '_is_view', '_iset_item', '_ix', '_ixs', '_map_values', '_maybe_cache_changed', '_maybe_update_cacher', '_metadata', '_needs_reindex_multi', '_obj_with_exclusions', '_protect_consolidate', '_reduce', '_reindex_axes', '_reindex_indexer', '_reindex_multi', '_reindex_with_indexers', '_repr_data_resource_', '_repr_latex_', '_reset_cache', '_reset_cacher', '_selected_obj', '_selection', '_selection_list', '_selection_name', '_set_as_cached', '_set_axis', '_set_axis_name', '_set_is_copy', '_set_item', '_set_labels', '_set_name', '_set_value', '_set_values', '_set_with', '_set_with_engine', '_slice', '_stat_axis', '_stat_axis_name', '_stat_axis_number', '_take_with_is_copy', '_to_dict_of_blocks', '_try_aggregate_string_function', '_typ', '_update_inplace', '_validate_dtype', '_values', '_where', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'append', 'apply', 'argmax', 'argmin', 'argsort', 'array', 'asfreq', 'asof', 'astype', 'at', 'at_time', 'attrs', 'autocorr', 'axes', 'backfill', 'between', 'between_time', 'bfill', 'bool', 'clip', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'divmod', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'dtype', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'ewm', 'expanding', 'explode', 'factorize', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'floordiv', 'ge', 'get', 'groupby', 'gt', 'hasnans', 'head', 'hist', 'iat', 'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'interpolate', 'is_monotonic', 'is_monotonic_decreasing', 'is_monotonic_increasing', 'is_unique', 'isin', 'isna', 'isnull', 'item', 'items', 'iteritems', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lt', 'mad', 'map', 'mask', 'max', 'mean', 'median', 'memory_usage', 'min', 'mod', 'mode', 'mul', 'multiply', 'name', 'nbytes', 'ndim', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'radd', 'rank', 'ravel', 'rdiv', 'rdivmod', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'repeat', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'searchsorted', 'sem', 'set_axis', 'shape', 'shift', 'size', 'skew', 'slice_shift', 'sort_index', 'sort_values', 'squeeze', 'std', 'str', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_frame', 'to_hdf', 'to_json', 'to_latex', 'to_list', 'to_markdown', 'to_numpy', 'to_period', 'to_pickle', 'to_sql', 'to_string', 'to_timestamp', 'to_xarray', 'transform', 'transpose', 'truediv', 'truncate', 'tz_convert', 'tz_localize', 'unique', 'unstack', 'update', 'value_counts', 'values', 'var', 'view', 'where', 'xs']\n",
            "The Series as a dictionary: {0: 'San Francisco', 1: 'Los Angeles', 2: 'Las Vegas'}\n",
            "Greatest value in the Series: San Francisco\n",
            "Smallest value in the Series: Las Vegas\n",
            "Does the Series contain an index 2?: True\n",
            "Does the Series contain a value \"Las Vegas\"?: True\n",
            "Does not the Series contain a value \"Paris\"?: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i29dH0sdwGkO"
      },
      "source": [
        "### Sorting a Series <a name=\"2.1.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TfwteZQGqRd"
      },
      "source": [
        "Both the values and the index of a Series can be sorted in ascending or descending order.\n",
        "\n",
        "**Sorting by values**\n",
        "\n",
        "The `sort_values` method returns a new Series with the values sorted in ascending order. The Series object that the method is invoked upon will not be modified (by default, we'll see how to change this with the `inplace` parameter later). \n",
        "\n",
        "- For a Series of strings, the values are sorted in alphabetical order. In pandas, as in Python, lowercase characters are sorted after uppercase characters.\n",
        "\n",
        "- The order of the sort is determined by the `ascending` parameter. To return a Series with values sorted in descending (or reverse alphabetical) order, we can set `ascending=False`.\n",
        "\n",
        "- By default, missing values will be placed at the end of a sorted Series, regardless of the `ascending` parameter. To display the missing values first, we set the parameter `na_position=\"first\"`. If we want to get rid of `NaN` values, we can use the `dropna` method (which returns a new Series with all missing values removed) before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biyuSv7bHBoM",
        "outputId": "75d4ab9f-de8d-4ed3-a560-b2abcc02cb31"
      },
      "source": [
        "my_series = pd.Series(data=[np.nan, 3, 9, 5])\n",
        "print('The Series\"s values sorted by values in ascending order: '\n",
        "      f'{my_series.sort_values().values}')\n",
        "print('The Series\"s values sorted by values in descending order: '\n",
        "      f'{my_series.sort_values(ascending=False).values}')\n",
        "print('The Series\"s values sorted by values in ascending order with missing '\n",
        "      f'values first: {my_series.sort_values(na_position = \"first\").values}')\n",
        "print('The Series\"s values sorted by values in ascending order with missing '\n",
        "      f'values removed: {my_series.dropna().sort_values().values}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Series\"s values sorted by values in ascending order: [ 3.  5.  9. nan]\n",
            "The Series\"s values sorted by values in descending order: [ 9.  5.  3. nan]\n",
            "The Series\"s values sorted by values in ascending order with missing values first: [nan  3.  5.  9.]\n",
            "The Series\"s values sorted by values in ascending order with missing values removed: [3. 5. 9.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIdjqSBzGyoN"
      },
      "source": [
        "**Sorting by index**\n",
        "\n",
        "The index of our Series can be sorted with the `sort_index` method. By default, it returns a new Series object, and the original Series object is not mutated or overwritten (we'll see how to change this with the `inplace` parameter next).\n",
        "\n",
        "- The `sort_index` method also supports the `ascending` and `na_position` parameters for modifying the sort order and the placement of `NaN` values. \n",
        "\n",
        "- When dealing with date-based data as indexes, an ascending sort will display them from earliest to latest. A descending sort will display the dates from latest to earliest. Notice that pandas uses another type of object, `NaT` (not a time), in place of missing date values. This is done to maintain integrity with the datetime data type of the index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-md7imnHdwH",
        "outputId": "15240d3b-9ee8-4169-97c7-4d315659f70a"
      },
      "source": [
        "from datetime import datetime\n",
        "my_series = pd.Series(\n",
        "    data=[1, 3, 9], \n",
        "    index=[datetime(2018, 1, 1), datetime(2019, 1, 1), np.nan]\n",
        ")\n",
        "print('The Series sorted by index in ascending order:\\n'\n",
        "      f'{my_series.sort_values()}\\n')\n",
        "print('The Series sorted by index in descending order with missing values first'\n",
        "      f':\\n{my_series.sort_values(ascending = False, na_position = \"first\")}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Series sorted by index in ascending order:\n",
            "2018-01-01    1\n",
            "2019-01-01    3\n",
            "NaT           9\n",
            "dtype: int64\n",
            "\n",
            "The Series sorted by index in descending order with missing values first:\n",
            "NaT           9\n",
            "2019-01-01    3\n",
            "2018-01-01    1\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5MOoxzaWIsD"
      },
      "source": [
        "**Overwriting a Series with the `inplace` parameter**\n",
        "\n",
        "The methods that we've invoked have returned new Series objects. The original Series objects referenced have remained unaffected. Many Series and DataFrame methods include an `inplace` parameter that expects a Boolean value. If we set `inplace=True`, the method will modify the original object that it is invoked upon rather than return a new object. The invocation of a method with `inplace=True` will not produce any output. It tweaks or modifies the existing object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXy0pLeQWP4f",
        "outputId": "054f20aa-1154-48c2-a03b-f609e96015e6"
      },
      "source": [
        "my_series = pd.Series(data=[1, 3, 9, 5])\n",
        "print(f'The series as numpy array: {my_series.values}')\n",
        "my_series.sort_values(inplace = True)\n",
        "print(f'The object Series as numpy array modified: {my_series.values}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The series as numpy array: [1 3 9 5]\n",
            "The object Series as numpy array modified: [1 3 5 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBbJY1iXHjCc"
      },
      "source": [
        "**Retrieve smallest and largest values**\n",
        "\n",
        "The `nlargest` method returns a new Series with the largest values from the original Series. The parameter `n` specifies the number of records to return; by default `n=5`. Values are sorted in descending order. The complementary `nsmallest` method returns the smallest values, sorted in ascending order. Both methods work only on Series with numeric values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIjDX0g7HjJ9",
        "outputId": "504fe174-f1c8-4168-8a5d-a5d3587a01a7"
      },
      "source": [
        "pd.Series(data=[1, 3, 9, 5]).nlargest(n = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    9\n",
              "3    5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVAoa7npwGkO"
      },
      "source": [
        "### Counting values <a name=\"2.1.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgIa3dEKwGkP"
      },
      "source": [
        "The [`value_counts`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) Series's method counts the number of occurrences of each unique value in a Series. The return value is a new Series object. The index labels of this new Series hold the unique values of the original Series and the values represent their respective counts. The length of the returned Series will be equal to the number of unique values from the original Series.\n",
        "\n",
        "- The Series returned is sorted by values in descending order. To sort the values in ascending order, we can set the parameter `ascending=True`.\n",
        "\n",
        "- The `normalize` parameter can be set to `True` to return the frequencies of each unique value. The frequency represents what portion of the dataset a given value makes up. The values in the resulting Series can be multiplied by 100 to return percentage values.\n",
        "\n",
        "- `NaN` values will be missing from the list by default. We can set `dropna=False` to count null values as a distinct category.\n",
        "\n",
        "- The index of a Series also supports the `value_counts` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX9qhb3PIRv5",
        "outputId": "fee5d8f5-5519-4406-97fd-8f7559d641e6"
      },
      "source": [
        "my_series = pd.Series(data=[3, 3, 9, 5])\n",
        "print('Number of occurrences of each unique value in the Series: '\n",
        "      f'\\n{my_series.value_counts()}\\n')\n",
        "len(my_series.value_counts()) == my_series.nunique()\n",
        "print('Frequencies in percentage of each unique value in the Series: '\n",
        "      f'\\n{my_series.value_counts(normalize = True) * 100}\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of occurrences of each unique value in the Series: \n",
            "3    2\n",
            "5    1\n",
            "9    1\n",
            "dtype: int64\n",
            "\n",
            "Frequencies in percentage of each unique value in the Series: \n",
            "3    50.0\n",
            "5    25.0\n",
            "9    25.0\n",
            "dtype: float64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GS_IT2zIq1j"
      },
      "source": [
        "To identity trends in **larger numeric Series**, it's usually better to group the values into predefined intervals. Let's create a Series with 1000 random numbers within the interval (0,1000). We want to group the values into buckets of 200, starting at 0 and working up to 1000. We can define these intervals as values in a list and pass it to the `bins` parameter.\n",
        "\n",
        "- The resulting Series is sorted by occurrences of unique values. If we want to sort by the intervals, we can invoke the `sort_index` method on the Series returned from the `value_counts` method. Alternatively, we can pass a value of `False` to the `sort` parameter of the `value_counts` method.\n",
        "\n",
        " Notice that the first interval includes the value `-0.001` instead of `0`. When Pandas organizes the Series's values into segments, it may extend the range of any segment up to .1% in either direction.\n",
        "\n",
        "- The `bins` parameter also accepts an integer argument. Pandas will find the difference between the maximum and minimum values in the Series and divide the range into the specified number of intervals. The bins/buckets may not be perfectly equal in size (due to the possible .1% extension of any interval) but will be fairly close."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arFh9G1WgcqP",
        "outputId": "016dff3a-966f-4a6b-8bfa-03ce287273db"
      },
      "source": [
        "my_series = pd.Series(data=np.random.randint(0, 1000, size=1000))\n",
        "bins = [0, 200, 400, 600, 800, 1000]\n",
        "print('Number of occurrences in each interval: '\n",
        "      f'\\n{my_series.value_counts(bins=bins)}\\n')\n",
        "# my_series.value_counts(bins = bins).sort_index()\n",
        "print('Number of occurrences in each interval sorted by the intervals: '\n",
        "      f'\\n{my_series.value_counts(bins=bins, sort=False)}\\n')\n",
        "print('Number of occurrences in each interval of 5 bins: '\n",
        "      f'\\n{my_series.value_counts(bins = 5, sort = False)}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of occurrences in each interval: \n",
            "(200.0, 400.0]     213\n",
            "(800.0, 1000.0]    206\n",
            "(400.0, 600.0]     203\n",
            "(600.0, 800.0]     195\n",
            "(-0.001, 200.0]    183\n",
            "dtype: int64\n",
            "\n",
            "Number of occurrences in each interval sorted by the intervals: \n",
            "(-0.001, 200.0]    183\n",
            "(200.0, 400.0]     213\n",
            "(400.0, 600.0]     203\n",
            "(600.0, 800.0]     195\n",
            "(800.0, 1000.0]    206\n",
            "dtype: int64\n",
            "\n",
            "Number of occurrences in each interval of 5 bins: \n",
            "(-1.0, 199.8]     180\n",
            "(199.8, 399.6]    215\n",
            "(399.6, 599.4]    203\n",
            "(599.4, 799.2]    192\n",
            "(799.2, 999.0]    210\n",
            "dtype: int64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nIyi4RLI4gz"
      },
      "source": [
        "**Note**: A representation of a sample index interval like `(-0.001, 200.0]` is an instance of pandas `Interval` class. A parenthesis before or after a value marks it as exclusive, or not included. A square bracket before or after a value marks it as inclusive or included in the interval. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7eLhGXwGkP"
      },
      "source": [
        "###  Invoking a function on a Series <a name=\"2.1.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je9DCM4mJigC"
      },
      "source": [
        "Suppose we want to compute the [levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between each value of a Series of strings and the word \"Luca\".\n",
        "\n",
        "The [`apply`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html) method on a Series invokes a function on every value within the Series. It returns a new Series consisting of the return values of those function invocations. The first argument apply expects is a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOUjQT8-sDtU",
        "outputId": "aaafde57-92ef-4ca9-af09-ce7b5789efa3"
      },
      "source": [
        "from nltk import edit_distance\n",
        "\n",
        "def compute_levenshtein_distance_to_luca(string):\n",
        "    return edit_distance(string, \"Luca\")\n",
        "\n",
        "my_series = pd.Series([\"Ituma\", \"Bobby\", \"Lua\", \"Buddy\"])\n",
        "my_series.apply(func=compute_levenshtein_distance_to_luca)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    3\n",
              "1    5\n",
              "2    1\n",
              "3    4\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTCYd9SS279A"
      },
      "source": [
        "## DataFrames <a name=\"2.2\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfb71U_k29yA"
      },
      "source": [
        "Structured data is made up of rows and columns: from the spreadsheet to relational database systems, structured data is an intuitive way to store information. A row represents a logical entity while columns represent things we know about each entity. Many kinds of data can be fit into this shape. \n",
        "\n",
        "A [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) is a Pandas data structure for **storing and manipulating 2-D arrays**. Like a spreadsheet or a database table, DataFrames are organized into **rows and columns**. However, we have a few additional terms: **indexes and axes**. \n",
        "\n",
        "The next figure displays the anatomy of a DataFrame. **Rows** are referred to as **\"axis 0\"** and **columns** are referred to as **\"axis 1\"**. The index provides an identifier for each row. By default, pandas DataFrames are created with an ascending integer index starting at 0, although, like with Series, we can specify our own index.\n",
        "\n",
        "![dask book page 46](https://i.ibb.co/FB2LSmk/dataframe-overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKTpDIOlI_VX"
      },
      "source": [
        "### Populating a DataFrame <a name=\"2.2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O95mbA9KAlcM"
      },
      "source": [
        "In a later section, we'll see how to load a DataFrame from different file types. In this section, we'll create pandas DataFrames with the `DataFrame` constructor. The `data` parameter in the DataFrame constructor accepts a variety of inputs, including native Python objects like `dict` and other objects like numpy `ndarray`.\n",
        "\n",
        "**Create a DataFrame from a dictionary**\n",
        "\n",
        "When instantiating a DataFrame from a dictionary, its keys will serve as the column names and the corresponding values will serve as that column's values. We pass the dictionary to the `data` parameter. The example below uses three equal-sized lists to store cities, countries, and populations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbHaB7HA2_Vf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "5452c19f-36b5-4c0c-f83e-7742a201a7c2"
      },
      "source": [
        "city_data = {\n",
        "    \"City\": [\"New York City\", \"Paris\", \"Barcelona\", \"Rome\"],\n",
        "    \"Country\": [\"United States\", \"France\", \"Spain\", \"Italy\"],\n",
        "    \"Population\": [8600000, 2141000, 5515000, 2873000]\n",
        "}\n",
        "cities = pd.DataFrame(data=city_data)\n",
        "cities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>City</th>\n",
              "      <th>Country</th>\n",
              "      <th>Population</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New York City</td>\n",
              "      <td>United States</td>\n",
              "      <td>8600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Paris</td>\n",
              "      <td>France</td>\n",
              "      <td>2141000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Barcelona</td>\n",
              "      <td>Spain</td>\n",
              "      <td>5515000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rome</td>\n",
              "      <td>Italy</td>\n",
              "      <td>2873000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            City        Country  Population\n",
              "0  New York City  United States     8600000\n",
              "1          Paris         France     2141000\n",
              "2      Barcelona          Spain     5515000\n",
              "3           Rome          Italy     2873000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923rbwqNAt4g"
      },
      "source": [
        "**Note**: The DataFrame class also includes a convenient `from_dict` class method, which saves Pandas some extra calculations. The `orient` parameter can be passed an argument of `\"index\"` to orient the headers as index labels.\n",
        "\n",
        "**Create a DataFrame from a numpy array**\n",
        "\n",
        "The DataFrame constructor also accepts a NumPy `ndarray` object. Let's create a 3x5 DataFrame of integers between 1 and 100. Next, we can pass it into the `DataFrame` constructor. \n",
        "\n",
        "- Just like with the rows, Pandas will assign each column a numeric index if a set of custom column headers is not provided. The `columns` parameter allows us to set the names of the columns in the DataFrame.\n",
        "\n",
        "- We can pass the `index` parameter an iterable sequence like a list, tuple, or ndarray to serve as the row labels. The length of the iterable must be equal to the number of rows in the dataset.\n",
        "\n",
        "- Both the row and column indices are allowed to contain duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v9LVrZvCH1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "afb20e9f-6bc3-4fcf-f7b5-178d13852712"
      },
      "source": [
        "data = np.random.randint(1, 101, [3, 5])\n",
        "index = [\"Morning\", \"Afternoon\", \"Evening\"]\n",
        "columns = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
        "temperatures = pd.DataFrame(data=data, index=index, columns=columns)\n",
        "temperatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Monday</th>\n",
              "      <th>Tuesday</th>\n",
              "      <th>Wednesday</th>\n",
              "      <th>Thursday</th>\n",
              "      <th>Friday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Morning</th>\n",
              "      <td>16</td>\n",
              "      <td>52</td>\n",
              "      <td>52</td>\n",
              "      <td>97</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Afternoon</th>\n",
              "      <td>70</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>99</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Evening</th>\n",
              "      <td>97</td>\n",
              "      <td>64</td>\n",
              "      <td>40</td>\n",
              "      <td>9</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Monday  Tuesday  Wednesday  Thursday  Friday\n",
              "Morning        16       52         52        97      81\n",
              "Afternoon      70       16          7        99      39\n",
              "Evening        97       64         40         9      46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_K5uPeFJLDJ"
      },
      "source": [
        "### Attributes and methods <a name=\"2.2.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwlyszEpqCWq"
      },
      "source": [
        "Many of the Series attributes and methods introduced previously are also available on a DataFrame. Although, the implementations are often different.\n",
        "\n",
        "**Attributes**\n",
        "\n",
        "- The values in a Series must be of a single homogenous data type. The columns in a DataFrame can hold heterogeneous data. The `dtypes` attribute on a DataFrame returns a Series object with the DataFrame 's columns and their respective data types. We can count the number of columns with each data type by invoking the `value_counts` method on the resulting Series.\n",
        "\n",
        "- The `index` attribute returns the `Index` object for a DataFrame. Like with Series, if we don't provide an index when creating the DataFrame, a `RangeIndex` will be created, which is optimized for storing numeric values that are in sequence.\n",
        "\n",
        "- We can access the NumPy ndarray holding the values through the DataFrame's `values` attribute.\n",
        "\n",
        "- The `columns` attribute returns an `Index` object containing the headers. Both the horizontal and vertical indices are collected in a list referenced by the `axes` attribute.\n",
        "\n",
        "- The `shape` attribute returns a tuple with the dimensions of the DataFrame. The `size` attribute calculates the total number of values in the dataset, including missing ones.\n",
        "\n",
        "- If we want the data flipped around, with our column headers serving as the index labels, we can either invoke the `transpose` method or access its `T` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9SzZnV0gJIJ",
        "outputId": "5ce5342e-42e8-426d-aee6-f2dd2af964b1"
      },
      "source": [
        "print(f'Datatype of the columns in the DataFrame:\\n{cities.dtypes}\\n')\n",
        "print('Number of columns per datatype in the DataFrame:\\n'\n",
        "      f'{cities.dtypes.value_counts()}\\n')\n",
        "print(f'Index of the DataFrame: {cities.index}')\n",
        "print(f'The DataFrame values as a numpy array:\\n{cities.values}\\n')\n",
        "print(f'Dimensions of the DataFrame (rows, columns): {cities.shape}')\n",
        "print(f'Number of elements in the DataFrame: {cities.size}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datatype of the columns in the DataFrame:\n",
            "City          object\n",
            "Country       object\n",
            "Population     int64\n",
            "dtype: object\n",
            "\n",
            "Number of columns per datatype in the DataFrame:\n",
            "object    2\n",
            "int64     1\n",
            "dtype: int64\n",
            "\n",
            "Index of the DataFrame: RangeIndex(start=0, stop=4, step=1)\n",
            "The DataFrame values as a numpy array:\n",
            "[['New York City' 'United States' 8600000]\n",
            " ['Paris' 'France' 2141000]\n",
            " ['Barcelona' 'Spain' 5515000]\n",
            " ['Rome' 'Italy' 2873000]]\n",
            "\n",
            "Dimensions of the DataFrame (rows, columns): (4, 3)\n",
            "Number of elements in the DataFrame: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtAgXjN1_zbe"
      },
      "source": [
        "**Methods**\n",
        "\n",
        "- We can extract any number of rows from the top or bottom of the dataset with the `head` and `tail` methods.\n",
        "\n",
        "- The `sample` method extracts several random rows from the DataFrame.\n",
        "\n",
        "- The `nunique` method returns a Series object with a count of unique values found in each column.\n",
        "\n",
        "- The `count` method returns a Series with the number of non-null values per DataFrame column.\n",
        "\n",
        "- On a DataFrame, the `max` and `min` methods will return a Series holding the maximum and minimum values of each column. The maximum value for a datetime column will be the latest date in chronological order.\n",
        "\n",
        "- The `nlargest` and `nsmallest` methods retrieve a subset of rows where given a column/s has the largest and smallest values in the dataset respectively. We use the `columns` parameter to specify which column(s) to use as the basis for sorting. The argument can be either a string or a list of strings representing column names.\n",
        "\n",
        "- We might want to calculate the sum of all values in a column. One strategy is to isolate the column Series and invoke the `sum` method on that. We can also invoke `sum` directly on the DataFrame. The `numeric_only` parameter can be used to target only columns with numeric values. Otherwise, pandas will return the sum of string columns as well (i.e. the concatenation).\n",
        "\n",
        "- We can find the average of columns with the `mean` method. Other statistical calculations like median and standard deviation are also available. They will automatically filter for only numeric columns.\n",
        "\n",
        "- The [`describe`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method computes and returns common statistical evaluations per each column. By default, it only includes numerical columns, but we can include or exclude data types with the `include` and `exclude` parameters.\n",
        "\n",
        "- The [`corr`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) method computes the pairwise correlation of the numerical columns in the DataFrame. Correlation shows how much a linear relationship exists between two numerical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crD20Lhsil9v",
        "outputId": "2bdfb0a4-45b6-4886-a4b0-66a5513467cb"
      },
      "source": [
        "print(f'Number of unique values per column of the DataFrame:\\n{cities.nunique()}\\n')\n",
        "print(f'Greatest values per column of the DataFrame:\\n{cities.max()}\\n')\n",
        "print('2 rows with largest population values of the DataFrame:\\n'\n",
        "      f'{cities.nlargest(n=2, columns=\"Population\")}\\n')\n",
        "print('Sum of the numeric columns in the DataFrame:\\n'\n",
        "      f'{cities.sum(numeric_only=True)}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique values per column of the DataFrame:\n",
            "City          4\n",
            "Country       4\n",
            "Population    4\n",
            "dtype: int64\n",
            "\n",
            "Greatest values per column of the DataFrame:\n",
            "City                   Rome\n",
            "Country       United States\n",
            "Population          8600000\n",
            "dtype: object\n",
            "\n",
            "2 rows with largest population values of the DataFrame:\n",
            "            City        Country  Population\n",
            "0  New York City  United States     8600000\n",
            "2      Barcelona          Spain     5515000\n",
            "\n",
            "Sum of the numeric columns in the DataFrame:\n",
            "Population    19129000\n",
            "dtype: int64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLmSDI1WJf6v"
      },
      "source": [
        "### Sorting a DataFrame <a name=\"2.2.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcsnvpSLJm6R"
      },
      "source": [
        "**Sort by columns**\n",
        "\n",
        "We can sort a DataFrame by one or more columns with the `sort_values` method. By default, the method returns a new DataFrame.\n",
        "\n",
        "- To sort by a column, we can pass a single string argument to the `by` parameter representing the column whose values we'd like to sort.\n",
        "\n",
        "- As with a Series, the `ascending` parameter is assigned by default to `True`. This will sort a column of strings in alphabetical order, a column of numbers in increasing order, and a column of datetimes in chronological order.\n",
        "\n",
        "- The `by` parameter also accepts a list of columns. The DataFrame's columns will be sorted in the order they are stored in the list. For example, we may want to sort by \"Country\" alphabetically, then sorts by city within each country. \n",
        "\n",
        " We can pass a single Boolean value to the `ascending` parameter to apply the same sort order to each column. Or we can sort each column with a different order by passing to the `ascending` parameter a list of Boolean values (the lists passed to the `by` and `ascending` parameters must be equal in length). For example, we might want to sort the countries in ascending order, then sort the cities within those countries in descending order.\n",
        "\n",
        "- As always, the `inplace` parameter mutates the original DataFrame instead of returning a copy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nex6-SJq_NI",
        "outputId": "e19939ad-831d-4c0a-d016-d221921e4248"
      },
      "source": [
        "print('Sorted DataFrame by \"Country\" in ascending order:\\n'\n",
        "      f'{cities.sort_values(by=\"Country\")}\\n')\n",
        "print('Sorted DataFrame by \"Country\" in descending order:\\n'\n",
        "      f'{cities.sort_values(by=\"Country\", ascending=False)}\\n')\n",
        "print('Sorted DataFrame by \"Country\" and \"City\" in ascending order:\\n'\n",
        "      f'{cities.sort_values(by=[\"Country\", \"City\"])}\\n')\n",
        "\n",
        "print('Sorted DataFrame by \"Country\" in ascending order and \"City\" in '\n",
        "      f'descending order:\\n'\n",
        "      f'{cities.sort_values(by=[\"Country\", \"City\"], ascending=[True,False])}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorted DataFrame by \"Country\" in ascending order:\n",
            "            City        Country  Population\n",
            "1          Paris         France     2141000\n",
            "3           Rome          Italy     2873000\n",
            "2      Barcelona          Spain     5515000\n",
            "0  New York City  United States     8600000\n",
            "\n",
            "Sorted DataFrame by \"Country\" in descending order:\n",
            "            City        Country  Population\n",
            "0  New York City  United States     8600000\n",
            "2      Barcelona          Spain     5515000\n",
            "3           Rome          Italy     2873000\n",
            "1          Paris         France     2141000\n",
            "\n",
            "Sorted DataFrame by \"Country\" and \"City\" in ascending order:\n",
            "            City        Country  Population\n",
            "1          Paris         France     2141000\n",
            "3           Rome          Italy     2873000\n",
            "2      Barcelona          Spain     5515000\n",
            "0  New York City  United States     8600000\n",
            "\n",
            "Sorted DataFrame by \"Country\" in ascending order and \"City\" in descending order:\n",
            "            City        Country  Population\n",
            "1          Paris         France     2141000\n",
            "3           Rome          Italy     2873000\n",
            "2      Barcelona          Spain     5515000\n",
            "0  New York City  United States     8600000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B282QzOHAC-i"
      },
      "source": [
        "**Sort by index**\n",
        "\n",
        "We have two indices by which we can sort. Each of them is considered an axis; the row index represents the horizontal axis while the column index represents the vertical axis.\n",
        "\n",
        "The `sort_index` method sorts a DataFrame by its row index values by default. \n",
        "\n",
        "- We can reverse the sort order by passing `False` to the `ascending` parameter. \n",
        "\n",
        "- To sort the columns in order, we can pass an argument of `1` or `\"columns\"` to the `axis` parameter.\n",
        "\n",
        "- We can also make any of these changes permanent with the `inplace` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyZQ-rNDrX4E",
        "outputId": "fb1dfad4-687a-4508-be8d-e3031d584139"
      },
      "source": [
        "print('Sorted DataFrame by row index in descending order:\\n'\n",
        "      f'{cities.sort_index(ascending=False)}\\n')\n",
        "print('Sorted DataFrame by columns in descending order:\\n'\n",
        "      f'{cities.sort_index(ascending=False, axis=1)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorted DataFrame by row index in descending order:\n",
            "            City        Country  Population\n",
            "3           Rome          Italy     2873000\n",
            "2      Barcelona          Spain     5515000\n",
            "1          Paris         France     2141000\n",
            "0  New York City  United States     8600000\n",
            "\n",
            "Sorted DataFrame by columns in descending order:\n",
            "   Population        Country           City\n",
            "0     8600000  United States  New York City\n",
            "1     2141000         France          Paris\n",
            "2     5515000          Spain      Barcelona\n",
            "3     2873000          Italy           Rome\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efxl5El1Jm9M"
      },
      "source": [
        "### Setting new index <a name=\"2.2.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8SF-OzjJpDW"
      },
      "source": [
        "The `set_index` method returns a new DataFrame with a given column serving as the index.\n",
        "\n",
        "- We can make the operation permanent by passing the `inplace` parameter to `True`.\n",
        "\n",
        "- If we know the column we'd like to use as the index when importing a dataset, we can also pass its name as a string to the `read_csv` method's `index_col` parameter.\n",
        "\n",
        "Unfortunately, invoking the `set_index` method we lose the current index. To preserve the current index, we need to first re-integrate the existing index as a regular column in our DataFrame. The `reset_index` method moves an existing index's values into a column and generates a fresh sequential index.\n",
        "\n",
        "- The `reset_index` method also accepts an `inplace` parameter. If the parameter is set to `True`, the method will not return a new DataFrame and thus the `set_index` method cannot be chained on in sequence as shown in the example below. We'll have to rely on two separate method calls in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQkaSwEHwqCa",
        "outputId": "73c05c8d-bf42-4dd3-afe8-b25be5ba7273"
      },
      "source": [
        "print('The DataFrame with \"Country\" as index:\\n'\n",
        "      f'{cities.set_index(\"Country\")}\\n')\n",
        "print('The DataFrame with the index as a column:\\n'\n",
        "      f'{cities.reset_index()}\\n')\n",
        "print('The DataFrame with \"Country\" as index and the original index as a column'\n",
        "      f':\\n{cities.reset_index().set_index(\"Country\")}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The DataFrame with \"Country\" as index:\n",
            "                        City  Population\n",
            "Country                                 \n",
            "United States  New York City     8600000\n",
            "France                 Paris     2141000\n",
            "Spain              Barcelona     5515000\n",
            "Italy                   Rome     2873000\n",
            "\n",
            "The DataFrame with the index as a column:\n",
            "   index           City        Country  Population\n",
            "0      0  New York City  United States     8600000\n",
            "1      1          Paris         France     2141000\n",
            "2      2      Barcelona          Spain     5515000\n",
            "3      3           Rome          Italy     2873000\n",
            "\n",
            "The DataFrame with \"Country\" as index and the original index as a column:\n",
            "               index           City  Population\n",
            "Country                                        \n",
            "United States      0  New York City     8600000\n",
            "France             1          Paris     2141000\n",
            "Spain              2      Barcelona     5515000\n",
            "Italy              3           Rome     2873000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX86_5ABAZyE"
      },
      "source": [
        "### Rename/add/drop columns and rows <a name=\"2.2.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cbS88iXAZyF"
      },
      "source": [
        "**Rename columns or row indices**\n",
        "\n",
        "We can rename the columns in the DataFrame by overwriting the `columns` attribute with a list of new names. The original DataFrame will be modified.\n",
        "\n",
        "The [`rename`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) method is an alternate option. We can pass its `columns` parameter a dictionary with keys representing the existing column names and values representing their new names. \n",
        "\n",
        "- The `rename` method returns a new DataFrame with the new column names and it won't altere the original DataFrame. The `inplace` parameter mutates the original DataFrame instead of returning a copy.\n",
        "\n",
        "- The `rename` method can rename index labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks9LIuDFue3K",
        "outputId": "418ee97e-7db5-4df7-b0c4-ba2d275715ab"
      },
      "source": [
        "cities.columns = [\"city\", \"country\", \"population\"]\n",
        "print(f'The DataFrame with renamed columns:\\n {cities}\\n')\n",
        "print('The DataFrame with renamed columns:\\n'\n",
        "      f'{cities.rename(columns={\"city\": \"twon\"})}\\n')\n",
        "print('The DataFrame with index 0 set to 1:\\n'\n",
        "      f'{cities.rename(index={0: 1})}\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The DataFrame with renamed columns:\n",
            "            city        country  population\n",
            "0  New York City  United States     8600000\n",
            "1          Paris         France     2141000\n",
            "2      Barcelona          Spain     5515000\n",
            "3           Rome          Italy     2873000\n",
            "\n",
            "The DataFrame with renamed columns:\n",
            "            twon        country  population\n",
            "0  New York City  United States     8600000\n",
            "1          Paris         France     2141000\n",
            "2      Barcelona          Spain     5515000\n",
            "3           Rome          Italy     2873000\n",
            "\n",
            "The DataFrame with index 0 set to 1:\n",
            "            city        country  population\n",
            "1  New York City  United States     8600000\n",
            "1          Paris         France     2141000\n",
            "2      Barcelona          Spain     5515000\n",
            "3           Rome          Italy     2873000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T85PFeDI6WSB"
      },
      "source": [
        "**Removing a Row/Column**\n",
        "\n",
        "In many cases, during the data cleaning process, we want to drop rows or columns of a DataFrame. We can use the [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method. \n",
        "\n",
        "- The `labels` parameter specifies the index or column labels to drop. It can be a single label or a list of labels.\n",
        "\n",
        "- With the `axis` parameter we can indicate whether we want to drop rows (`0` or `index`) or columns (`1` or `columns`). By default `axis=0`. \n",
        "\n",
        "- Alternatively to the `labels` and `axis` parameters, we can use the `index` or `columns` parameters to specify the labels we want to drop.\n",
        "\n",
        "- When a column or row is dropped, a new DataFrame or Series is returned without altering the original data structure. If the attribute `inplace` is set to True, the original DataFrame or Series is modified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZj2t0cq6WSD",
        "outputId": "51a66e74-c13a-484f-c394-643fd881be38"
      },
      "source": [
        "print('DataFrame with rows at index labels 1 and 3 dropped:\\n'\n",
        "      f'{cities.drop(labels=[1,3])}\\n')\n",
        "print('DataFrame with column \"population\" dropped:\\n'\n",
        "      f'{cities.drop(labels=\"population\", axis=1)}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame with rows at index labels 1 and 3 dropped:\n",
            "            city        country  population language\n",
            "0  New York City  United States     8600000  English\n",
            "2      Barcelona          Spain     5515000  Spanish\n",
            "\n",
            "DataFrame with column \"population\" dropped:\n",
            "            city        country language\n",
            "0  New York City  United States  English\n",
            "1          Paris         France   French\n",
            "2      Barcelona          Spain  Spanish\n",
            "3           Rome          Italy  Italian\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabaAR7w60TI"
      },
      "source": [
        "**Adding a Row/Column**\n",
        "\n",
        "We can add a new column to a Pandas DataFrame in different ways. Assuming that the column we want to add to the DataFrame is stored in a Series, and its index values match those in the DataFrame:\n",
        "\n",
        "- By using `df['column_name'] = column_series.values`. The original DataFrame will be modified.\n",
        "\n",
        "- By using the [`assign`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html) method we can add one or more columns. The `assign` method returns a new DataFrame with the new columns added and it won't modify the original DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT_V1AsN60TT",
        "outputId": "65e24aa2-e51f-4522-b0dc-b60517237023"
      },
      "source": [
        "language = pd.Series([\"English\", \"French\", \"Spanish\", \"Italian\"])\n",
        "cities[\"language\"] = language.values\n",
        "print(f'DataFrame with column \"language\" added:\\n {cities}\\n')\n",
        "continent = pd.Series([\"America\", \"Europe\", \"Europe\", \"Europe\"])\n",
        "print('DataFrame with columns \"language\" and \"continent\" added:\\n'\n",
        "      f'{cities.assign(language=language.values, continent=continent.values)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame with column \"language\" added:\n",
            "             city        country  population language\n",
            "0  New York City  United States     8600000  English\n",
            "1          Paris         France     2141000   French\n",
            "2      Barcelona          Spain     5515000  Spanish\n",
            "3           Rome          Italy     2873000  Italian\n",
            "\n",
            "DataFrame with columns \"language\" and \"continent\" added:\n",
            "            city        country  population language continent\n",
            "0  New York City  United States     8600000  English   America\n",
            "1          Paris         France     2141000   French    Europe\n",
            "2      Barcelona          Spain     5515000  Spanish    Europe\n",
            "3           Rome          Italy     2873000  Italian    Europe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeNBPD413cvb"
      },
      "source": [
        "# Data Indexing <a name=\"3\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnDyVGie3ff3"
      },
      "source": [
        "Similar to NumPy, Pandas objects can index or **subset the dataset to retrieve a specific sub-record of the larger dataset**. Note that data indexing returns a new **DataFrame** or **Series** if a 2-D or 1-D array is retrieved. They do not, however, alter the original dataset. Let’s go through some examples of indexing a Pandas DataFrame. First, let’s create a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmxHmbkV3i-H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "8ce57e57-57d7-404f-d55a-6a477a6011f4"
      },
      "source": [
        "data = {\n",
        "    'Age': [15, 17, 21, 29, 25],\n",
        "    'State':['Lagos', 'Cross River', 'Kano', 'Abia', 'Benue']\n",
        "}\n",
        "my_dataframe = pd.DataFrame(data=data, index=['a','b','c','d','e'])\n",
        "my_dataframe "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>State</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>15</td>\n",
              "      <td>Lagos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b</th>\n",
              "      <td>17</td>\n",
              "      <td>Cross River</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>21</td>\n",
              "      <td>Kano</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d</th>\n",
              "      <td>29</td>\n",
              "      <td>Abia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>e</th>\n",
              "      <td>25</td>\n",
              "      <td>Benue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age        State\n",
              "a   15        Lagos\n",
              "b   17  Cross River\n",
              "c   21         Kano\n",
              "d   29         Abia\n",
              "e   25        Benue"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Idfofn3kr4"
      },
      "source": [
        "### Selecting columns from a DataFrame <a name=\"3.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Daulj03mh6"
      },
      "source": [
        "A DataFrame is a collection of Series objects (the columns) sharing a common index. We can easily extract one or more columns from the DataFrame.\n",
        "\n",
        "- **Selecting a single column from a DataFrame**: Each Series column is available as an attribute on the DataFrame. Object attributes are accessed with dot syntax. A column can also be extracted by passing its name between a pair of square brackets (it supports columns with spaces in their names). The output will be a Series object. If we prefer to consistently work with 2-dimensional data structures, a Series can be converted to a DataFrame with the `to_frame` method.\n",
        "\n",
        "- **Selecting multiple columns from a DataFrame**: To extract multiple columns, we pass the column names as **strings** in a list between a pair of square brackets. The result will be a new DataFrame whose columns will have the order of the list elements.\n",
        "\n",
        " If we want to select columns based on their data types, we can use the `select_dtypes` method. The `include` and `exclude` parameters accept a single string or a list of data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4_sbD23oHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6dea63-16f8-4453-def3-a39f174f78c3"
      },
      "source": [
        "print(f'The column \"Age\" of the Dataframe:\\n{my_dataframe[\"Age\"]}\\n')\n",
        "column_names = ['Age','State']\n",
        "print('The columns \"Age\" and \"State\" of the Dataframe:\\n'\n",
        "      f'{my_dataframe[column_names]}\\n')\n",
        "print('The string columns of the Dataframe:\\n'\n",
        "      f'{my_dataframe.select_dtypes(include=\"object\")}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The column \"Age\" of the Dataframe:\n",
            "a    15\n",
            "b    17\n",
            "c    21\n",
            "d    29\n",
            "e    25\n",
            "Name: Age, dtype: int64\n",
            "\n",
            "The columns \"Age\" and \"State\" of the Dataframe:\n",
            "   Age        State\n",
            "a   15        Lagos\n",
            "b   17  Cross River\n",
            "c   21         Kano\n",
            "d   29         Abia\n",
            "e   25        Benue\n",
            "\n",
            "The string columns of the Dataframe:\n",
            "         State\n",
            "a        Lagos\n",
            "b  Cross River\n",
            "c         Kano\n",
            "d         Abia\n",
            "e        Benue\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01b7PQp-3sdX"
      },
      "source": [
        "### Selecting rows from a DataFrame <a name=\"3.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQpOo_YX3u73"
      },
      "source": [
        "Pandas uses two attributes for indexing rows from a **DataFrame** or a cell from a **Series** data structure: `iloc` and `loc` (they are also known as indexers). \n",
        "\n",
        "**Extract Rows by Index label**\n",
        "\n",
        "The `loc` attribute uses the explicit indices assigned to the DataFrame (index labels).\n",
        "\n",
        "- To extract a single row, it's declared with a pair of square brackets containing the index label and returns a Series object holding the values of the row with that label.\n",
        "\n",
        "- We can also pass a list in between the square brackets to extract multiple rows. The result will be a DataFrame. The rows will be returned in the order the index labels appear in the list.\n",
        "\n",
        "- Pandas also supports Python's list slicing syntax for extracting a selection of index labels. With string labels, both endpoints will be inclusive.\n",
        "\n",
        "- A `KeyError` exception will be raised if an index label does not exist in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7X7yNh83wr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcb0f5f-b77a-49c2-ecf6-59dc6ab4e624"
      },
      "source": [
        "print(f'Row with label index \"a\" of the DataFrame:\\n{my_dataframe.loc[\"a\"]}\\n')\n",
        "print('Rows with label index \"a\" and \"b\" of the DataFrame:\\n'\n",
        "      f'{my_dataframe.loc[[\"a\", \"b\"]]}\\n')\n",
        "print('Rows with label index from \"c\" to the end of the DataFrame:\\n'\n",
        "      f'{my_dataframe.loc[\"c\":]}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Row with label index \"a\" of the DataFrame:\n",
            "Age         15\n",
            "State    Lagos\n",
            "Name: a, dtype: object\n",
            "\n",
            "Rows with label index \"a\" and \"b\" of the DataFrame:\n",
            "   Age        State\n",
            "a   15        Lagos\n",
            "b   17  Cross River\n",
            "\n",
            "Rows with label index from \"c\" to the end of the DataFrame:\n",
            "   Age  State\n",
            "c   21   Kano\n",
            "d   29   Abia\n",
            "e   25  Benue\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffo43qF0tLgn"
      },
      "source": [
        "**Extract Rows by Index Position**\n",
        "\n",
        "The `iloc` attribute allows us to select a row(s) of a DataFrame using the intrinsic Python index format (index position). \n",
        "\n",
        "- It accepts either a single integer for one record or a list of integers for multiple records.\n",
        "\n",
        "- List slicing syntax is also valid. However, in this scenario, the numeric value after the colon is exclusive. A third number after a second colon specifies the step sequence. In the example shown, we select each alternate row from the first five rows. The results thus have index positions 0, 2, and 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894YcvmZtPCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc037bb6-fbbe-435a-ee05-e8ab0ec9899e"
      },
      "source": [
        "print('Rows with position index 0 and 1 of the DataFrame:\\n'\n",
        "      f'{my_dataframe.iloc[[0, 1]]}\\n')\n",
        "print('Rows with position index from 3 to the end of the DataFrame:\\n'\n",
        "      f'{my_dataframe.iloc[3:]}\\n')\n",
        "print('Rows from the 3th-to-last row up to (but not including) the last row of'\n",
        "      f' the DataFrame:\\n{my_dataframe.iloc[-3:-1]}\\n')\n",
        "print('Rows with position index 0, 2 and 4 of the DataFrame:\\n'\n",
        "      f'{my_dataframe.iloc[0:5:2]}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rows with position index 0 and 1 of the DataFrame:\n",
            "   Age        State\n",
            "a   15        Lagos\n",
            "b   17  Cross River\n",
            "\n",
            "Rows with position index from 3 to the end of the DataFrame:\n",
            "   Age  State\n",
            "d   29   Abia\n",
            "e   25  Benue\n",
            "\n",
            "Rows from the 3th-to-last row up to (but not including) the last row of the DataFrame:\n",
            "   Age State\n",
            "c   21  Kano\n",
            "d   29  Abia\n",
            "\n",
            "Rows with position index 0, 2 and 4 of the DataFrame:\n",
            "   Age  State\n",
            "a   15  Lagos\n",
            "c   21   Kano\n",
            "e   25  Benue\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTyxtZLXtf4k"
      },
      "source": [
        "**Extract Values from Specific Columns**\n",
        "\n",
        "Both the `loc` and `iloc` attributes accept a second argument representing the column(s) to extract.\n",
        "\n",
        "- With `loc`, we use the name of the columns. List slicing syntax can be used to extract multiple columns without explicitly writing out all of their names. Both endpoints will be inclusive. The column names must be passed in the order they appear in the DataFrame.\n",
        "\n",
        "- Each DataFrame column is assigned an index position (apart from its name). In our current DataFrame, \"Age\" has an index of 0, and \"State\" has an index of 1. With `iloc` we use the index position of the columns. List slicing syntax can be used here as well.\n",
        "\n",
        "- A list can be passed for either one of the two arguments or both of them.\n",
        "\n",
        "The `iloc` and `loc` attributes are remarkably versatile. The disadvantage of this is that it demands extra overhead since pandas has to perform several conditional checks to figure out what kind of input we've given to them. Two alternatives attributes, `at` and `iat`, are available when we want to extract a single value from a DataFrame. The `at` attribute accepts the row and columns labels, while the `iat` attribute accepts the row and column indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF1A2q9ktjg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42cbaab-f89f-410c-c5cd-2e82922885f7"
      },
      "source": [
        "print('Value at column \"Age\" and label index \"a\" of the DataFrame:\\n'\n",
        "      f'{my_dataframe.loc[\"a\", \"Age\"]}\\n')\n",
        "print('\"Age\" and \"Stage\" of rows with label indices \"a\" and \"b\" of the '\n",
        "      f'DataFrame :\\n{my_dataframe.loc[[\"a\", \"b\"], [\"Age\", \"State\"]]}\\n')\n",
        "\n",
        "print('Row with position index 0 and column with position index 0 of the '\n",
        "      f'DataFrame:\\n{my_dataframe.iloc[0,0]}\\n')\n",
        "print('Rows with position index 0 and 1, and columns with position index 0 and'f\n",
        "      f' 1 of the DataFrame:\\n{my_dataframe.iloc[[0,1],[0,1]]}\\n')\n",
        "\n",
        "print('Value at column \"Age\" and label index \"a\" of the DataFrame:\\n'\n",
        "      f'{my_dataframe.at[\"a\", \"Age\"]}')\n",
        "print('Row with position index 0 and column with position index 0 of the '\n",
        "      f'DataFrame: {my_dataframe.iat[0,0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value at column \"Age\" and label index \"a\" of the DataFrame:\n",
            "15\n",
            "\n",
            "\"Age\" and \"Stage\" of rows with label indices \"a\" and \"b\" of the DataFrame :\n",
            "   Age        State\n",
            "a   15        Lagos\n",
            "b   17  Cross River\n",
            "\n",
            "Row with position index 0 and column with position index 0 of the DataFrame:\n",
            "15\n",
            "\n",
            "Row with position index 0 and 1, and column with position index 0 and 1 of the DataFrame:\n",
            "   Age        State\n",
            "a   15        Lagos\n",
            "b   17  Cross River\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AitngYuquK3P"
      },
      "source": [
        "**Extract Value from Series**\n",
        "\n",
        "The `loc`, `iloc`, `at`, and `iat` attributes are available on Series objects as well. We can practice by extracting a sample Series from our DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08wo4cbguOT4"
      },
      "source": [
        "my_dataframe[\"Age\"].loc[\"a\"]\n",
        "my_dataframe[\"Age\"].at[\"a\"]\n",
        "my_dataframe[\"Age\"].iloc[0]\n",
        "my_dataframe[\"Age\"].iat[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBDjRY5gHyoI"
      },
      "source": [
        "# Filtering a DataFrame <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsHP92tJInWj"
      },
      "source": [
        "In the previous section, we practiced selecting subsets of the data,  individual rows, columns, values from a DataFrame, etc. Let's see now how we can **extract a subset of rows based on one or more conditions**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "WbSwWJndm2BF",
        "outputId": "46475f27-f290-4226-f686-46cc3faa8d5b"
      },
      "source": [
        "dataframe_data = {\n",
        "    'Age': [15, 17, 21, 25, 15, 21],\n",
        "    'Country':['Spain', 'Germany', 'Belgium', 'France', 'Germany', \"Spain\"]\n",
        "}\n",
        "my_dataframe = pd.DataFrame(data=dataframe_data)\n",
        "my_dataframe "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>Belgium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Country\n",
              "0   15    Spain\n",
              "1   17  Germany\n",
              "2   21  Belgium\n",
              "3   25   France\n",
              "4   15  Germany\n",
              "5   21    Spain"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrOQwe9dJp2h"
      },
      "source": [
        "## Filtering by a single condition <a name=\"4.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCqBT-vNJvTe"
      },
      "source": [
        "Suppose we want to get all people who are 21 years old. Using the equality operator (`==`) with a pandas Series and an integer value will return a new Series of Booleans where a value of True indicates a coincidence at that index position in the Series. In an example shown, the \"Age\" 21 is found in the rows with index positions 2 and 5.\n",
        "\n",
        "- To get the rows where the values of the column \"Age\" is `21`, we can pass this Boolean Series in between a pair of square brackets. It will return the rows of the DataFrame whose indices take a value of True in the Boolean Series.\n",
        "\n",
        "- We can use other [comparison operators](https://docs.python.org/3/library/stdtypes.html#comparisons) like inequality (`!=`), strictly greater than (`>=`), less than or equal (`<`), etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgasiR69JvbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0e447e-3e52-458b-b655-65f232b041d2"
      },
      "source": [
        "print('Boolean Series of column \"Age\" equal to 21:\\n'\n",
        "      f'{my_dataframe[\"Age\"] == 21}\\n')\n",
        "series_21_years = my_dataframe[\"Age\"] == 21\n",
        "print(f'Rows where column \"Age\" equal to 21:\\n{my_dataframe[series_21_years]}\\n')\n",
        "print('Rows where column \"Age\" is bigger than 17:\\n'\n",
        "      f'{my_dataframe[my_dataframe[\"Age\"] > 17]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boolean Series of column \"Age\" equal to 21:\n",
            "0    False\n",
            "1    False\n",
            "2     True\n",
            "3    False\n",
            "4    False\n",
            "5     True\n",
            "Name: Age, dtype: bool\n",
            "\n",
            "Rows where column \"Age\" equal to 21:\n",
            "   Age  Country\n",
            "2   21  Belgium\n",
            "5   21    Spain\n",
            "\n",
            "Rows where column \"Age\" is bigger than 17:\n",
            "   Age  Country\n",
            "2   21  Belgium\n",
            "3   25   France\n",
            "5   21    Spain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cTeeDVtKAot"
      },
      "source": [
        "## Filtering by Multiple Conditions <a name=\"4.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUZfgPrDKIDS"
      },
      "source": [
        "A DataFrame can also be filtered by multiple conditions. The strategy is to create two or more Boolean Series, then specify the logical criteria that must be met between them.\n",
        "\n",
        "**The `and` condition**\n",
        "\n",
        "Suppose we want to find all people older than 18 years who live in Spain. First, we get two Boolean Series for both conditions. Then we calculate the intersection of the two Series (the rows in which both of them have a value of True) by passing both of the Series into the square brackets and place an ampersand (`&`) symbol in between them. The `&` specifies an AND criteria. Both Series must have a True value at the same index position for a row to be selected.\n",
        "\n",
        "- We can pass as many Series in the square brackets as we want as long as we separate every subsequent two with a `&` symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sz8jZQzKINg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "49876ecb-24d5-4f99-f84e-b08f0d314c68"
      },
      "source": [
        "is_adult = my_dataframe[\"Age\"] >= 18\n",
        "live_in_spain = my_dataframe[\"Country\"] == \"Spain\"\n",
        "my_dataframe[is_adult & live_in_spain]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age Country\n",
              "5   21   Spain"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHY0VunCKXYK"
      },
      "source": [
        "**The `or` condition**\n",
        "\n",
        "Rows can also be extracted if they fit one of several conditions. For example, what if we want to find people older than 18 years or live in Spain. To specify an OR criteria, we use a pipe symbol (`|`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d540OgW5KeDV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "d443b383-52a5-430c-98b4-151e64bfda53"
      },
      "source": [
        "my_dataframe[is_adult | live_in_spain]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>Belgium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Country\n",
              "0   15    Spain\n",
              "2   21  Belgium\n",
              "3   25   France\n",
              "5   21    Spain"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqWdys8EKlKl"
      },
      "source": [
        "**Inversion with `~`**\n",
        "\n",
        "The tilde symbol (`~`) inverts the values in a Series of Booleans. All True values become False, and all False values become True. This is helpful when we want to invert or reverse a condition. For example, if we want to find people who are older than 17 years, we could write `my_dataframe[\"Age\"] > 17`. Or we could invert the results set of people who are younger than (or equal to) 17. The results will be identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N26Nz0kyKsSW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5d2cc403-b814-45e7-d1c1-badda4b3296f"
      },
      "source": [
        "my_dataframe[my_dataframe[\"Age\"] > 17]\n",
        "my_dataframe[~(my_dataframe[\"Age\"] <= 17)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>Belgium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Country\n",
              "2   21  Belgium\n",
              "3   25   France\n",
              "5   21    Spain"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT_NadzKKwZf"
      },
      "source": [
        "**Methods for Booleans**\n",
        "\n",
        "An alternative syntactical option is available for those who prefer methods over mathematical operators. The table below outlines six built-in methods that all return Boolean Series. Note that categorical values do not support any mathematical operations besides equality.\n",
        "\n",
        "| Operation | Method Syntax| \n",
        "|:--- |:---- |\n",
        "|Equality| `my_dataframe[\"Age\"].eq(18)`| \n",
        "|Inequality| `my_dataframe[\"Age\"].ne(18)`| \n",
        "|Less than | `my_dataframe[\"Age\"].lt(18)`| \n",
        "|Less than or equal to | `my_dataframe[\"Age\"].le(18)`|\n",
        "|Greater than | `my_dataframe[\"Age\"].gt(18)`|\n",
        "|Greater than or equal to | `my_dataframe[\"Age\"].ge(18)`|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCNkYWMALA2Z"
      },
      "source": [
        "Some filtering operations are more complex. Luckily, Pandas ships with many helper methods that return Boolean Series.\n",
        "\n",
        "**The `isin` method**\n",
        "\n",
        "Suppose we want to get all people who live in either Spain, Germany, or France countries. We could declare three separate Series and use them all inside the square brackets with the OR criteria. This approach works but it isn't scalable. What if the report called for 15 countries instead of 3?\n",
        "\n",
        "A better solution is the [`isin`](https://pandas.pydata.org/docs/reference/api/pandas.Series.isin.html) Series's method, which accepts a list of elements. It returns a Boolean Series in which a True indicates that a row's value is found amongst the list's values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBGTwlwLDdS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3923664d-b0bf-412e-b235-6472094f0d85"
      },
      "source": [
        "all_countries = [\"Spain\", \"Germany\", \"France\"]\n",
        "in_country = my_dataframe[\"Country\"].isin(all_countries)\n",
        "my_dataframe[in_country]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>Spain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Country\n",
              "0   15    Spain\n",
              "1   17  Germany\n",
              "3   25   France\n",
              "4   15  Germany\n",
              "5   21    Spain"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjqMti0lLGvA"
      },
      "source": [
        "**The `between` method**\n",
        "\n",
        "Another common challenge, especially when dealing with numeric data, is extracting values that fall within a range. For example, what if we want to extract a list of all people whose age is between 15 and 21? We could use two separate Series. \n",
        "\n",
        "There's a better way, however. The [`between`](https://pandas.pydata.org/docs/reference/api/pandas.Series.between.html) Series's method accepts a lower bound and upper bound. It returns a Boolean Series where a True indicates that a row's value falls between the specified interval.\n",
        "\n",
        "- The `between` method also works on columns of datetime values. We can pass strings representing the start and end dates of our time range. The `between_time` method only works with time values.\n",
        "\n",
        "- Finally, we can apply the `between` method to string columns. In an example shown we extract all people whose country where they live starts with \"S\". We'll start with a capital \"S\" as our inclusive lower bound and go up to the non-inclusive upper bound of \"T\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MERg24I6LKAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2bc269-2373-4166-a72d-4d88908d258c"
      },
      "source": [
        "between_15_and_21 = my_dataframe[\"Age\"].between(left=15, right=21)\n",
        "print('People who is between 15 and 21 years old:\\n'\n",
        "      f'{my_dataframe[between_15_and_21]}')\n",
        "\n",
        "country_starts_with_r = my_dataframe[\"Country\"].between(\"S\", \"T\")\n",
        "print('People whose country starts with \"S\":\\n'\n",
        "      f'{my_dataframe[country_starts_with_r]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "People who is between 15 and 21 years old:\n",
            "   Age  Country\n",
            "0   15    Spain\n",
            "1   17  Germany\n",
            "2   21  Belgium\n",
            "4   15  Germany\n",
            "5   21    Spain\n",
            "People whose country starts with \"S\":\n",
            "   Age Country\n",
            "0   15   Spain\n",
            "5   21   Spain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJJ4pVJz487H"
      },
      "source": [
        "# Handling missing and duplicate data <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jAzBIkp4_Bs"
      },
      "source": [
        "Dealing with missing data is an integral part of the data cleaning/data analysis process. Let’s see some Pandas **methods for identifying and removing missing data, as well as imputing values into missing data**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAWY8hMw-frU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "0428a897-7286-4395-d830-cc521bb718ce"
      },
      "source": [
        "dataframe_data = {\n",
        "    'Age': [15, 17, 21, 25, np.nan, np.nan],\n",
        "    'Country':[np.nan, 'Germany', 'Belgium', 'France', 'Germany', np.nan]\n",
        "}\n",
        "my_dataframe = pd.DataFrame(data=dataframe_data)\n",
        "my_dataframe "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17.0</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21.0</td>\n",
              "      <td>Belgium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Age  Country\n",
              "0  15.0      NaN\n",
              "1  17.0  Germany\n",
              "2  21.0  Belgium\n",
              "3  25.0   France\n",
              "4   NaN  Germany\n",
              "5   NaN      NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6xqJkhf4baz"
      },
      "source": [
        "## Identifying missing values <a name=\"5.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTGoU9Q9-psv"
      },
      "source": [
        "Our DataFrame includes missing values. Missing values are marked with a `NaN` (not a number) designation. The one exception is datetime values, which have a `NaT` (not a time) designation. We can use several methods to isolate rows with either null or present values in a given column. \n",
        "\n",
        "- The [`isnull`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html) method can be used to detect null values. When it's applied to a pandas Series, it returns a Boolean Series where a True indicates that a row's value is absent. When it's applied to a pandas DataFrame, it returns a Boolean DataFrame where a True indicates that the value at the corresponding column and row is absent.\n",
        "\n",
        " `NaT` values will be considered null as well.\n",
        "\n",
        " We can use the Boolean Series to select rows with missing values at specific columns from the DataFrame.\n",
        "\n",
        "- The [`notnull`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.notnull.html) method returns the inverse Series or DataFrame, one in which a True indicates a value is present.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eEmrYWhb6wI",
        "outputId": "32009936-f0fd-4114-8c62-8a508b0e99fd"
      },
      "source": [
        "print(f'Number of missing values per column:\\n{my_dataframe.isnull().sum()}\\n')\n",
        "print('Are there missing values on the DataFrame: '\n",
        "      f'{my_dataframe.isnull().values.any()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of missing values per column:\n",
            "Age        2\n",
            "Country    2\n",
            "dtype: int64\n",
            "\n",
            "Are there missing values on the DataFrame: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PYioOXO5RDw"
      },
      "source": [
        "## Removing missing values <a name=\"5.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xa_K1ty_UNK"
      },
      "source": [
        "There are multiple options when dealing with missing values. One option is to remove them.\n",
        "\n",
        "By default, the [`dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) method will return a new DataFrame with all rows from the DataFrame that hold any `NaN` values removed. It doesn’t matter if the row has one missing value or six; it will exclude them all.\n",
        "\n",
        "- We can pass an argument of `\"all\"` to the `how` parameter to remove only the rows where all values are missing.\n",
        "\n",
        "- The `subset` parameter is used to remove rows with a missing value in specific columns. It takes a list with the column names. A row will be removed if it has a missing value in any of the specified columns.\n",
        "\n",
        "- The `thresh` parameter specifies a minimum number of non-null values that a row must have in order to be kept.\n",
        "\n",
        "- The `axis` parameter, by default to 0, can be set to 1 (or \"columns\") to remove columns instead of rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Ebeuks_f2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0a6572-b289-4de5-d067-01d5a1085604"
      },
      "source": [
        "print('DataFrame with rows that contain at least one missing value removed:\\n'\n",
        "      f'{my_dataframe.dropna()}\\n')\n",
        "print('DataFrame with rows where all values are missing removed:\\n'\n",
        "      f'{my_dataframe.dropna(how=\"all\")}\\n')\n",
        "print('DataFrame with rows where age is missing removed:\\n'\n",
        "      f'{my_dataframe.dropna(subset=[\"Age\"])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame with rows that contain at least one missing value removed:\n",
            "    Age  Country\n",
            "1  17.0  Germany\n",
            "2  21.0  Belgium\n",
            "3  25.0   France\n",
            "\n",
            "DataFrame with rows where all values are missing removed:\n",
            "    Age  Country\n",
            "0  15.0      NaN\n",
            "1  17.0  Germany\n",
            "2  21.0  Belgium\n",
            "3  25.0   France\n",
            "4   NaN  Germany\n",
            "\n",
            "DataFrame with rows where age is missing removed:\n",
            "    Age  Country\n",
            "0  15.0      NaN\n",
            "1  17.0  Germany\n",
            "2  21.0  Belgium\n",
            "3  25.0   France\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8DPW6-P5kF_"
      },
      "source": [
        "## Imputing missing data <a name=\"5.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkUJyVJj5Qlz"
      },
      "source": [
        "Imputing values as substitutes for missing data is a standard practice in preparing data. Pandas DataFrames have a general method, [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html), to substitute values, but also a specific method to replace missing values, [`fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). Let's use it.\n",
        "\n",
        "- The argument `value` serves us to define the value that we want to fill the missing values. Alternately, we can pass a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled.\n",
        "\n",
        "- By default, it returns a new Dataframe. We can set the parameter `inplace=True` to modify the original DataFrame.\n",
        "\n",
        "- The argument `method` can be used (by default is `None`) to choose a method to use for filling missing values.\n",
        "\n",
        "  - With the \"forward fill\" strategy, the last valid observation is substituted when a NaN value is encountered. We can pass the `method` parameter a value of `pad` or `ffill`.\n",
        "\n",
        " - With the \"back fill\" strategy, the next valid observation is substituted when a NaN value is encountered. We can pass the `method`  parameter a value of `bfill` or `backfill`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXGiUCS85o-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ad9d44-b148-49d9-d8f8-ab94b249b593"
      },
      "source": [
        "my_dataframe.fillna({\"Age\": my_dataframe[\"Age\"].mean(), \"Country\": \"unknown\"},\n",
        "                    inplace=True)\n",
        "print('DataFrame whith missing values at column \"Age\" replace by the mean and '\n",
        "      f'missing values at \"Country\" replace by \"unknown\":\\n{my_dataframe}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame whith missing values at column \"Age\" replace by the mean and missing values at \"Country\" replace by \"unknown\":\n",
            "    Age  Country\n",
            "0  15.0  unknown\n",
            "1  17.0  Germany\n",
            "2  21.0  Belgium\n",
            "3  25.0   France\n",
            "4  19.5  Germany\n",
            "5  19.5  unknown\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpfNOGrhNSjc"
      },
      "source": [
        "## Dealing with duplicates <a name=\"5.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j82L0dJSNSjo"
      },
      "source": [
        "Pandas includes several methods to identify and remove duplicate values and rows in a dataset. \n",
        "\n",
        "**The `duplicated` method**\n",
        "\n",
        "The `duplicated` method, available for both pandas Series and DataFrames, can be used to detect duplicate values or rows. \n",
        "\n",
        "- The [Series's method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.duplicated.html) returns a Boolean Series where a True indicates that a value has previously been encountered (i.e. a duplicate).\n",
        "\n",
        "- The [DataFrame's method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html), it also returns a Boolean Series, but a True indicates that the entire row has previously been encountered.\n",
        "\n",
        " The `subset` parameter (of the DataFrame's method) can specify a list of columns whose values will be used to determine a row's uniqueness. Pandas will consider the combo of those values across a row when identifying a duplicate.\n",
        "\n",
        "- In both methods, when there is a duplicate, Pandas marks the first occurrence as a non-duplicate (with a False) and all subsequent occurrences as duplicates (with a True). However, we can ask Pandas to mark the last occurrence as a non-duplicate by passing the string `\"last\"` to the `keep` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiPjFbD7NSjp",
        "outputId": "80442f77-6130-479c-dd6f-7d4e302c310c"
      },
      "source": [
        "print(f'Duplicate rows:\\n{my_dataframe[my_dataframe.duplicated()]}\\n')\n",
        "print('Duplicate rows by column \"Age\":\\n'\n",
        "      f'{my_dataframe[my_dataframe.duplicated(subset=[\"Age\"])]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Duplicate rows:\n",
            "Empty DataFrame\n",
            "Columns: [Age, Country]\n",
            "Index: []\n",
            "\n",
            "Duplicate rows by column \"Age\":\n",
            "    Age  Country\n",
            "5  19.5  unknown\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaQ-y2s8NSjq"
      },
      "source": [
        "Let's say we want to extract one person from each country. One solution is to pull out the first encountered row for each value in the column \"Country\". The `duplicated` Series's method returns a Series where True marks all duplicate values after the first encounter. If we invert those results, we'll get a Series where True marks the first time a value is encountered. Then we can extract exactly one person per country. This time around, a NaN will be considered a unique value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "GDONMH4bNSjr",
        "outputId": "b97e7980-9efb-4997-b33a-8d47d7d4e28e"
      },
      "source": [
        "first_one_in_country = ~my_dataframe[\"Country\"].duplicated()\n",
        "my_dataframe[first_one_in_country]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.0</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17.0</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21.0</td>\n",
              "      <td>Belgium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Age  Country\n",
              "0  15.0  unknown\n",
              "1  17.0  Germany\n",
              "2  21.0  Belgium\n",
              "3  25.0   France"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hckg7uoNSjr"
      },
      "source": [
        "**The `drop_duplicates` method**\n",
        "\n",
        "The `drop_duplicates` method, available for both pandas Series and DataFrames, can be used to remove duplicated values or rows.\n",
        "\n",
        "- The DataFrame's [`drop_duplicates`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) method, by default, will return a DataFrame with duplicate rows removed (any rows where all values are shared with a previously encountered row). In the example shown, the combination of values in each row is unique in our dataset, so it won't accomplish anything.\n",
        "\n",
        " Like with the `duplicated` DataFrame's method, the `subset` parameter can specify a list of columns whose values will be used to determine a row's uniqueness. Pandas will consider the combo of those values across a row when identifying a duplicate.\n",
        "\n",
        "- The Series's [`drop_duplicates`](https://pandas.pydata.org/docs/reference/api/pandas.Series.drop_duplicates.html) will return a Series with duplicate values removed.\n",
        "\n",
        "- Both methods also accept a `keep` parameter. We can pass in an argument of `\"last\"` to keep the rows with the last occurrence of each encountered value. The `keep` parameter accepts one other argument, False, which will discard all duplicated rows/values.\n",
        "\n",
        "- Both methods accept the `inplace` parameter to optionally mutate the original DataFrame instead of returning a new Series/DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVySmK4INSjs",
        "outputId": "3f1e28a3-ccd8-43e7-a22f-176e923f5466"
      },
      "source": [
        "print('DataFrame with duplicate rows removed:\\n'\n",
        "      f'{my_dataframe.drop_duplicates()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame with duplicate rows removed:\n",
            "    Age  Country\n",
            "0  15.0  unknown\n",
            "1  17.0  Germany\n",
            "2  21.0  Belgium\n",
            "3  25.0   France\n",
            "4  19.5  Germany\n",
            "5  19.5  unknown\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWsaLCWb5uuB"
      },
      "source": [
        "# Data aggregation (grouping) <a name=\"6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJU2vGc05oSx"
      },
      "source": [
        "A common practice in data science is grouping a set of data attributes. **Isolating groups of rows based on common values in a column** is **useful** either for retrieving some **group statistics** or applying a particular set of functions to the group. Grouping is commonly used for **data exploration and plotting graphs** to understand more about the dataset. First, let's create a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f12mPUy55zRN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "bd26d479-9653-4898-adff-708d525cc9dc"
      },
      "source": [
        "my_dataframe = pd.DataFrame({\n",
        "    'Genre': ['Male', 'Female', 'Male', 'Female','Male', 'Male','Male', 'Male'],\n",
        "    'City': ['NY', 'Rome', 'Paris', 'Rome', 'NY', 'NY', 'Rome', 'Rome'],\n",
        "    'Age': np.random.randint(20, 60, 8),\n",
        "    'Salary': np.random.randint(15000, 80000, 8)\n",
        "})\n",
        "my_dataframe "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genre</th>\n",
              "      <th>City</th>\n",
              "      <th>Age</th>\n",
              "      <th>Salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>NY</td>\n",
              "      <td>35</td>\n",
              "      <td>73187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Female</td>\n",
              "      <td>Rome</td>\n",
              "      <td>36</td>\n",
              "      <td>76700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>Paris</td>\n",
              "      <td>40</td>\n",
              "      <td>59283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Female</td>\n",
              "      <td>Rome</td>\n",
              "      <td>41</td>\n",
              "      <td>24752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>NY</td>\n",
              "      <td>23</td>\n",
              "      <td>76581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Male</td>\n",
              "      <td>NY</td>\n",
              "      <td>37</td>\n",
              "      <td>57456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Male</td>\n",
              "      <td>Rome</td>\n",
              "      <td>39</td>\n",
              "      <td>68302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Male</td>\n",
              "      <td>Rome</td>\n",
              "      <td>22</td>\n",
              "      <td>48809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Genre   City  Age  Salary\n",
              "0    Male     NY   35   73187\n",
              "1  Female   Rome   36   76700\n",
              "2    Male  Paris   40   59283\n",
              "3  Female   Rome   41   24752\n",
              "4    Male     NY   23   76581\n",
              "5    Male     NY   37   57456\n",
              "6    Male   Rome   39   68302\n",
              "7    Male   Rome   22   48809"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34aheEGqbLDS"
      },
      "source": [
        "## The `GroupBy` object <a name=\"6.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUKEIt4cPN4"
      },
      "source": [
        "Suppose we want to get the average age of male and female people. We can create a Boolean Series to extract rows based on a specific value (say \"Male\") within any DataFrame column (\"Genre\"). We can then calculate the Male genre's average age by calling the `mean` method on the \"Age\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGCRXV1scuRF",
        "outputId": "4b5c670c-9cd6-4f2c-e4b9-796c0e898cfa"
      },
      "source": [
        "male = my_dataframe[\"Genre\"] == \"Male\"\n",
        "male_people = my_dataframe[male]\n",
        "print(f'Average age of male people: {male_people[\"Age\"].mean()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average age of male people: 32.666666666666664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6EuVJcVidmt"
      },
      "source": [
        "With this approach, however, we'll need to write more code to apply the same logic to the female people. The `GroupBy` object offers a better solution.\n",
        "\n",
        "A [`GroupBy`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) operation is ideal when we want to group DataFrame rows into clusters based on shared values in a column. For example, by isolating the \"Male\" rows and \"Female\" rows into separate groups, it's easier to perform an aggregate analysis on (like the average age within each group).\n",
        "\n",
        "We can invoke the `groupby` method on our DataFrame, by passing in the name of the column whose values should be used to create the groups (\"Genre\"). The output will be a `DataFrameGroupBy` object. This object is just a bundle of multiple DataFrames. Behind the scenes, pandas has repeated the extraction process we used before, but it's done for all categories (2 in this case) in the \"Genre\" column.\n",
        "\n",
        "- There are 2 unique values within the \"Genre\" column, so there will be 2 groups (DataFrames) within the GroupBy object. We can count the number of DataFrames stored within the GroupBy object by passing it into Python's built-in `len` function.\n",
        "\n",
        "- Another way to visualize our GroupBy object is as a dictionary that maps the 2 unique genres to a collection of rows belonging to each one. The `groups` attribute reveals a dictionary with these group-to-row associations. The keys are genres names and the values are Index objects that hold row index positions from the DataFrame.\n",
        "\n",
        "- The GroupBy supports **aggregate operations** to apply to every group. In a single method call, pandas applies the calculation to each nested DataFrame in the GroupBy object. If we only want to target a single column, we can pass the column name inside square brackets after the GroupBy object.\n",
        "\n",
        " The `agg` method can apply different aggregate operations to different columns. It accepts a dictionary as an argument, where the keys represent the column names from the DataFrame, and the values, the aggregate operations. The example shown extracts the average age, the highest age, and the highest salary of people for each genre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-CwhSsOibDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8534290a-1c04-47bd-fe46-6b898bc7eccb"
      },
      "source": [
        "group_by_genre = my_dataframe.groupby('Genre')\n",
        "print(f'Number of groups:{len(group_by_genre)}')\n",
        "print(f'GroupBy object as a dictionary: {group_by_genre.groups}')\n",
        "print(f'Average age per genre:\\n{group_by_genre[\"Age\"].mean()}\\n')\n",
        "aggregations = {\"Age\": [\"mean\", \"max\"], \"Salary\": \"max\"}\n",
        "print('Maximum age, maximum salary, and average age per genre:\\n'\n",
        "      f'{group_by_genre.agg(aggregations)}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of groups:2\n",
            "GroupBy object as a dictionary: {'Female': [1, 3], 'Male': [0, 2, 4, 5, 6, 7]}\n",
            "Average age per genre:\n",
            "Genre\n",
            "Female    38.500000\n",
            "Male      32.666667\n",
            "Name: Age, dtype: float64\n",
            "\n",
            "Maximum age, maximum salary, and average age per genre:\n",
            "              Age     Salary\n",
            "             mean max    max\n",
            "Genre                       \n",
            "Female  38.500000  41  76700\n",
            "Male    32.666667  40  76581\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTpMmLhebSM_"
      },
      "source": [
        "## Methods on a `GroupBy` object <a name=\"6.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htuarEppjQ81"
      },
      "source": [
        "Now that we understand the GroupBy object, let's explore some of its methods.\n",
        "\n",
        "- The `first` method extracts the first row listed for each group in the DataFrame. The return value of first is a 2-row DataFrame (one person per genre). Conversely, the `last` method extracts the last row listed for each group in the DataFrame.\n",
        "\n",
        "- The `nth` method extracts rows at a specified index position within their group. For example, invoking the `nth` method with an argument of 0 will return the first people listed within each genre (identical to the result returned from invoking the `first` method).\n",
        "\n",
        "- The `head` and `tail` methods can be used to extract multiple rows for each group within the DataFrame. In the example shown, `head(2)` extracts the first two rows for each genre within the DataFrame. The complementary `tail` method extracts the last rows for each group within the DataFrame.\n",
        "\n",
        "- The `size` method on the GroupBy object returns a Series with the number of observations that belong to each group.\n",
        "\n",
        "- If we want to target all the rows for a given group, we can use the `get_group` method, which extracts a nested DataFrame from the GroupBy object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcwOqaUvjRJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4861135c-8761-4f79-86e6-36598678db75"
      },
      "source": [
        "print(f'The first row of the DataFrame per gender:\\n{group_by_genre.first()}\\n')\n",
        "print(f'Number of observations per genre:\\n{group_by_genre.size()}\\n')\n",
        "print(f'Male people:\\n{group_by_genre.get_group(\"Male\")}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first row of the DataFrame per gender:\n",
            "        City  Age  Salary\n",
            "Genre                    \n",
            "Female  Rome   36   76700\n",
            "Male      NY   35   73187\n",
            "\n",
            "Number of observations per genre:\n",
            "Genre\n",
            "Female    2\n",
            "Male      6\n",
            "dtype: int64\n",
            "\n",
            "Male people:\n",
            "    City  Age  Salary\n",
            "0     NY   35   73187\n",
            "2  Paris   40   59283\n",
            "4     NY   23   76581\n",
            "5     NY   37   57456\n",
            "6   Rome   39   68302\n",
            "7   Rome   22   48809\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y67WO5Ybbnf"
      },
      "source": [
        "## Applying custom operation to all groups <a name=\"6.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSLi94MQkHnb"
      },
      "source": [
        "Sometimes we want to apply custom operations to each nested group within a GroupBy object.\n",
        "\n",
        "Let's say we want to get the people with the highest salary in each genre. The GroupBy object's `max` method can find each genre's maximum salary value but cannot extract the corresponding person row. A DataFrame has a `nlargest` method that can extract the rows with the greatest value in a specified column. We'd like to invoke the `nlargest` method on each nested DataFrame within the GroupBy object. The `apply` method can be used here. It expects a function as an argument.\n",
        "\n",
        "The `apply` method passes each grouped DataFrame from the GroupBy object into the method specified. It then aggregates the return values returned from the lambda function invocations. In the example shown, we invoke the `nlargest` method on each DataFrame passed in. The method extracts the row with the greatest value in that DataFrame's Salary column. The resulting rows are then aggregated into a new DataFrame. Each genre is listed alongside the person with the highest salary within it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9AClI-CkJZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "40807d73-553d-432f-c3ff-befd6cb41333"
      },
      "source": [
        "group_by_genre.apply(lambda df: df.nlargest(1, \"Salary\")).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>City</th>\n",
              "      <th>Age</th>\n",
              "      <th>Salary</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Genre</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Female</th>\n",
              "      <th>1</th>\n",
              "      <td>Rome</td>\n",
              "      <td>36</td>\n",
              "      <td>76700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Male</th>\n",
              "      <th>4</th>\n",
              "      <td>NY</td>\n",
              "      <td>23</td>\n",
              "      <td>76581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          City  Age  Salary\n",
              "Genre                      \n",
              "Female 1  Rome   36   76700\n",
              "Male   4    NY   23   76581"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf5A3JS3bgKH"
      },
      "source": [
        "## Grouping by multiple columns <a name=\"6.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqE7tK1150n7"
      },
      "source": [
        "We can group by more than one variable. In the example shown, we pass a list of strings to the `groupby` method to group first by the values in the \"Genre\" column, then by the values in the \"City\" column.\n",
        "\n",
        "- The GroupBy object has a length of 4, which means there are 4 unique combinations of genre and city within our DataFrame.\n",
        "\n",
        "- The GroupBy object's `size` method now returns a MultiIndex Series with a count of rows within each internal DataFrame.\n",
        "\n",
        "- The `get_group` method now requires a tuple of values to extract any nested DataFrame. In the example shown, we target the male people with a city of \"NY\".\n",
        "\n",
        "- All **aggregate operations** will now return a MultiIndex DataFrame with the calculations. Individual columns from our DataFrame can be targeted for aggregate calculations as well. An example shown calculates the average salary for people within each genre/city combo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3PU4zH651uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04225af8-7025-48b0-b44f-8acefe52421d"
      },
      "source": [
        "group_by_genre_and_city = my_dataframe.groupby(by=['Genre','City'])\n",
        "print(f'Number of unique combinations between of genre and city in the '\n",
        "      f'DataFrame: {len(group_by_genre_and_city)}')\n",
        "print(f'Male people who live in New York:\\n'\n",
        "      f'{group_by_genre_and_city.get_group((\"Male\", \"NY\"))}\\n')\n",
        "print(f'Max age and salary per genre per city:\\n'\n",
        "      f'{group_by_genre_and_city.max()}\\n')\n",
        "print(f'Average salary per genre per city:\\n'\n",
        "      f'{group_by_genre_and_city[\"Salary\"].mean()}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique combinations between of genre and city in the DataFrame: 4\n",
            "Male people who live in New York:\n",
            "  Genre City  Age  Salary\n",
            "0  Male   NY   35   73187\n",
            "4  Male   NY   23   76581\n",
            "5  Male   NY   37   57456\n",
            "\n",
            "Max age and salary per genre per city:\n",
            "              Age  Salary\n",
            "Genre  City              \n",
            "Female Rome    41   76700\n",
            "Male   NY      37   76581\n",
            "       Paris   40   59283\n",
            "       Rome    39   68302\n",
            "\n",
            "Average salary per genre per city:\n",
            "Genre   City \n",
            "Female  Rome     50726.000000\n",
            "Male    NY       69074.666667\n",
            "        Paris    59283.000000\n",
            "        Rome     58555.500000\n",
            "Name: Salary, dtype: float64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYxSoc-DaRdG"
      },
      "source": [
        "# Working with dates and times <a name=\"7\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTdak8t-aUwF"
      },
      "source": [
        "A datetime is a data type used to **store date and time information**. A datetime **can represent a** specific **date, a time, or both**. Datetimes are important because they hold the key to identifying trends over time. A financial analyst may use datetimes to track the weekdays that a stock performs best. A restaurant owner may use them to identify the peak hours that customers are patronizing their business, etc.\n",
        "\n",
        "In this section, we'll review Python's built-in solutions for datetimes and how Pandas improves upon them with the `Timestamp` and `Timedelta` objects. We'll see how we can use the library to convert strings to dates, add and subtract offsets of time, calculate durations, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35JBLT5yadaU"
      },
      "source": [
        "## Python's `datetime` <a name=\"7.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnUByyJ8ajHa"
      },
      "source": [
        "Python's native `datetime` module holds classes for working with dates and times. We are going to explore four classes within the module:\n",
        "\n",
        "- A **`date`** represents a single day in history. It does not store the time of day. The date class constructor accepts `year`, `month`, and `day` parameters as integers. An example shown instantiates a date object for April 12th, 1991. \n",
        "\n",
        " - The attributes `year`, `month`, and `day` are available on the `date` object.\n",
        "\n",
        "- The **`time`** class represents a specific time of day, irrespective of date. The first three parameters accept integer arguments for the `hour`, `minute`, and `second` (`0` is the default argument for all three parameters). An example shown uses a time object to model 6:43:25 AM.\n",
        "\n",
        " - The attributes `hour`, `minute`, and `second` are available on the `time` object. \n",
        "\n",
        " - The time constructor is based on a **24-hour clock**. The time  19:43:22 is equivalent to 7:43:22PM.\n",
        "\n",
        "- A **`datetime`** object holds both a date and a time. Its first six parameters represent the `year`, `month`, `day`, `hour`, `minute`, and `second`. \n",
        "\n",
        " - Arguments are only required for the date-related parameters, `year`, `month`, and `day`. The time-related attributes are optional and default to 0. The example shown models January 1st, 2021, at midnight (22:56:00).\n",
        "\n",
        "- The datetime module's **`timedelta`** object models a duration, a length of time. \n",
        "\n",
        " - All of the timedelta constructor's arguments are optional and default to 0. The object will add the times passed to it when calculating the total duration.\n",
        "\n",
        "- **These objects are immutable**. Their internal state cannot change once they have been created. An attempt to overwrite any of the attributes will raise an `AttributeError` exception."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssF8X3t9algC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80733798-0d33-4ea7-a2c3-c70fadae74d7"
      },
      "source": [
        "import datetime as dt\n",
        "\n",
        "my_date = dt.date(year=1991, month=4, day=12)\n",
        "print(f'My date: {my_date}. Variable type: {type(my_date)}')\n",
        "print(f'Year/month/day of date: {my_date.year}/{my_date.month}/{my_date.day}\\n')\n",
        "\n",
        "my_time = dt.time(hour=6, minute=43, second=25)\n",
        "print(f'My time: {my_time}. Variable type: {type(my_time)}')\n",
        "print('Hour:minute:second of time: '\n",
        "      f'{my_time.hour}:{my_time.minute}:{my_time.second}\\n')\n",
        "\n",
        "my_datetime = dt.datetime(year=2021, month=1, day=1, hour=22, minute=56)\n",
        "print(f'My datetime: {my_datetime}. Variable type: {type(my_datetime)}')\n",
        "print('Year/month/day; Hour:minute:second of datetime: '\n",
        "      f'{my_datetime.year}/{my_datetime.month}/{my_datetime.day}; '\n",
        "      f'{my_datetime.hour}:{my_datetime.minute}:{my_datetime.second}\\n')\n",
        "\n",
        "my_timedelta = dt.timedelta(weeks=8, days=6, hours=3, minutes=58, seconds=12)\n",
        "print(f'My timedelta: {my_timedelta}. Variable type: {type(my_timedelta)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My date: 1991-04-12. Variable type: <class 'datetime.date'>\n",
            "Year/month/day of date: 1991/4/12\n",
            "\n",
            "My time: 06:43:25. Variable type: <class 'datetime.time'>\n",
            "Hour:minute:second of time: 6:43:25\n",
            "\n",
            "My datetime: 2021-01-01 22:56:00. Variable type: <class 'datetime.datetime'>\n",
            "Year/month/day; Hour:minute:second of datetime: 2021/1/1; 22:56:0\n",
            "\n",
            "My timedelta: 62 days, 3:58:12. Variable type: <class 'datetime.timedelta'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov6Eu8J0YG2q"
      },
      "source": [
        "## `Timestamp` and `Timedelta` objects <a name=\"7.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4HBmZYubLmN"
      },
      "source": [
        "Python's datetime module has some concerns. There are even open-source libraries like [Arrow](https://github.com/arrow-py/arrow) that aim to replace the datetime module.\n",
        "\n",
        "The pandas's **`Timestamp` object** is a replacement for Python's `datetime` object. They are often interchangeable in pandas, `Timestamp` adds additional features though. A `Timestamp` object is instantiated with the same parameters as a `datetime` object. \n",
        "\n",
        "- While day-related parameters like `year`, `month` and `day` are required. time-related parameters are optional (default to 0). The attributes `year`, `month`, `day`, `hour`, `minute`, and `second` are available on the `Timestamp` object.\n",
        "\n",
        "- The `Timestamp` constructor **accepts a variety of inputs**.\n",
        "\n",
        " - We can **pass a string** into the constructor instead of a sequence of integers. Pandas will attempt to decipher the format of the string date and parse its datetime information. It recognizes the most common storage formats: YYYY-MM-DD, MM/DD/YYYY, etc.\n",
        "\n",
        " -  We can pass Python's native `date` and `datetime` objects directly in the `Timestamp` constructor.\n",
        "\n",
        "- We can use some [comparison operators](https://docs.python.org/3/library/stdtypes.html#comparisons) between `Timestamp` objects (and even between a `Timestamp` and a python `date/datetime`). An example shown uses the less than symbol (`<`) to see if one `Timestamp` occurs earlier than another.\n",
        "\n",
        "- The **`DatetimeIndex`** is an index for storing `Timestamp` objects. If we pass a list of `Timestamps` (or Python `datetime` objects) to the Series/DataFrame constructor's `index` parameter, a `DatetimeIndex` will be applied.\n",
        "\n",
        " - We can also create a `DatetimeIndex` from scratch. Its `data` parameter expects an iterable of date values (strings, python `datetimes`, `Timestamps`, or even a mix of them). Pandas will convert all values to `Timestamps` and lodge them within the index. We can then pass this `DatetimeIndex` to the Series/DataFrame constructor's `index` parameter.\n",
        "\n",
        " - A `DatetimeIndex` can be sorted. In an example shown, we invoke the `sort_index` method on a Series to sort the dates from earliest to latest.\n",
        "\n",
        " - Sometimes a column of a DataFrame represents dates but its data type is `object` (the pandas designation for a string). The `to_datetime` function converts an iterable object (a list, tuple, Series, index, etc) to `Timestamp` objects, and returns the new values in a `DatetimeIndex`.\n",
        "\n",
        "   If we would load a dataset from a CSV file with the `read_csv` method, we could pass to its `parse_dates` parameter a list of strings with the name of columns whose values should be parsed as datetimes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov-Te0i8bN_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04196e3e-017e-46d5-ae92-ac13533d8a37"
      },
      "source": [
        "my_timestamp = pd.Timestamp(year=1991, month=4, day=12)\n",
        "print(f'My timestamp: {my_timestamp}. Variable type: {type(my_timestamp)}')\n",
        "print(f'Timestamp from a string: {pd.Timestamp(\"1991-04-12 6:13:29 PM\")}')\n",
        "print(f'My timestamp from a datetime: {pd.Timestamp(dt.datetime(1991, 4, 12))}')\n",
        "morning = pd.Timestamp(\"2020-01-01 11:23:22 AM\")\n",
        "evening = pd.Timestamp(\"2020-01-01 11:23:22 PM\")\n",
        "print(f'Timestamp occurs earlier than another?: {morning < evening}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My timestamp: 1991-04-12 00:00:00. Variable type: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
            "Timestamp from a string: 1991-04-12 18:13:29\n",
            "My timestamp from a datetime: 1991-04-12 00:00:00\n",
            "Timestamp occurs earlier then another?: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2EoH_q4gs6C",
        "outputId": "f953423b-654b-4b28-b97a-bd979a17ffe0"
      },
      "source": [
        "timestamps = [pd.Timestamp(\"2020-01-01\"), pd.Timestamp(\"2020-02-01\")] \n",
        "print(f'DatetimeIndex: {pd.Series([1, 2], index=timestamps).index}')\n",
        "dt_index = pd.DatetimeIndex(data=[\"2020-01-01\", \"2020-02-01\"])\n",
        "my_series = pd.Series(data = [1, 2], index=dt_index)\n",
        "print(f'DatetimeIndex: {my_series.index}')\n",
        "print(f'DatetimeIndex of series sorted: {my_series.sort_index().index}')\n",
        "pd.to_datetime([\"2020-01-01\", \"2020-02-01\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DatetimeIndex: DatetimeIndex(['2020-01-01', '2020-02-01'], dtype='datetime64[ns]', freq=None)\n",
            "DatetimeIndex: DatetimeIndex(['2020-01-01', '2020-02-01'], dtype='datetime64[ns]', freq=None)\n",
            "DatetimeIndex of series sorted: DatetimeIndex(['2020-01-01', '2020-02-01'], dtype='datetime64[ns]', freq=None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex(['2020-01-01', '2020-02-01'], dtype='datetime64[ns]', freq=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idbTws1qXCdU"
      },
      "source": [
        "The `Timestamp` object represents a moment in time. A related concept is a duration. A duration like \"1 hour\" represents a length of time. It does not have a specific date or time attached to it. Duration measures the distance or difference between two dates.\n",
        "\n",
        "The **`Timedelta`** constructor, like the datetime module's `timedelta` constructor, accepts keyword parameters for units of time like days, hours, minutes, and seconds. It's easy to confuse the two objects; `timedelta` is built into Python, `Timedelta` is built into pandas. They are often interchangeable in pandas.\n",
        "\n",
        "- The `to_timedelta` function converts its argument to a `Timedelta` object. In an example shown, we convert a string to a `Timedelta` object. We can also pass an integer along with the `unit` parameter. The `unit` parameter accepts a string representing the unit of time the number represents (\"hour\", \"day\", \"minute\", etc).\n",
        "\n",
        " If we pass an iterable object as an argument, the `to_timedelta` function will convert all of the iterable's values into `Timedelta` objects and return them within a `TimedeltaIndex` object. A `TimedeltaIndex` is another index that pandas offers. It can serve as the index of a data structure or as a column in a DataFrame.\n",
        "\n",
        "- Usually, `Timedelta` objects are derived rather than created from scratch. For example, the subtraction of one `Timestamp` from another will return a `Timedelta` object.\n",
        "\n",
        "- A `Timedelta` object can be added to or subtracted from `Timestamp` objects.\n",
        "\n",
        "- The `sort_values` method works with `Timedelta` Series.\n",
        "\n",
        "- Mathematical methods like `max` or `mean` are also available on `Timedelta` Series.\n",
        "\n",
        "- We can use [comparison operators](https://docs.python.org/3/library/stdtypes.html#comparisons) like equality (`==`), strictly greater than (`>=`), less than or equal (`<`), etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXxbWNoRXP2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb8ec73-9b8e-4e9c-e65f-0ceb33b378d1"
      },
      "source": [
        "print(f'A Timedelta object: {pd.Timedelta(days=8, hours=7, minutes=6)}')\n",
        "print(f'A Timedelta from a string: {pd.to_timedelta(\"3 hours, 5 minutes\")}')\n",
        "print(f'A Timedelta: {pd.to_timedelta(5, unit=\"hour\")}')\n",
        "print(f'A TimedeltaIndex: {pd.to_timedelta([5, 10, 15], unit=\"day\")}')\n",
        "print('A Timedelta created from a substraction of timestamps: '\n",
        "      f'{pd.Timestamp(\"1999-02-05\") - pd.Timestamp(\"1998-05-24\")}')\n",
        "print('A Timestamp plus 5 days: '\n",
        "      f'{pd.Timestamp(year=1991, month=4, day=12) + pd.to_timedelta(\"5 days\")}')\n",
        "print('Are the two Timedelta objects equal?: '\n",
        "      f'{pd.to_timedelta(\"6 days\") == pd.to_timedelta(\"5 days\")}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Timedelta object: 8 days 07:06:00\n",
            "A Timedelta from a string: 0 days 03:05:00\n",
            "A Timedelta: 0 days 05:00:00\n",
            "A TimedeltaIndex: TimedeltaIndex(['5 days', '10 days', '15 days'], dtype='timedelta64[ns]', freq=None)\n",
            "A Timedelta created from a substraction of timestamps: 257 days 00:00:00\n",
            "A Timestamp plus 5 days: 1991-04-17 00:00:00\n",
            "Are the two Timedelta objects equal?: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "govop0OzkvfA",
        "outputId": "67b5a990-ab2e-454a-e420-0da1ed5738fe"
      },
      "source": [
        "timedeltas = [pd.Timedelta(days=8), pd.Timedelta(days=6)]\n",
        "print(f'TimedeltaIndex: {pd.Series([1, 2], index=timedeltas).index}')\n",
        "pd.TimedeltaIndex([\"8 days\",\"6 days\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TimedeltaIndex: TimedeltaIndex(['8 days', '6 days'], dtype='timedelta64[ns]', freq=None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimedeltaIndex(['8 days', '6 days'], dtype='timedelta64[ns]', freq=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IHiZuJ2cTOE"
      },
      "source": [
        "## The `DatetimeProperties` object <a name=\"7.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQTGIXulWjB"
      },
      "source": [
        "Let's explore some of the datetime functionalities that pandas provides. First, we create a DataFrame with a column that represents a date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HJpdjl_lWqj"
      },
      "source": [
        "my_dataframe = pd.DataFrame({\n",
        "    'Genre': ['Male', 'Female', 'Male'],\n",
        "    'Birthday': pd.to_datetime([\"1988/01/02\", \"1976/04/12\", \"1991/09/07\"])\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVV04-ACcU7q"
      },
      "source": [
        "A datetime Series holds a special `dt` attribute that exposes a `DatetimeProperties` object. We can access attributes and invoke methods on this nested object to extract information from each datetime value in the column. The `dt` accessor is to datetime values what the `str` accessor is to string values (see the next section \"Working with text data\"). Both specialize in manipulations on a specific type of data.\n",
        "\n",
        "- The `day`/`month`/`year` attributes pull out the day/month/year from each date. The values are returned in a new Series.\n",
        "\n",
        "- The `dayofweek` attribute returns a Series of numbers. A 0 marks a Monday, a 1 marks a Tuesday, and so on up to 6 for Sunday. The `day_name` method will return the weekday name. The complementary `month_name` method returns a Series with the names of the months.\n",
        "\n",
        "- Some attributes on the `dt` object return Booleans. \n",
        "\n",
        " - The `is_quarter_start` attribute returns a Boolean Series where a True value confirms that the row's date fell on a quarter start day (the four quarters of a business year start on January 1st, April 1st, July 1st, and October 1st). We can use the Boolean Series from above to extract the people from the DataFrame whose birthday fell at the beginning of a quarter. The `is_quarter_end` attribute can pull out the dates that fell at the end of a quarter.\n",
        "\n",
        " - The complementary `is_month_start` and `is_month_end` methods confirm that a date fell at the beginning or end of a month. The `is_year_start` and `is_year_end` methods do the same for the beginning and end of a year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOO9CBy0cW1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476f9e99-6f6f-4f1b-b368-33c71539b4ba"
      },
      "source": [
        "print(f'Name of the months:\\n{my_dataframe[\"Birthday\"].dt.month_name()}\\n')\n",
        "print('People whose birthday fall at the beginning of a quarter:\\n'\n",
        "      f'{my_dataframe[my_dataframe[\"Birthday\"].dt.is_quarter_start]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of the months:\n",
            "0      January\n",
            "1        April\n",
            "2    September\n",
            "Name: Birthday, dtype: object\n",
            "\n",
            "People whose birthday fall at the beginning of a quarter:\n",
            "Empty DataFrame\n",
            "Columns: [Genre, Birthday]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13nFVmdJczAj"
      },
      "source": [
        "## Adding/Subtracting durations of time <a name=\"7.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbr2Bmr0c2DE"
      },
      "source": [
        "We can add or subtract consistent durations of time with the **`DateOffset`** object. Its constructor accepts keyword parameters for `years`, `months`, `days`, and more.\n",
        "\n",
        "We can add an amount of time to each date in a datetime Series with a plus sign (`+`) and a `DateOffset` object. The plus sign means \"move forward\" or \"into the future\". The minus sign (`-`) subtracts a duration from each date in a datetime Series. The minus sign means \"move backward\" or \"into the past\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVj9p_6Gc3yG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0aaf0a4-6cbb-4647-ae9c-dc300cd651cf"
      },
      "source": [
        "print('The birthday column plus 5 days and 6 hours:\\n'\n",
        "      f'{(my_dataframe[\"Birthday\"] + pd.DateOffset(days=5, hours=6))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The birthday column plus 5 days and 6 hours:\n",
            "0   1988-01-07 06:00:00\n",
            "1   1976-04-17 06:00:00\n",
            "2   1991-09-12 06:00:00\n",
            "Name: Birthday, dtype: datetime64[ns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnrz8etwdGnj"
      },
      "source": [
        "Let's say we want to round each date to the end of its current month. Each date has a different number of days from the end of its month; a `DateOffset` addition won't be sufficient. Pandas ships with multiple offset objects, each of which holds business logic for dynamic time-based calculations. They are contained in the module `pd.tseries.offsets`.\n",
        "\n",
        "- One sample offset is `MonthEnd`; it rounds each date to the next month-end. The addition and subtraction syntax is also used here. An example shown uses the `MonthEnd` offset to round the dates to the next month-end.\n",
        "\n",
        " A date cannot be rounded to the same date; there has to be some movement. Thus, if a date falls at the end of a month, it will be rounded to the end of the next month.\n",
        "\n",
        " When paired with an offset, the minus sign moves each date backward in time. An example shown uses the `MonthEnd` offset to round the dates to the previous month-end.\n",
        "\n",
        "- The complementary `MonthBegin` offset performs similar calculations for the first date of a month.\n",
        "\n",
        "- A set of offsets is available for business time calculations. Their names always begin with a capital `B`. For example, the Business Month End (`BMonthEnd`) offset rounds to the last business day (weekday) of the month.\n",
        "\n",
        "- The `pd.tseries.offsets` package includes additional offsets for rounding to starts and ends of quarters, business quarters, years, business years, and more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyT7VwCtdLUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f2d969-d4c7-4dd1-8cc3-456361d89a56"
      },
      "source": [
        "print(f'The original birthday column:\\n{my_dataframe[\"Birthday\"]}\\n')\n",
        "print('The birthday column rounded to the next month end:\\n'\n",
        "      f'{my_dataframe[\"Birthday\"] + pd.tseries.offsets.MonthEnd()}\\n')\n",
        "print('The birthday column rounded to the previous month end:\\n'\n",
        "      f'{my_dataframe[\"Birthday\"] - pd.tseries.offsets.MonthEnd()}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original birthday column:\n",
            "0   1988-01-02\n",
            "1   1976-04-12\n",
            "2   1991-09-07\n",
            "Name: Birthday, dtype: datetime64[ns]\n",
            "\n",
            "The birthday column rounded to the next month end:\n",
            "0   1988-01-31\n",
            "1   1976-04-30\n",
            "2   1991-09-30\n",
            "Name: Birthday, dtype: datetime64[ns]\n",
            "\n",
            "The birthday column rounded to the previous month end:\n",
            "0   1987-12-31\n",
            "1   1976-03-31\n",
            "2   1991-08-31\n",
            "Name: Birthday, dtype: datetime64[ns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE8ZV4B1ebFS"
      },
      "source": [
        "# Working with text data <a name=\"8\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKoU1cd_eeqX"
      },
      "source": [
        "Text data are usually riddled with whitespace, improper characters, incorrect casings, and more. One of the purposes of Pandas is to ease the difficulty of cleaning up these improperly formatted values. This process of smoothing data into an optimal shape before analysis is called wrangling or munging. In this section, we'll explore **methods to efficiently clean up text data**. First, we create a DataFrame with information of establishments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ3aq4fi9Og-"
      },
      "source": [
        "my_dataframe = pd.DataFrame({\n",
        "    \"Name\": [\" JET'S PIZZA\", \"Cafe 608 \", \" TEMPO CAFE\", \" ROOM 1520\"],\n",
        "    \"Risk\": [\"1 (High)\", \"3 (Low)\", \"2 (Medium)\", \"2 (Medium)\"]\n",
        "})\n",
        "my_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Xj1ZK8kSL9"
      },
      "source": [
        "## String casing <a name=\"8.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aCGDwF_e7He"
      },
      "source": [
        "There are visual inconsistencies in the \"Name\" column of our DataFrame. The row values are surrounded by whitespace, both at the beginning and end of the strings. Moreover, some values in the column are all uppercased.\n",
        "\n",
        "A Series object includes a `str` attribute whose value is a `StringMethods` object that holds powerful methods. Some of these methods match Python's native string methods, while others are exclusive to Pandas.\n",
        "\n",
        "- The `strip` family of methods are native in python, and they are available on the `StringMethods` object as well. They each return a new Series object with the operation applied to every value in the column.\n",
        "\n",
        " - The `lstrip` method removes whitespace from the beginning of a string. \n",
        "\n",
        " - The `rstrip` method removes whitespace at the end of a string. \n",
        "\n",
        " - The `strip` method removes whitespace from both ends of a string.\n",
        "\n",
        "- Python's string casing methods are all available on Series objects. \n",
        "\n",
        " - The `str.lower` method lowercases the characters in each row's value. The complementary `str.upper` method returns a Series with all strings in uppercase.\n",
        "\n",
        " - The `str.capitalize` method capitalizes the first letter of each string in the Series. The `str.title` method capitalizes the first letter of each word in a string. In the example shown, we use the `title` method and overwrite the old column with our new one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KONG0qrffBHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6655c31-0b57-4097-99c9-d737630ebff9"
      },
      "source": [
        "my_dataframe[\"Name\"] = my_dataframe[\"Name\"].str.strip().str.title()\n",
        "my_dataframe[\"Name\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Jet'S Pizza\n",
              "1       Cafe 608\n",
              "2     Tempo Cafe\n",
              "3      Room 1520\n",
              "Name: Name, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qb-NbU2kYhN"
      },
      "source": [
        "## String slicing <a name=\"8.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1susyN1Bf3Bs"
      },
      "source": [
        "The \"Risk\" column includes both a numeric and categorical representation of risk (1 vs. \"High\"). Let's say we want to extract the integer value from each row. First of all, we would have to check if all values follow a \"Risk Number (Risk Level)\" format (we can use the `value_counts` method). Ok, we have a consistent format for all values in the \"Risk\" column.\n",
        "\n",
        "- To extract the number from each row in the \"Risk\" column, the `slice` method on the `StringMethods` object is helpful. Like Python's list slicing syntax, it accepts a lower bound and an upper bound as its first two arguments. The lower bound (the starting point) is inclusive while the upper bound (the ending point) is exclusive. Python's list slicing syntax can be used on the `StringMethods` object in place of the `slice` method. \n",
        "\n",
        "- What if we want to extract the categorical ranking (\"High\", \"Medium\", and \"Low\") instead? This process is a bit more complex. The lengths of the words are different. There are a few solutions available here. We can use the `str.slice` method and extract from the beginning to the end of each word. In the example shown, we extract from index position 3 to the last index of each string.\n",
        "\n",
        "- The `str.replace` method mirrors Python's `replace` method, which accepts a string to look for and a string to replace it with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzEbnWtFf86_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d22836-b364-4640-aa7c-97564d2b09a9"
      },
      "source": [
        "print(f'Numbers of the risk column:\\n{my_dataframe[\"Risk\"].str.slice(0, 1)}\\n')\n",
        "my_dataframe[\"Risk\"].str[0:1]\n",
        "print(f'Ranking name of the risk column:\\n{my_dataframe[\"Risk\"].str[3:-1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numbers of the risk column:\n",
            "0    1\n",
            "1    3\n",
            "2    2\n",
            "3    2\n",
            "Name: Risk, dtype: object\n",
            "\n",
            "Ranking name of the risk column:\n",
            "0      High\n",
            "1       Low\n",
            "2    Medium\n",
            "3    Medium\n",
            "Name: Risk, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aqxOzdSkeQS"
      },
      "source": [
        "## Boolean methods <a name=\"8.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtoCK2okgMtF"
      },
      "source": [
        "The previous methods on the `str` object return new Series of strings. Other methods return Series of Booleans. These can be helpful when filtering a DataFrame. For example, let's say we want to isolate all establishments with the word \"Cafe\" in their names.\n",
        "\n",
        "-  The `str.contains` method returns a True for each row where its argument is found in the string. It is the equivalent of using the `in` operator on a string in Python. In an example shown, we lowercase the \"Name\" column first with `str.lower` (to ensure a consistent casing), then chain on the `str.contains` method on the resulting Series. Then we can extract all the rows with \"cafe\" anywhere in the name.\n",
        "\n",
        "- What if we want to extract all establishments beginning with the string \"cafe\"? It's no just the presence of a substring that we care about, it is its position in each string. The `str.startswith` method solves the problem for us. The complementary `str.endswith` method checks for the presence of a substring at the end of each string in a Series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVXaoLEmA_Fn",
        "outputId": "6c6d3d75-bb4b-4fa9-fb0e-7e068ec36de6"
      },
      "source": [
        "has_cafe = my_dataframe[\"Name\"].str.lower().str.contains(\"cafe\")\n",
        "print('Establishments whose name contains the word \"cafe\":\\n'\n",
        "      f'{my_dataframe[has_cafe]}\\n')\n",
        "start_with_cafe = my_dataframe[\"Name\"].str.lower().str.startswith(\"cafe\")\n",
        "print('Establishments whose name starts with the word \"cafe\":\\n'\n",
        "      f'{my_dataframe[start_with_cafe]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Establishments whose name contains the word \"cafe\":\n",
            "          Name        Risk\n",
            "1    Cafe 608      3 (Low)\n",
            "2   Tempo Cafe  2 (Medium)\n",
            "\n",
            "Establishments whose name starts with the word \"cafe\":\n",
            "        Name     Risk\n",
            "1  Cafe 608   3 (Low)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBBgnMPbkimC"
      },
      "source": [
        "## Splitting strings <a name=\"8.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQjSpes5hLiO"
      },
      "source": [
        "- The `str.len` method returns the length of each row's object.\n",
        "\n",
        "- The `split` method in Python separates a string based on the occurrences of a delimiter. It returns a list consisting of all the individual elements after the split. The `str.split` method performs the same operation on each row in a Pandas Series. The return value will be a Series of list elements. The delimiter is passed to a method parameter called `pat`(short for pattern).\n",
        "\n",
        " We can limit the number of splits to perform with the `n` parameter. \n",
        "\n",
        "- How can we pull out the first name from each row? The `str.get` method accepts the index position of the element to pull out from each row's list. We need to target index 0 here to get the first name.\n",
        "\n",
        " To pull out the last name, the `str.get` method supports negative notation. An argument of -1 will extract the last element from each row's list, regardless of how many elements it contains.\n",
        "\n",
        " The `str.split` method accepts another parameter called `expand`. When passed an argument of True, instead of returning a Series of lists, the method will return a new DataFrame where the columns correspond to the strings split. Note that if we do not limit the number of splits with the `n` parameter, Pandas will place null values in rows that do not have sufficient elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ZaWan_hLsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c555ec1-2eb5-498b-ac49-1963cb8272d0"
      },
      "source": [
        "print(f'Lenght of values at column \"Name\":\\n{my_dataframe[\"Name\"].str.len()}\\n')\n",
        "print('Values at column \"Name\" split by a whitespace:\\n'\n",
        "      f'{my_dataframe[\"Name\"].str.split(pat=\" \")}\\n')\n",
        "print('Last values of column \"Name\" split by a whitespace:\\n'\n",
        "      f'{my_dataframe[\"Name\"].str.split(pat=\" \").str.get(-1)}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lenght of values at column \"Name\":\n",
            "0    11\n",
            "1     8\n",
            "2    10\n",
            "3     9\n",
            "Name: Name, dtype: int64\n",
            "\n",
            "Values at column \"Name\" split by a whitespace:\n",
            "0    [Jet'S, Pizza]\n",
            "1       [Cafe, 608]\n",
            "2     [Tempo, Cafe]\n",
            "3      [Room, 1520]\n",
            "Name: Name, dtype: object\n",
            "\n",
            "Last values of column \"Name\" split by a whitespace:\n",
            "0    Pizza\n",
            "1      608\n",
            "2     Cafe\n",
            "3     1520\n",
            "Name: Name, dtype: object\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXWiyoZYK0U5"
      },
      "source": [
        "# Merging, joining and concatenating <a name=\"9\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se0OPvzyK1Vg"
      },
      "source": [
        "Relational databases store data across tables and establish relationships between them. A table can be considered the database equivalent of a pandas DataFrame.\n",
        "\n",
        "Pandas excels at joining data together. It can [**append, concatenate, join, merge, and combine DataFrames**](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) together on a wide range of criteria in both vertical and horizontal directions. It supports SQL operations like different types of joins.\n",
        "\n",
        "We first have to determine what each dataset represents. Are they simply two halves of a greater whole? Or are the two collections of data distinct and separate from each other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrwYT-yKLV2s"
      },
      "source": [
        "##  Concatenating DataFrames <a name=\"9.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQG6feCtLWs7"
      },
      "source": [
        "For this section, we create two DataFrames that represent users. Something funny happened during their export, and the data was split up across two files. We'd like to combine their rows into a single DataFrame. **Concatenation** refers to the **appending of one DataFrame's rows/cols to the end of another DataFrame's rows/cols**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFJPCzlS8Vza"
      },
      "source": [
        "members1 = pd.DataFrame(data={\n",
        "    \"member_id\": [1, 2, 3, 4 , 5],\n",
        "    \"Name\": [\"Matt\", \"Scott\", \"Eric\", \"John\", \"james\"]\n",
        "})\n",
        "members2 = pd.DataFrame(data={\n",
        "    \"member_id\": [6, 7, 8, 9, 10],\n",
        "    \"Name\": [\"Mark\", \"Liam\", \"William\", \"Noah\", \"Lucas\"]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rqk5wIB9BBS"
      },
      "source": [
        "The [`concat`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) function concatenate pandas objects along a particular axis (by default the row axis). The `objs` parameter receives a list of DataFrame objects. The DataFrames will be concatenated in the order they are stored within the `objs` list.\n",
        "\n",
        "- Pandas preserves the original index labels from the DataFrames in the concatenated DataFrame. We can pass the parameter `ignore_index=True` to generate the standard ascending numeric index starting at 0 (the original index labels will be lost).\n",
        "\n",
        "- If we want to preserve which DataFrame each row of data came from, we can pass a `keys` parameter with a list of strings with the same length of the `objs` list. Each key will be associated with the DataFrame at the same index position in the `objs` list. The `concat` function returns a MultiIndex DataFrame where the first level stores the keys and the second level stores the index labels from the respective DataFrame.\n",
        "\n",
        "- Missing values can arise when column names differ between DataFrames. We can set the parameter `join=\"inner\"` to return only the column names that are shared.\n",
        "\n",
        "- A useful shortcut to `concat` is the [`append`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html#pandas.DataFrame.append) method. It concatenates along row axis. Unlike the python list's `append` method, which appends to the original list and returns None,  here it does not modify the DataFrame and returns its copy with other DataFrame appended.\n",
        "\n",
        "- We can also concatenate two DataFrames together across the column axis by setting the parameter `axis=1` (or `axis=\"columns\"`).\n",
        "\n",
        " - Missing values can arise when row indices differ between DataFrames. Again, we can set the parameter `join=\"inner\"` to return only the row indices that are shared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWttytjhLZbn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "e64eac30-276f-4b55-86c0-e26e0dc65da7"
      },
      "source": [
        "pd.concat(objs=[members1, members2], ignore_index=True)\n",
        "pd.concat(objs=[members1, members2], keys=[\"A\", \"B\"])\n",
        "members1.append(members2, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>member_id</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Matt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Scott</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Eric</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>John</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>james</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>Mark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>Liam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>William</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>Noah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>Lucas</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   member_id     Name\n",
              "0          1     Matt\n",
              "1          2    Scott\n",
              "2          3     Eric\n",
              "3          4     John\n",
              "4          5    james\n",
              "5          6     Mark\n",
              "6          7     Liam\n",
              "7          8  William\n",
              "8          9     Noah\n",
              "9         10    Lucas"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7DCTU1Rqwct"
      },
      "source": [
        "## Joining/merging <a name=\"9.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm31k83Cq4ax"
      },
      "source": [
        "This section covers different types of joins in pandas. Pandas has full-featured, high-performance in-memory join operations idiomatically very similar to relational databases like SQL.\n",
        "\n",
        "Each join applies a different logical criterion for extraction. A join can extract rows from two DataFrames by values that are exclusive to one or shared between both.\n",
        "\n",
        "![](https://i.ibb.co/0rkNdpc/join-types.png)\n",
        "\n",
        "- An **inner join** targets shared elements across two datasets, that is, the values that only exist in both DataFrames. Values that exist in the one DataFrame but do not exist in the other are excluded.\n",
        "\n",
        "- An **outer join** combines all elements across two datasets. A record's exclusivity to either the left or right DataFrame does not matter. An outer join inherently includes the results of an inner join.\n",
        "\n",
        "- A **left join** pulls in values from a second dataset based on foreign keys in an original dataset. A left join is optimal when one of the datasets is the focal point of the analysis. The right dataset is pulled in to provide supplemental information related to the primary records in the left dataset.\n",
        "\n",
        "Suppose we have two DataFrames that represent people and pets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBryqTIorjGF"
      },
      "source": [
        "people = pd.DataFrame(data={\n",
        "    \"person_id\": [1000, 1001, 1002, 1003],\n",
        "    \"Name\": [\"Jesse\", \"John\", \"Sarah\", \"Amy\"]\n",
        "})\n",
        "pets = pd.DataFrame(data={\n",
        "    \"pet_id\": [100, 101, 102, 103],\n",
        "    # \"owner_id\": [1001, 1001, 1000, 1003],\n",
        "    \"person_id\": [1001, 1001, 1000, 1003],\n",
        "    \"Name\": [\"Nobert\", \"Sally\", \"Jack\", \"Fido\"]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9I2n6FFL4mW"
      },
      "source": [
        "These two tables are connected by a relationship. For example, the pet named Jack has a `person_id` of `1000`, which corresponds to the `person_id` of `1000` of the people table (Jesse). Therefore, if we want additional information about Jack, like who his owner is, we could use this relationship to look up information. We have to target the rows from the two DataFrames that have an equal value in their `person_id` columns.\n",
        "\n",
        "The [`merge`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) method can perform all standard database join operations. \n",
        "\n",
        "- The `left` and `right` parameters to the `merge` method expects the DataFrames to merge. The terminology comes from the previous diagram. The \"left\" DataFrame is the circle on the left side, and the \"right\" DataFrame is the circle on the right side. For an inner join, which identifies common elements in both datasets, and for outer join, which identifies both common and uncommon elements in both datasets, the results will be the same in either direction.\n",
        "\n",
        "- The `how` parameter expects a string with the type of join. Notice that in the inner join, the person Sarah doesn’t appear in the joined table cause she doesn’t have a pet. In the outer join, however, she appears, but with missing values for the columns from the DataFrame `pets`. The logic applies in reverse as well (if there would be some pets whose owner would not be present in the table `people`, it is not the case here).\n",
        "\n",
        " We can pass the parameter `indicator=True` to identify whether the `person_id` exists in the left DataFrame, the right, or both. The merged DataFrame will include a `_merge` column that stores the values \"both\", \"left_only\", and \"right_only\".\n",
        "\n",
        "- Pandas also needs to know the columns to be used to identify matches. We pass the column name as a string to the `on` parameter. We can also pass the `on` parameter a list to identify matches across multiple columns.\n",
        "\n",
        " We're lucky in this case because both DataFrames have a `group_id` column. If the column name differs across the DataFrames, we pass to the `left_on` and `right_on` parameters different strings to indicate the column names in the respective DataFrames. A DataFrame that we'd like to merge into another one may have its ids stored in its index rather than in a column. We can pass the parameter `right_index=True` (or `left_index=True`) to tells pandas to look for in the right (or left) DataFrame's index.\n",
        "\n",
        "- The merged DataFrame includes all columns from both DataFrames. By default, the overlapping columns name from the left DataFrame are suffixed with `\"_x\"` and the columns from the right are suffixed with `\"_y\"`. We can modify this with the `suffixes` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUcIdM96tbN-",
        "outputId": "90d41847-83ea-4b62-da72-23b098f4095c"
      },
      "source": [
        "print('Inner join:\\n'\n",
        "      f'{pd.merge(left=people, right=pets, how=\"inner\", on=\"person_id\")}\\n')\n",
        "outer_join = pd.merge(left=people, right=pets, how=\"outer\", on=\"person_id\", \n",
        "                      indicator=True)\n",
        "print(f'Outer join:\\n{outer_join}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inner join:\n",
            "   person_id Name_x  pet_id  Name_y\n",
            "0       1000  Jesse     102    Jack\n",
            "1       1001   John     100  Nobert\n",
            "2       1001   John     101   Sally\n",
            "3       1003    Amy     103    Fido\n",
            "\n",
            "Outer join:\n",
            "   person_id Name_x  pet_id  Name_y     _merge\n",
            "0       1000  Jesse   102.0    Jack       both\n",
            "1       1001   John   100.0  Nobert       both\n",
            "2       1001   John   101.0   Sally       both\n",
            "3       1002  Sarah     NaN     NaN  left_only\n",
            "4       1003    Amy   103.0    Fido       both\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYfIy2NTO0VF"
      },
      "source": [
        "# Importing/Exporting data <a name=\"10\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS1FY9FZO0VO"
      },
      "source": [
        "Datasets come in a variety of file formats: comma-separated values (CSV), tab-separated values (TSV), Excel workbooks (XLSX), JavaScript Object Notation (JSON), and more. In this section, we'll learn how to **use pandas to load/export data structures from/to various file types and data structures**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqUcY7JbO0VP"
      },
      "source": [
        "## JSON files <a name=\"10.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZwgp7m2O0VQ"
      },
      "source": [
        "JavaScript Object Notation (JSON) is a format for storing and transferring text data and it is a popular response format for many modern APIs. Most languages today, including Python, can generate and parse JSON. JSON is similar to Python's dictionary object. A JSON consists of key-value pairs, where a key serves as a unique identifier for a value. Keys must be strings. Values can be of any valid data type, including strings, numbers, Booleans, array (an ordered collection of elements equivalent to a Python list). JSON can store additional key-value pairs within nested objects.\n",
        "\n",
        "**Loading a JSON File Into a DataFrame**\n",
        "\n",
        "To import a JSON file to a pandas DataFrame, we have some options:\n",
        "\n",
        "- We can load the JSON file as a python dictionary with the [json python module](https://docs.python.org/3/library/json.html) and then instantiate a DataFrame from a dictionary with the `DataFrame` constructor or with the `from_dict` DataFrame's method. The [`json_normalize`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html) function can help when there are nested records in the JSON file. \n",
        "\n",
        "- We can use the [`read_json`](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) pandas function to load directly a JSON file to a pandas DataFrame.\n",
        "\n",
        "\n",
        "**Exporting a DataFrame to a JSON File**\n",
        "\n",
        "To convert a DataFrame to a JSON representation and writing it to a JSON file we can use the [`to_json`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html) method. \n",
        "\n",
        "- Its `orient` parameter customizes the format in which pandas returns the data. \n",
        "\n",
        "  If `orient=\"records\"`, it returns a JSON array of key-value objects. Pandas stores the column names as dictionary keys pointing to the row's respective values.\n",
        "\n",
        "  If `orient=\"split\"`, it returns a dictionary with separate columns, index, and data keys. This option avoids the duplication of column names for each row.\n",
        "\n",
        "  There are additional arguments available for the `orient` parameter including \"index\", \"columns\", \"values\", and \"table\".\n",
        "\n",
        "- Once the JSON format fits our expectations, we pass the path of the file where we want to save the JSON file as the argument `path_or_buf` to the `to_json` method. If we provide only the file name, Pandas will write the string to a JSON file in the same directory where the code is executing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5cmuGi3O0VR"
      },
      "source": [
        "dataframe_data = {\n",
        "    'Age': [15, 17, 21, 25],\n",
        "    'Country':['Spain', 'Germany', 'Belgium', 'France']\n",
        "}\n",
        "my_dataframe = pd.DataFrame(data=dataframe_data)\n",
        "my_dataframe.to_json(path_or_buf=\"my_dataframe.json\", orient=\"records\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywEVKdcUO0VS"
      },
      "source": [
        "## CSV files <a name=\"10.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkK-JtR3O0VS"
      },
      "source": [
        "Our dataset to be read is a collection of baby names in New York City. The CSV file is hosted on New York City's government [website](https://data.cityofnewyork.us/api/views/25th-nujf/rows.csv).\n",
        "\n",
        "The [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function reads a comma-separated values (CSV) file into DataFrame.\n",
        "\n",
        "- The first parameter of the function, `filepath_or_buffer`,  accepts the path of the CSV file. If we pass the filename, Pandas will look for the file in the same directory where the code is executing. We can also pass a URL directly and pandas will fetch the dataset.\n",
        "\n",
        "- The `index_col` parameter accepts a string with the name of the column that should serve as the index of the DataFrame.\n",
        "\n",
        "- Pandas will infer the most logical data type for each column. When it comes to dates, it imports them as strings. To explicitly tell pandas to treat the values in a column/s as datetime objects we can pass to the `parse_dates` parameter a list of strings representing the columns whose text values should be converted to datetime objects. Pandas recognizes a variety of different string formats for dates.\n",
        "\n",
        "- We can limit the imported columns with the `usecols` parameter. It accepts a list of strings for columns that should be included in the import. All other columns will be ignored.\n",
        "\n",
        "- If the dataset only contains one column and we want that the returned object would be a Series, we can pass `squeeze=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "nLRK2ysxO0VT",
        "outputId": "15ded5e6-cb51-4128-d6d8-3ab58974caf3"
      },
      "source": [
        "url = \"https://data.cityofnewyork.us/api/views/25th-nujf/rows.csv\"\n",
        "baby_names = pd.read_csv(filepath_or_buffer=url, squeeze=True)\n",
        "baby_names.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year of Birth</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>Child's First Name</th>\n",
              "      <th>Count</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2011</td>\n",
              "      <td>FEMALE</td>\n",
              "      <td>HISPANIC</td>\n",
              "      <td>GERALDINE</td>\n",
              "      <td>13</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>FEMALE</td>\n",
              "      <td>HISPANIC</td>\n",
              "      <td>GIA</td>\n",
              "      <td>21</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011</td>\n",
              "      <td>FEMALE</td>\n",
              "      <td>HISPANIC</td>\n",
              "      <td>GIANNA</td>\n",
              "      <td>49</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011</td>\n",
              "      <td>FEMALE</td>\n",
              "      <td>HISPANIC</td>\n",
              "      <td>GISELLE</td>\n",
              "      <td>38</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011</td>\n",
              "      <td>FEMALE</td>\n",
              "      <td>HISPANIC</td>\n",
              "      <td>GRACE</td>\n",
              "      <td>36</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year of Birth  Gender Ethnicity Child's First Name  Count  Rank\n",
              "0           2011  FEMALE  HISPANIC          GERALDINE     13    75\n",
              "1           2011  FEMALE  HISPANIC                GIA     21    67\n",
              "2           2011  FEMALE  HISPANIC             GIANNA     49    42\n",
              "3           2011  FEMALE  HISPANIC            GISELLE     38    51\n",
              "4           2011  FEMALE  HISPANIC              GRACE     36    53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXd6EECfO0VU"
      },
      "source": [
        "The [`to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) DataFrames's method writes the DataFrame to a CSV file. Without an argument, the method outputs the CSV string directly. Following CSV conventions, pandas separates row values with commas and different rows with line breaks (`\\n` in python).\n",
        "\n",
        "- To write the string to a CSV file, we pass its file path as the first argument to the method. If we do not provide a specific path, pandas will write the file to the same directory where the code is executing.\n",
        "\n",
        "- By default, pandas includes the DataFrame index in the CSV string. We can exclude the index by passing the `index` parameter an argument of `False`.\n",
        "\n",
        "- By default, pandas writes all DataFrame columns to the CSV file. We can limit the columns by passing a subset of column names to the `columns` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhFQNiUjO0VV"
      },
      "source": [
        "baby_names.to_csv(path_or_buf=\"NYC_Baby_Names.csv\", index=False, \n",
        "                  columns=[\"Gender\", \"Child's First Name\", \"Count\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Jm0kKVO0VV"
      },
      "source": [
        "## Excel workbooks <a name=\"10.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlSJ9lbxO0VW"
      },
      "source": [
        "Excel is the most popular spreadsheet application. Pandas makes it easy to read from and write to Excel workbooks and even specific worksheets.\n",
        "\n",
        "**Importing excel workbooks**\n",
        "\n",
        "The [`read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) function imports an Excel workbook into a DataFrame. \n",
        "\n",
        "- Its parameter `io` accepts a string with the path to the workbook. \n",
        "\n",
        "- The `read_excel` function supports many of the same parameters as `read_csv`, including `index_col` to set the index columns, `usecols` to select the columns and `squeeze` to coerce a one-column worksheet into a Series object. \n",
        "\n",
        "- By default, pandas will import only the first worksheet in the workbook..During import, pandas assigns each worksheet an index position starting at 0. To import a specific worksheet, we can pass the `sheet_name` parameter either the worksheet's index position or its name.\n",
        "\n",
        " To import all worksheets, we pass `sheet_name=None`. The `read_excel` function will return a dictionary with the worksheets' names as keys and the respective DataFrames as values.\n",
        "\n",
        " To limit the worksheets that pandas imports, we can pass the `sheet_name` parameter a list of index positions or worksheet names. Pandas will return a dictionary whose keys match the strings in the `sheet_name` list.\n",
        "\n",
        "**Exporting excel workbooks**\n",
        "\n",
        "Writing to an Excel workbook requires a few more steps than writing to a CSV. \n",
        "\n",
        "- First up, we need to create an ExcelWriter object. This object serves as the foundation of the workbook. We'll attach individual worksheets to it.\n",
        "\n",
        " Its parameter `path` accepts a string with the new workbook's filename. If we do not provide a path, pandas will create the Excel file in the same directory where the code is executing it.\n",
        "\n",
        "- Next up, we need to connect our DataFrame to an individual worksheet in the workbook. The DataFrame object's `to_excel` method accepts an ExcelWriter object for its first parameter, `excel_writer`. The `sheet_name` parameter accepts a string with the desired worksheet name. Finally, we can pass the index parameter a value of False to exclude the DataFrame index. To filter the columns that pandas will include, we can pass a list of their names to the `columns` parameter. Note that pandas has not created the Excel workbook yet.\n",
        "\n",
        "- Finally, we call the `save` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBMfE4LzO0VW"
      },
      "source": [
        "excel_file = pd.ExcelWriter(\"my_data.xlsx\")\n",
        "my_dataframe.to_excel(excel_writer=excel_file, sheet_name=\"my_dataframe\", \n",
        "                      index=False)\n",
        "excel_file.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z6CvDotmaAy"
      },
      "source": [
        "# Optimizing a dataset for memory usage <a name=\"11\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqa8EBXKYrmW"
      },
      "source": [
        "Pandas provides data structures for **in-memory** analytics, which makes using pandas to analyze large datasets somewhat tricky. In this section, we'll see a few recommendations for **improving the speed and efficiency of our analysis to larger datasets**.\n",
        "\n",
        "**Load less data**\n",
        "\n",
        "When we use the `read_csv` function, we can specify `usecols` to limit the columns to read into memory. When we use the `read_parquet` method we can use the `columns` parameter, etc. Not all file formats that can be read by pandas provide an option to read a subset of columns.\n",
        "\n",
        "**Efficient datatypes**\n",
        "\n",
        "The default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values. By using more efficient data types, we can reduce memory usage considerably in large datasets.\n",
        "\n",
        "- We can use the [`astype`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) method to convert the values of a Pandas object to a different data type. The argument to the method represents the data type to convert to.\n",
        "\n",
        " - For example, categorical columns are imported by pandas with the data type `object`, which is the library's internal lingo for strings. If a column is fundamentally a collection of Booleans, but it was imported as an `object` data type, we can convert it to boolean.\n",
        "\n",
        " - Pandas includes a special data type called `category`, which is ideal when a categorical column includes a small number of unique values relative to its total size. Behind the scenes, categorical values are stored as plain Python objects instead of NumPy ndarrays and are optimized for speed.\n",
        "\n",
        "- We can downcast the numeric columns to their smallest types using the [`to_numeric`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html#pandas.to_numeric) function.\n",
        "\n",
        "\n",
        "**Other ways to work with large datasets that don't fit in memory**\n",
        "\n",
        "- [Chunking](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#use-chunking).\n",
        "\n",
        "- [Use of other libraries](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#use-other-libraries) like [Dask](https://docs.dask.org/en/latest/), a library for parallel computing in Python, which has [dask.dataframe](https://docs.dask.org/en/latest/dataframe.html), a pandas-like API for working with larger datasets in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kszar-5aTg_H"
      },
      "source": [
        "# MultiIndex DataFrames <a name=\"12\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLrHKpIdTnx9"
      },
      "source": [
        "We've explored the 1-dimensional Series and the 2-dimensional DataFrame objects. Pandas provides support for datasets with any number of dimensions through the use of a `MultiIndex`. A `MultiIndex` is a **special index** object that consists of **multiple levels**. A DataFrame can hold a MultiIndex **in the row index, the column index, or both**. The introduction of layers to an index adds complexity but also versatility in the way that a dataset can be sliced and diced.\n",
        "\n",
        "With Pandas, there are many combinations in the ways we can shape the dataset for analysis. When defining the indexes of a DataFrame, we ask ourselves which values matter most to our problem. What is the key information? Could several pieces of data be tied together? Which data would we like to see stored as rows versus columns? Are there rows or columns that can be grouped into categories? For some problems, a MultiIndex can provide an effective solution for storing our data.\n",
        "\n",
        "The Series and DataFrame objects can hold **indices** with different **data types: strings, numbers**, intervals, datetimes, etc. But these options are limited cause their data types **can only store one value per index position**. A **tuple** doesn't have that limitation. Imagine a list of tuples serving as the row index of a DataFrame. Instead of being referenced by a simple data type like a string, **each row may be referenced by a tuple holding multiple elements within in**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x08le47gTs_d"
      },
      "source": [
        "## The `MultiIndex` object <a name=\"12.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grT546h0UFgg"
      },
      "source": [
        "We can create a `MultiIndex` object with the `from_tuples` class method. It instantiates a `MultiIndex` from a list of tuples. The index positions usually represent an idea or value. For our example, the values at index position 0 represent a street address and the values at index position 1 represent a city. We can assign a name to each of these levels by passing a list to the `names` parameter. We can then attach the MultiIndex object to a DataFrame via the `index` parameter in the DataFrame constructor. We get a DataFrame with a MultiIndex for its rows. Each row's label is a tuple holding two values: a street and a city.\n",
        "\n",
        "Pandas stores the column headers of a DataFrame in an index object as well. A DataFrame's column values can also be stored in a MultiIndex. In the next example shown, we create another MultiIndex (`column_index`), but this time, passing a list of four tuples, each holding two values. Our goal is for each one of these tuples to represent a column in the DataFrame. Finally, we create our DataFrame. In the constructor, we assign the `index` parameter the `my_index` variable holding our row's MultiIndex, and the `columns` parameter the `column_index` MultiIndex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKiMEB8nUHxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c980320-2176-4732-c465-7837b1026a03"
      },
      "source": [
        "addresses = [(\"Abbey Road\", \"London\"), (\"Anita Street\", \"Manchester\")]\n",
        "my_index = pd.MultiIndex.from_tuples(tuples=addresses, names=[\"Street\", \"City\"])\n",
        "my_dataframe = pd.DataFrame(data=[[\"A\", \"B+\"], [\"D-\", \"F\"]], index=my_index,\n",
        "                            columns=[\"Schools\", \"Cost of living\"])\n",
        "print(f'DataFrame with a MultiIndex in the row index: \\n{my_dataframe}\\n')\n",
        "\n",
        "column_index = pd.MultiIndex.from_tuples([(\"Culture\", \"Restaurants\"),\n",
        "                                          (\"Culture\", \"Museums\"),\n",
        "                                          (\"Services\", \"Police\"),\n",
        "                                          (\"Services\", \"Schools\")])\n",
        "my_dataframe = pd.DataFrame(data=[[\"A\", \"B+\", \"C+\", \"D\"],[\"D-\", \"F\", \"B\", \"C\"]],\n",
        "                            index=my_index, columns=column_index)\n",
        "print('DataFrame with a MultiIndex in the row index and in the column index:\\n'\n",
        "      f'{my_dataframe}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame with a MultiIndex in the row index: \n",
            "                        Schools Cost of living\n",
            "Street       City                             \n",
            "Abbey Road   London           A             B+\n",
            "Anita Street Manchester      D-              F\n",
            "\n",
            "DataFrame with a MultiIndex in the row index and in the column index:\n",
            "                            Culture         Services        \n",
            "                        Restaurants Museums   Police Schools\n",
            "Street       City                                           \n",
            "Abbey Road   London               A      B+       C+       D\n",
            "Anita Street Manchester          D-       F        B       C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR0hqwOFUcbm"
      },
      "source": [
        "**Reading a DataFrame with MultiIndex**\n",
        "\n",
        "The [neighborhoods.csv](https://github.com/paskhaver/pandas-in-action/blob/master/chapter_07_multiindex_dataFrames/neighborhoods.csv) file holds a dataset of addresses across cities. Each address is assigned several grades based on four characteristics: Restaurants, Museums, Police, and Schools. These characters are grouped under two parent categories: Culture and Services. This is one example of an optimal use case for a MultiIndex. When we have a parent category that encompasses several smaller child categories, creating a MultiIndex is an optimal way to enable quick slicing. Here's an output of the first couple of rows from the raw CSV file.\n",
        "\n",
        "```\n",
        ",,,Culture,Culture,Services,Services\n",
        ",,,Restaurants,Museums,Police,Schools\n",
        "State,City,Street,,,,\n",
        "MO,Fisherborough,244 Tracy View,C+,F,D-,A+\n",
        "```\n",
        "How will this CSV file's data be imported by Pandas? Let's find out with the `read_csv` method.\n",
        "\n",
        "- We have to tell Pandas that the three leftmost columns should serve as the index of the DataFrame. We pass the `index_col` parameter a list of numbers representing the numeric position of the columns (in the CSV file) that should be in the DataFrame's index. When `index_col` is passed a list with multiple values, Pandas creates a MultiIndex for the DataFrame.\n",
        "\n",
        "- We also need to tell Pandas which rows from the dataset we want to use for our DataFrame's headers. The `read_csv` method assumes that only the first row will hold the headers but, in this case, it’s the first two. The `header` parameter accepts a list of integers representing the rows (in the CSV file) whose values should be used as column headers. If we provide a list with multiple values, Pandas will assign a MultiIndex to the columns as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q5hM7bSUmPu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "0fd8ccda-dbbf-4d7b-e278-f4c618f9eefe"
      },
      "source": [
        "dataset_path = ('https://raw.githubusercontent.com/paskhaver/pandas-in-action/'\n",
        "                'master/chapter_07_multiindex_dataFrames/neighborhoods.csv')\n",
        "neighborhoods = pd.read_csv(dataset_path, index_col = [0, 1, 2], header = [0, 1])\n",
        "neighborhoods.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">Culture</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Services</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Restaurants</th>\n",
              "      <th>Museums</th>\n",
              "      <th>Police</th>\n",
              "      <th>Schools</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>Street</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MO</th>\n",
              "      <th>Fisherborough</th>\n",
              "      <th>244 Tracy View</th>\n",
              "      <td>C+</td>\n",
              "      <td>F</td>\n",
              "      <td>D-</td>\n",
              "      <td>A+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <th>Port Curtisville</th>\n",
              "      <th>446 Cynthia Inlet</th>\n",
              "      <td>C-</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>D+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WV</th>\n",
              "      <th>Jimenezview</th>\n",
              "      <th>432 John Common</th>\n",
              "      <td>A</td>\n",
              "      <td>A+</td>\n",
              "      <td>F</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AK</th>\n",
              "      <th>Stevenshire</th>\n",
              "      <th>238 Andrew Rue</th>\n",
              "      <td>D-</td>\n",
              "      <td>A</td>\n",
              "      <td>A-</td>\n",
              "      <td>A-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ND</th>\n",
              "      <th>New Joshuaport</th>\n",
              "      <th>877 Walter Neck</th>\n",
              "      <td>D+</td>\n",
              "      <td>C-</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Culture         Services        \n",
              "                                         Restaurants Museums   Police Schools\n",
              "State City             Street                                                \n",
              "MO    Fisherborough    244 Tracy View             C+       F       D-      A+\n",
              "SD    Port Curtisville 446 Cynthia Inlet          C-       B        B      D+\n",
              "WV    Jimenezview      432 John Common             A      A+        F       B\n",
              "AK    Stevenshire      238 Andrew Rue             D-       A       A-      A-\n",
              "ND    New Joshuaport   877 Walter Neck            D+      C-        B       B"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kna_e4pfUtTX"
      },
      "source": [
        "**Exploring DataFrame with MultiIndex**\n",
        "\n",
        "- In the output of the DataFrame's method `info` we can see that each column's name is now stored as a tuple of two values. Similarly, each row's label is a tuple made of three values. We can access the MultiIndex objects through the `index` and `columns` DataFrame's attributes. \n",
        "\n",
        "- A MultiIndex is composed of multiple Index objects. Each Index in our row MultiIndex was assigned a name from a CSV header value. We can access the complete list through the MultiIndex's attribute `names`.\n",
        "\n",
        " Each nestled `Index` is assigned a position representing its place in the MultiIndex. The MultiIndex's method `get_level_values` extracts an Index object at any layer of the MultiIndex. We can pass its parameter `level` the position or the name of the level to pull out. The Index object includes a `unique` method that returns a new Index containing no duplicates.\n",
        "\n",
        " Our columns index doesn't have names because there were no names provided for them in the CSV file. We can assign a new list of names directly to the `names` attribute of the MultiIndex object stored in the `columns` attribute. We name \"Category\" and \"Subcategory\". The level names can also be changed via the `set_names` method on the index object (it can target a specific level or levels).\n",
        "\n",
        "- The DataFrame's `nunique` method returns a Series with a count of unique values per column. The DataFrame's column MultiIndex will serve as the row's MultiIndex in the resulting Series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLZq-lR5UtbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "662b30f3-4a53-4d05-b0d0-aec1170e0d27"
      },
      "source": [
        "print(f'Names of row indices: {neighborhoods.index.names}')\n",
        "neighborhoods.index.get_level_values(1).unique()\n",
        "neighborhoods.columns.names = [\"Category\", \"Subcategory\"]\n",
        "print(f'Names of columns indices: {neighborhoods.columns.names}')\n",
        "print(f'Unique values per column:\\n{neighborhoods.nunique()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Names of row indices: ['State', 'City', 'Street']\n",
            "Names of columns indices: ['Category', 'Subcategory']\n",
            "Unique values per column:\n",
            "Category  Subcategory\n",
            "Culture   Restaurants    13\n",
            "          Museums        13\n",
            "Services  Police         13\n",
            "          Schools        13\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGeSpfomVMF_"
      },
      "source": [
        "**Sorting A MultiIndex**\n",
        "\n",
        "As we've already seen, the `sort_index` method sorts the index of a Pandas data structure. When invoked on a DataFrame with a MultiIndex, by default, it returns a new DataFrame with all levels sorted in ascending order (A-Z or smallest to largest). In the example shown, the values in the outermost level (State) are sorted first, followed by the values in the City level, and then finally the values in the Street level.\n",
        "\n",
        "- We can pass the parameter `ascending=False` to apply a descending order to all levels of the MultiIndex. If we want to vary up the sort order for different levels, the `ascending` parameter accepts a list of Booleans. Each Boolean represents the sort order for the level in the MultiIndex, starting with the outermost one and proceeding inwards. If the length of the list passed to the ascending parameter is less than the number of levels in the MultiIndex, then any remaining levels will keep their original sort order. For example, an argument of `[False, False]` will sort the State and City levels in descending order, and the Street level in ascending order.\n",
        "\n",
        " We can also sort an individual level of the MultiIndex while ignoring the others. Let's say we want to sort the index by the values of the second level, City. We can pass the level's index position or its name to the `level` parameter. The `level` parameter also accepts a list of levels. The `ascending` parameter can be paired with the `level` parameter.\n",
        "\n",
        "- The column's MultiIndex can be sorted as well by setting the parameter `axis=1`. The `level` and `ascending` parameters can be combined with the `axis` parameter to customize the order of the column sorts.\n",
        "\n",
        "- The `sort_index` method includes the `inplace` parameter to mutate the existing DataFrame instead of returning a new one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFq4dkJlVP1c"
      },
      "source": [
        "neighborhoods.sort_index()\n",
        "neighborhoods.sort_index(ascending = [False, False])\n",
        "neighborhoods.sort_index(ascending = True, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1arV5dT5ViXg"
      },
      "source": [
        "## Indexing with a MultiIndex <a name=\"12.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEPOaKogVp25"
      },
      "source": [
        "In this section, we'll look at how we can extract rows and columns from a MultiIndex DataFrame.Things get especially tricky when extra dimensions are involved. As mentioned earlier, it's optimal to have our index sorted before looking up any row. \n",
        "\n",
        "**Extracting columns**\n",
        "\n",
        "Each of the four columns in our neighborhoods DataFrame requires two identifiers: a Category and a Subcategory.\n",
        "\n",
        "- If we pass a single value within square brackets, Pandas will look for it in the outermost level of the column MultiIndex. The example shown searches for \"Services\", which is a valid value in the Category level. The column index in the returned DataFrame is just a plain Index object with two values, \"Police\" and \"Schools\". These two columns represent subcategories that both fall under the singular \"Services\" category. Pandas will raise a KeyError exception if the value passed in square brackets does not exist in the outermost level of the column MultiIndex.\n",
        "\n",
        "- To specify values across multiple levels in the column's MultiIndex, we pass them inside a tuple. The example shown targets a value of \"Services\" in the Category level and a value of \"Schools\" in the Subcategory level. The return value is a Series.\n",
        "\n",
        "- To extract multiple columns, we need to pass the square brackets a list of tuples. Each tuple should include the level values needed to pull out one column. The order of tuples within the list determines the order of columns in the resulting DataFrame. An example shown pulls out two columns from the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8qwGQbVrRM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "1e4939da-bb62-474d-ff44-1ba6585b733c"
      },
      "source": [
        "neighborhoods[\"Services\"].head(2)\n",
        "neighborhoods[(\"Services\", \"Schools\")].head(2)\n",
        "neighborhoods[[(\"Services\", \"Schools\"), (\"Culture\", \"Museums\")]].head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Services</th>\n",
              "      <th>Culture</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Subcategory</th>\n",
              "      <th>Schools</th>\n",
              "      <th>Museums</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>Street</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">AK</th>\n",
              "      <th>Rowlandchester</th>\n",
              "      <th>386 Rebecca Cove</th>\n",
              "      <td>C</td>\n",
              "      <td>A-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Scottstad</th>\n",
              "      <th>082 Leblanc Freeway</th>\n",
              "      <td>B+</td>\n",
              "      <td>C-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Category                                 Services Culture\n",
              "Subcategory                               Schools Museums\n",
              "State City           Street                              \n",
              "AK    Rowlandchester 386 Rebecca Cove           C      A-\n",
              "      Scottstad      082 Leblanc Freeway       B+      C-"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNzeSNQNV2YL"
      },
      "source": [
        "**Extracting rows with `loc`**\n",
        "\n",
        "We've already introduced the `loc` and `iloc` attribute accessors for selecting rows and columns from a DataFrame (in section \"Data indexing\"). The `loc` accessor extracts a row, column or value by index label.\n",
        "\n",
        "- If we know the exact values we want to target in each level of the MultiIndex, we can pass them in a tuple within the square brackets. The example shown provides values for the State, City, and Address layers. The final result is a Series object with an index constructed from the column values of the targeted DataFrame row.\n",
        "\n",
        "- If we pass a single argument to `loc`, Pandas will look for the label in the outermost level of the MultiIndex.\n",
        "\n",
        "- Pandas recommends using the first argument to `loc` for the row index and the second argument for the column index by wrapping all arguments for a given index inside a single tuple. An example shown access the rows with a State value of \"CA\" and a City value of \"Dustinmouth\", showing the Services columns.\n",
        "\n",
        "- We can use Python's list slicing syntax for selecting sequential values. An example shown pulls all consecutive rows starting from those with a State value of \"NE\" and ending with those with a State value of \"NH\".\n",
        "\n",
        " List slicing syntax can be applied to tuples passed to the `loc` accessor. An example shown extracts all consecutive rows starting from a value of \"NE\" in the State level and \"Shawnchester\" in the City level and ending with a value of \"NH\" in the State level and \"North Latoya\" in the City level.\n",
        "\n",
        " Partial slicing can be used with only one of the endpoints. An example shown does not include a City level value for the second tuple, only a State value. Thus, Pandas pulls the rows from (\"NE\", \"Shawnchester\") until the end of all rows with a State value of \"NH\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ZGlGofV8qn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c315dc99-1c43-484e-9ace-f06b652a3bfd"
      },
      "source": [
        "neighborhoods.loc[(\"TX\", \"Kingchester\", \"534 Gordon Falls\")]\n",
        "neighborhoods.loc[\"CA\"]\n",
        "neighborhoods.loc[(\"CA\", \"Dustinmouth\"), (\"Services\")]\n",
        "neighborhoods[\"NE\":\"NH\"]\n",
        "neighborhoods.loc[(\"NE\", \"Shawnchester\"):(\"NH\", \"North Latoya\")]\n",
        "neighborhoods.loc[(\"NE\", \"Shawnchester\"):(\"NH\")]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Culture</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Services</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Subcategory</th>\n",
              "      <th>Restaurants</th>\n",
              "      <th>Museums</th>\n",
              "      <th>Police</th>\n",
              "      <th>Schools</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>Street</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">NE</th>\n",
              "      <th>Shawnchester</th>\n",
              "      <th>802 Cook Cliff</th>\n",
              "      <td>D-</td>\n",
              "      <td>D+</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>South Kennethmouth</th>\n",
              "      <th>346 Wallace Pass</th>\n",
              "      <td>C-</td>\n",
              "      <td>B-</td>\n",
              "      <td>A</td>\n",
              "      <td>A-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>South Nathan</th>\n",
              "      <th>821 Jake Fork</th>\n",
              "      <td>C+</td>\n",
              "      <td>D</td>\n",
              "      <td>D+</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">NH</th>\n",
              "      <th>Courtneyfort</th>\n",
              "      <th>697 Spencer Isle</th>\n",
              "      <td>A+</td>\n",
              "      <td>A+</td>\n",
              "      <td>C+</td>\n",
              "      <td>A+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>East Deborahberg</th>\n",
              "      <th>271 Ryan Mount</th>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>D+</td>\n",
              "      <td>B-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ingramton</th>\n",
              "      <th>430 Calvin Underpass</th>\n",
              "      <td>C+</td>\n",
              "      <td>D+</td>\n",
              "      <td>C</td>\n",
              "      <td>C-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>North Latoya</th>\n",
              "      <th>603 Clark Mount</th>\n",
              "      <td>D-</td>\n",
              "      <td>A-</td>\n",
              "      <td>B+</td>\n",
              "      <td>B-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>South Tara</th>\n",
              "      <th>559 Michael Glens</th>\n",
              "      <td>C-</td>\n",
              "      <td>C-</td>\n",
              "      <td>F</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Category                                          Culture  ... Services\n",
              "Subcategory                                   Restaurants  ...  Schools\n",
              "State City               Street                            ...         \n",
              "NE    Shawnchester       802 Cook Cliff                D-  ...        A\n",
              "      South Kennethmouth 346 Wallace Pass              C-  ...       A-\n",
              "      South Nathan       821 Jake Fork                 C+  ...        A\n",
              "NH    Courtneyfort       697 Spencer Isle              A+  ...       A+\n",
              "      East Deborahberg   271 Ryan Mount                 B  ...       B-\n",
              "      Ingramton          430 Calvin Underpass          C+  ...       C-\n",
              "      North Latoya       603 Clark Mount               D-  ...       B-\n",
              "      South Tara         559 Michael Glens             C-  ...        B\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoOGg8GMWVJu"
      },
      "source": [
        "**Extracting rows with `iloc`**\n",
        "\n",
        "The `iloc` accessor attribute extracts a row, column, or value by index position rather than by label.\n",
        "\n",
        "- We can pass in a single index position to `iloc` to get a row. We can extract multiple rows by wrapping their index positions in a list.\n",
        "\n",
        "- We can pass two arguments to `iloc` to pull out the value at the intersection of a specific row and a specific column. An example shown targets the row with index position 25 and the column with index position 2.\n",
        "\n",
        "- We can use Python's list slicing syntax for selecting sequential values. Negative slicing is also permitted. An example shown pulls rows starting from the fourth-to-last row and the columns starting from the second-to-last column.\n",
        "\n",
        "- We cannot index across consecutive MultiIndex levels with `iloc`. `iloc` serves as a \"strict positional indexer\" that \"does not regard the structure of the DataFrame\" ([reference](https://github.com/pandas-dev/pandas/issues/15228)). It is each row in the dataset that is assigned a numeric order in sequence, not each value in a given level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Bp3R8kWVRn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "979c1dbe-2f06-4d89-f04a-ca24a58ea2f2"
      },
      "source": [
        "neighborhoods.iloc[25]\n",
        "neighborhoods.iloc[[25, 30]]\n",
        "neighborhoods.iloc[25, 2]\n",
        "neighborhoods.iloc[25:30, 1:3]\n",
        "neighborhoods.iloc[-4:, -2:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Services</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Subcategory</th>\n",
              "      <th>Police</th>\n",
              "      <th>Schools</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>Street</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">WY</th>\n",
              "      <th>Lake Nicole</th>\n",
              "      <th>933 Jennifer Burg</th>\n",
              "      <td>A-</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Martintown</th>\n",
              "      <th>013 Bell Mills</th>\n",
              "      <td>A-</td>\n",
              "      <td>B-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Port Jason</th>\n",
              "      <th>624 Faulkner Orchard</th>\n",
              "      <td>C+</td>\n",
              "      <td>C+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Reneeshire</th>\n",
              "      <th>717 Patel Square</th>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Category                               Services        \n",
              "Subcategory                              Police Schools\n",
              "State City        Street                               \n",
              "WY    Lake Nicole 933 Jennifer Burg          A-       C\n",
              "      Martintown  013 Bell Mills             A-      B-\n",
              "      Port Jason  624 Faulkner Orchard       C+      C+\n",
              "      Reneeshire  717 Patel Square            D       A"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fO1ChVxWmkE"
      },
      "source": [
        "**Cross sections**\n",
        "\n",
        "The `xs` method creates a cross-section, which allows us to extract rows with a value in one level irrespective of the values in other levels. The `key` parameter accepts the value to look for and the `level` parameter accepts either the numeric index or the name of the level in which to look for it. In the example shown, we find all addresses in the city of \"Lake Nicole\", regardless of the state or street.\n",
        "\n",
        "- We can apply the same extraction technique to columns by passing the parameter `axis=\"columns\"`. The example shown selects the columns that have a key of \"Museums\" in the Subcategory level of the column MultiIndex.\n",
        "\n",
        "- We can also provide multiple keys across non-consecutive levels of the MultiIndex. In an example shown, we pull out the row with a street address of \"238 Andrew Rue\" and a state of \"AK\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPJl2E5Wmta"
      },
      "source": [
        "neighborhoods.xs(key=\"Lake Nicole\", level=\"City\")\n",
        "neighborhoods.xs(axis=\"columns\", key=\"Museums\", level=\"Subcategory\").head()\n",
        "neighborhoods.xs(key=(\"AK\", \"238 Andrew Rue\"), level=[\"State\", \"Street\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b8ZxHJHWvuj"
      },
      "source": [
        "## Manipulating the index <a name=\"12.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-jc4XzUW2gQ"
      },
      "source": [
        "We've contorted our neighborhoods dataset into its current shape by altering the parameters to the `read_csv` method. Pandas also allows us to manipulate and set the index on an existing DataFrame. \n",
        "\n",
        "**Resetting the Index**\n",
        "\n",
        "The neighborhoods DataFrame currently has \"State\" as its outermost MultiIndex level followed by \"City\".\n",
        "\n",
        "- The `reorder_levels` method returns a new DataFrame with the MultiIndex levels arranged in a specified order. We can pass its `order` parameter a list of levels in their desired order or a list of integers (the numbers must represent the current index positions of the levels within the MultiIndex).\n",
        "\n",
        "- The `reset_index` method returns a new DataFrame that reintegrates the MultiIndex levels as columns. The standard numeric index is used in its place. The new columns will become values in the outermost level of the column's MultiIndex (Category in our DataFrame) assigning empty strings to lower layers if they exist (Subcategory in our DataFrame).\n",
        "\n",
        " We can add the three columns to an alternate MultiIndex level by passing that level's index position or name to the `col_level` parameter. In the example shown, we integrate the three columns into the Subcategory level instead. Pandas will now default to an empty string for Category (the parent level that holds the Subcategory level). We can replace the empty string with a value of our choice by passing an argument the `col_fill` parameter. In the example shown, we group the three new columns under an Address parent. Now, the outer Category level holds three values: Address, Culture, and Services.\n",
        "\n",
        " The standard invocation of `reset_index` transforms all index levels into columns. We can also move a subset of the levels by passing one (a string, the level name) or more names (a list with the level names) to the `levels` parameter.\n",
        "\n",
        " If we pass the parameter `drop=True`, it will delete the specified level instead of adding it amongst the columns.\n",
        "\n",
        "- For further exploration in the next section, we finally make our index reset permanent with the `inplace` parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8kM_W_CW2q2"
      },
      "source": [
        "neighborhoods.reorder_levels(order=[\"City\", \"State\", \"Street\"]).head(2)\n",
        "neighborhoods.reset_index().head(2)\n",
        "neighborhoods.reset_index(col_level=\"Subcategory\").head(2)\n",
        "neighborhoods.reset_index(col_fill=\"Address\", col_level=\"Subcategory\").head(2)\n",
        "neighborhoods.reset_index(level = \"Street\").head(2)\n",
        "neighborhoods.reset_index(level = [\"Street\", \"City\"]).head(2)\n",
        "neighborhoods.reset_index(level = \"Street\", drop = True).tail()\n",
        "neighborhoods.reset_index(inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGkX9c_eXKAD"
      },
      "source": [
        "**Setting the Index**\n",
        "\n",
        "We now have 7 columns in our DataFrame with a single-level Index for the rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "eX93SV9H3BjR",
        "outputId": "6aa862ff-aa5c-465b-d659-336f0475f52e"
      },
      "source": [
        "neighborhoods.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>Category</th>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>Street</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Culture</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Services</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Subcategory</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Restaurants</th>\n",
              "      <th>Museums</th>\n",
              "      <th>Police</th>\n",
              "      <th>Schools</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AK</td>\n",
              "      <td>Rowlandchester</td>\n",
              "      <td>386 Rebecca Cove</td>\n",
              "      <td>C-</td>\n",
              "      <td>A-</td>\n",
              "      <td>A+</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AK</td>\n",
              "      <td>Scottstad</td>\n",
              "      <td>082 Leblanc Freeway</td>\n",
              "      <td>D</td>\n",
              "      <td>C-</td>\n",
              "      <td>D</td>\n",
              "      <td>B+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Category    State            City               Street  ... Culture Services        \n",
              "Subcategory                                             ... Museums   Police Schools\n",
              "0              AK  Rowlandchester     386 Rebecca Cove  ...      A-       A+       C\n",
              "1              AK       Scottstad  082 Leblanc Freeway  ...      C-        D      B+\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI3VaE5z3CUe"
      },
      "source": [
        "The `set_index` method returns a new DataFrame with one or more columns from the dataset used as the new index. We can pass to the `keys` parameter the column name we want to set as the index. To have multiple columns serve as the index, we can pass them inside a list.\n",
        "\n",
        "If we want to use one of the four columns on the right side of the DataFrame as the new index, the `keys` parameter must be passed a tuple containing the values at each level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DksbZ7WiXNM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "f7829904-c938-4145-9339-d7127718ec2d"
      },
      "source": [
        "neighborhoods.set_index(keys = \"City\").head()\n",
        "neighborhoods.set_index(keys = [\"State\", \"City\"]).head()\n",
        "neighborhoods.set_index(keys = (\"Culture\", \"Museums\")).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>Category</th>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>Street</th>\n",
              "      <th>Culture</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Services</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Subcategory</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Restaurants</th>\n",
              "      <th>Police</th>\n",
              "      <th>Schools</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(Culture, Museums)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A-</th>\n",
              "      <td>AK</td>\n",
              "      <td>Rowlandchester</td>\n",
              "      <td>386 Rebecca Cove</td>\n",
              "      <td>C-</td>\n",
              "      <td>A+</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C-</th>\n",
              "      <td>AK</td>\n",
              "      <td>Scottstad</td>\n",
              "      <td>082 Leblanc Freeway</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>B+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D-</th>\n",
              "      <td>AK</td>\n",
              "      <td>Scottstad</td>\n",
              "      <td>114 Jones Garden</td>\n",
              "      <td>D-</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>AK</td>\n",
              "      <td>Stevenshire</td>\n",
              "      <td>238 Andrew Rue</td>\n",
              "      <td>D-</td>\n",
              "      <td>A-</td>\n",
              "      <td>A-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F</th>\n",
              "      <td>AL</td>\n",
              "      <td>Clarkland</td>\n",
              "      <td>430 Douglas Mission</td>\n",
              "      <td>A</td>\n",
              "      <td>C+</td>\n",
              "      <td>B+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Category           State            City  ... Services        \n",
              "Subcategory                               ...   Police Schools\n",
              "(Culture, Museums)                        ...                 \n",
              "A-                    AK  Rowlandchester  ...       A+       C\n",
              "C-                    AK       Scottstad  ...        D      B+\n",
              "D-                    AK       Scottstad  ...        D       D\n",
              "A                     AK     Stevenshire  ...       A-      A-\n",
              "F                     AL       Clarkland  ...       C+      B+\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUkOQetezYbN"
      },
      "source": [
        "# References and further reading <a name=\"13\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfK3eA7v2i1Q"
      },
      "source": [
        "- [Pandas user guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)\n",
        "\n",
        "- [Pandas: Merge, join, concatenate and compare](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
        "\n",
        "- [Speed up operations on pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html)\n",
        "\n",
        "- [Pandas in action](https://www.manning.com/books/pandas-in-action). [repo](https://github.com/paskhaver/pandas-in-action)"
      ]
    }
  ]
}