{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML metrics and cost functions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7koB8ZKgrBV0",
        "QT32HhKCQ6WF",
        "cXWQk7nKQyC2",
        "Sl0uvuGIHoXL",
        "yGsH-712hXLk",
        "Q49dTkr3g3pd",
        "B7icoN8ruZni",
        "v9nbSPkR13n_",
        "_EHFHMbrKk1U",
        "oAPuyyGH5xOm",
        "HRe02pKHl4BD"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzKwp/ShQROurCv8b3v9+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Machine-Learning-Python/blob/master/ML_metrics_and_cost_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNqFbXrEop9S"
      },
      "source": [
        "## Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOunBijSo16s"
      },
      "source": [
        "\n",
        "1. [Introduction](#1)\n",
        "2. [Notation](#2)\n",
        "3. [Imports](#3)\n",
        "4. [Regression](#4)\n",
        "    1. [Metrics](#4.1)\n",
        "        1. [Mean squared error (MSE)](#4.1.1)\n",
        "        2. [Mean absolute error (MAE)](#4.1.2)\n",
        "        3. [Max error](#4.1.3)\n",
        "        4. [Mean squared logarithmic error (MSLE)](#4.1.4)\n",
        "        5. [Mean absolute percentage error (MAPE)](#4.1.5)\n",
        "        6. [Median absolute erro (MedAE)](#4.1.6)\n",
        "        7. [Explained variance](#4.1.7)\n",
        "        8. [Coefficient of determination (R²)](#4.1.8)\n",
        "    2. [Cost functions](#4.2)\n",
        "        1. [Huber](#4.2.1)\n",
        "        2. [Logcosh](#4.2.2)\n",
        "5. [Classification](#5)\n",
        "    1. [Metrics](#5.1)\n",
        "        1. [Cofusion matrix](#5.1.1)\n",
        "        2. [Accuracy](#5.1.2)\n",
        "        3. [Recall](#5.1.3)\n",
        "        4. [Precision](#5.1.4)\n",
        "        5. [F1-score](#5.1.5)\n",
        "        6. [Classification report](#5.1.6)\n",
        "        6. [ROC curve](#5.1.7)\n",
        "    2. [Cost functions](#5.2)\n",
        "        1. [Binary cross-entropy](#5.2.1)\n",
        "        2. [Categorical cross-entropy](#5.2.2)\n",
        "        3. [Hamming loss](#5.2.3)\n",
        "    \n",
        "6. [References](#6)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7koB8ZKgrBV0"
      },
      "source": [
        "# Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnChcoPcO9_r"
      },
      "source": [
        "In ML, losses and metrics are conceptually not the same things: \n",
        "\n",
        "- **Losses** are used by the algorithms (like Gradient Descent) and **optimized** during the training of the model, so they usually are differentiable (at least where they are evaluated) and their gradients are not 0 everywhere. Plus, it’s okay if they are **not easily interpretable** by humans (e.g. cross-entropy). The loss function usually refers to the error for a single training example while the cost function usually refers to the average of the loss functions of the entire set. However, some people use these terms interchangeably.\n",
        "\n",
        "- In contrast, **metrics** are used to **evaluate a model**, they must be more **easily interpretable**, and they can be non-differentiable or have 0 gradients everywhere (e.g., accuracy).\n",
        "\n",
        "That said, in most cases, defining a metric function is exactly the same as defining a loss function. For example, it's common for regression tasks, the usage of the mean square error (MSE) for both metric and loss function.\n",
        "\n",
        "In this notebook, we will dive into the most common metrics and cost functions used in ML, particularly, for both regression and classification supervised learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT32HhKCQ6WF"
      },
      "source": [
        "# Notation <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH6r5zWoQ7zz"
      },
      "source": [
        "- $\\boldsymbol{X}$ is the matrix of data where the columns represent the independent variables $\\boldsymbol{x}_1,...,\\boldsymbol{x}_p$, and the rows represent the observations $\\boldsymbol{x}^{(1)},...,\\boldsymbol{x}^{(m)}$.\n",
        "\n",
        "$$\n",
        "\\boldsymbol{X}=\n",
        "  \\begin{bmatrix}\n",
        "    x_1^{(1)} & x_2^{(1)} .& . & . & x_p^{(1)} \\\\\n",
        "    . & . & . & . & . \\\\\n",
        "    . & . & . & . & . \\\\\n",
        "    . & . & . & . & . \\\\\n",
        "    x_1^{(m)} & x_2^{(m)} .& . & . & x_p^{(m)}\n",
        "  \\end{bmatrix}\n",
        "=\n",
        "  \\begin{bmatrix}\n",
        "    \\boldsymbol{x}_1 & \\boldsymbol{x}_2 & . & . & \\boldsymbol{x}_p\n",
        "  \\end{bmatrix}\n",
        "=\n",
        "  \\begin{bmatrix}\n",
        "    \\boldsymbol{x}^{(1)} \\\\\n",
        "    \\boldsymbol{x}^{(2)} \\\\\n",
        "    . &\\\\\n",
        "    . & \\\\\n",
        "    \\boldsymbol{x}^{(m)}\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- $\\boldsymbol{y}=(y^{(1)} ... y^{(m)})^T$ is the target variable (continuous or discrete/categorical)\n",
        "\n",
        "- $\\hat{\\boldsymbol{y}}=(\\hat{y}^{(1)} ... \\hat{y}^{(m)})^T$ is the predicted variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXWQk7nKQyC2"
      },
      "source": [
        "# Imports <a name=\"3\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0KnMHKHQz9b"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvYBBl6nXlAi"
      },
      "source": [
        "!pip install scikit-learn==0.24.1\n",
        "!pip install tensorflow-addons\n",
        "# You must restart the runtime in order to use newly installed versions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl0uvuGIHoXL"
      },
      "source": [
        "# Regression <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdDQx59zHsQs"
      },
      "source": [
        "In machine learning, regression algorithms attempt to estimate the mapping function ($f$) from the input variables ($\\boldsymbol{X}$) to numerical or continuous output variable ($\\boldsymbol{y}$), $\\hat{\\boldsymbol{y}}=f(\\boldsymbol{X})$.\n",
        "\n",
        "In this case, $\\boldsymbol{y}$ is a real value vector where their elements can be integers or floating-point values. \n",
        "\n",
        "For example, when provided with a dataset about houses, and the task is to predict their prices, that is a regression task because the price will be a continuous output (an example is available [here](https://nbviewer.jupyter.org/github/victorviro/ML_algorithms_python/blob/master/Introduction_linear_regression_and_regularized_linear_models.ipynb))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLx8GEoISofd"
      },
      "source": [
        "y_true = np.array([3, 0.5, 2, 7])\n",
        "y_pred = np.array([2.5, 0.0, 2, 8])\n",
        "m = y_true.shape[0]\n",
        "residuals = y_true - y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtBQIRX0Hsrv"
      },
      "source": [
        "## Metrics <a name=\"4.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WguhOq_oHt6_"
      },
      "source": [
        "### Mean Squared Error (MSE) <a name=\"4.1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HNeyUO8VISP"
      },
      "source": [
        "The mean squared error metric computes the expected value of the squared (quadratic) error loss or $l_2$-norm loss.\n",
        "\n",
        "$$\\text{MSE}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})= \\frac{1}{m}\\sum_{i=1}^{m} (e^{(i)})^2 =  \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
        "\n",
        "Due to the square, large errors are emphasized and have a relatively greater effect on the value of the performance metric (if $e^{(i)} > 1$). At the same time, the effect of relatively small errors ($e^{(i)} < 1$) will be even smaller.\n",
        "Sometimes this property of the squared error is referred to as penalizing extreme errors or being susceptible to outliers. \n",
        "\n",
        "The squared error has a unit measure of squared units of data. This may not be intuitive, e.g. squared dollars. This could be reversed by taking the square root.\n",
        "\n",
        "$$\\text{RMSE}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})=\\sqrt{\\text{MSE}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})}$$\n",
        "\n",
        "Squared error is acknowledged for its good mathematical properties. It is continuously differentiable which facilitates optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6IQoes7SkLP",
        "outputId": "a1d48771-e226-4621-f965-a2ba66dd8bd2"
      },
      "source": [
        "mse = np.average(residuals**2)\n",
        "print(f'Mean squared error: {mse}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared error: 0.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTKG2VckTwIb"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [mean_squared_error](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z_Cj13NSg6i",
        "outputId": "27a20435-4d27-490e-a546-a03105a8158f"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(f'Mean squared error: {mean_squared_error(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared error: 0.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhYeZGeZd1v6"
      },
      "source": [
        "In Tensorflow, this metric is implemented with the [MeanSquaredError](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredError) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb8aKJxqeFhV",
        "outputId": "017b29b5-bb0e-45b9-b06f-43451c482231"
      },
      "source": [
        "mse = tf.keras.metrics.MeanSquaredError()\n",
        "print(f'Mean squared error: {mse(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared error: 0.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxzFLZjFq0ZF"
      },
      "source": [
        "### Mean absolute error (MAE) <a name=\"4.1.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ZdJp1iq7xw"
      },
      "source": [
        "The mean absolute error (MAE) metric computes the expected value of the absolute error or $l_1$-norm loss.\n",
        "\n",
        "$$\\text{MAE}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})= \\frac{1}{m}\\sum_{i=1}^{m} |e^{(i)}| =  \\frac{1}{m}\\sum_{i=1}^{m}|\\hat{y}^{(i)} - y^{(i)}|$$\n",
        "\n",
        "The idea behind the absolute error is to avoid the mutual cancellation of the positive and negative errors. The absolute error has only non-negative values.\n",
        "\n",
        "Absolute error preserves the same units of measurement as the data under analysis and gives all individual errors the same weights (as compared to squared error)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZReA4ra5UIcL",
        "outputId": "48276d0a-49f5-4260-bb65-cd77a0cf5545"
      },
      "source": [
        "mae = np.average(np.abs(residuals))\n",
        "print(f'Mean absolute error: {mae}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute error: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO30VmDGUIcc"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [mean_absolute_error](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfEMBQDYUIcd",
        "outputId": "6ef40031-4d39-4117-fb74-4b205d51f86f"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "print(f'Mean absolute error: {mean_absolute_error(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute error: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2YYuXmbedJw"
      },
      "source": [
        "In Tensorflow, this metric is implemented with the [MeanAbsoluteError](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh7oY1a7edJy",
        "outputId": "b7ca1dc5-ee62-4b60-9326-22004942e405"
      },
      "source": [
        "mae = tf.keras.metrics.MeanAbsoluteError()\n",
        "print(f'Mean absolute error: {mae(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute error: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v8Xl1-QpksN"
      },
      "source": [
        "### Max error <a name=\"4.1.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarf0pqxpmDQ"
      },
      "source": [
        "The max error metric computes the maximum residual error. It captures the worst-case error between the predicted value and the true value. In a perfectly fitted single-output regression model, the max error would be 0 on the training set, and thought this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.\n",
        "\n",
        "$$\\text{Max Error}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}}) = max(| y^{(i)} - \\hat{y}^{(i)} |)$$\n",
        "\n",
        "where $i=1,...,m$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzAA4YjUVFEE",
        "outputId": "e462c7b9-b6a0-468f-bf1b-b8074ec595ce"
      },
      "source": [
        "max_error = max(np.abs(residuals))\n",
        "print(f'Max error: {max_error}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max error: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13iAbhlxU3W7"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [max_error](https://scikit-learn.org/stable/modules/model_evaluation.html#max-error) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgHDufbmVEbC",
        "outputId": "4dac61c7-1914-4e6d-c25a-153de6e33ef8"
      },
      "source": [
        "from sklearn.metrics import max_error\n",
        "\n",
        "print(f'Max error: {max_error(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max error: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR0syvDi0Irw"
      },
      "source": [
        "### Mean squared logarithmic error (MSLE) <a name=\"4.1.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn_CchP40Joy"
      },
      "source": [
        "The mean squared log error (MSLE) metric computes the expected value of the squared logarithmic (quadratic) error.\n",
        "\n",
        "$$\\text{MSLE}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})=  \\frac{1}{m}\\sum_{i=1}^{m}(\\log_e(1+y^{(i)})-\\log_e(1+\\hat{y}^{(i)}))^2$$\n",
        "\n",
        "This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over years, etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nxTlxoVB6uO",
        "outputId": "72c2ed4b-e441-435d-c398-7f8c1ed18226"
      },
      "source": [
        "msle = np.average((np.log1p(y_true) - np.log1p(y_pred))**2)\n",
        "print(f'Mean squared log error: {msle}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared log error: 0.0490263575494607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQq2LxbgViPe"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [mean_squared_log_error](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntVLmDYXVia5",
        "outputId": "2057fa72-9ca6-4794-c229-992b35e9c202"
      },
      "source": [
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "print(f'Mean squared log error: {mean_squared_log_error(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared log error: 0.0490263575494607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yifkFOq3fGOg"
      },
      "source": [
        "In Tensorflow, this metric is implemented with the [MeanSquaredLogarithmicError](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredLogarithmicError) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAgIOQR-fGOo",
        "outputId": "e24162c5-9bed-46b7-f123-fe0f2448c8f5"
      },
      "source": [
        "msle = tf.keras.metrics.MeanSquaredLogarithmicError()\n",
        "print(f'Mean squared log error: {msle(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared log error: 0.04902633652091026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3ZW_6sQ1Jag"
      },
      "source": [
        "###  Mean absolute percentage error (MAPE) <a name=\"4.1.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xXR8jVJ1NVq"
      },
      "source": [
        "The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is sensitive to relative errors. It is, for example, not changed by a global scaling of the target variable.\n",
        "\n",
        "$$\\text{MAPE}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}}) = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{{}\\left| y^{(i)} - \\hat{y}^{(i)} \\right|}{max(\\epsilon, \\left| y^{(i)} \\right|)}$$\n",
        "\n",
        "Absolute percentage error does not preserve the same units of measurement as the data under analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhihTbVsXMN-",
        "outputId": "d9c740d1-a62d-4cfc-eb75-6ecf64c6e11e"
      },
      "source": [
        "absolute_percentage_error= np.abs(residuals)/np.maximum(np.abs(y_true), 0.0001)\n",
        "mape = np.average(absolute_percentage_error)\n",
        "print(f'Mean absolute percentage error: {mape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute percentage error: 0.3273809523809524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRZqdylVXMZH"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [mean_absolute_percentage_error](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKnum5t7XMlq",
        "outputId": "47cd7032-a8d9-4165-a674-ee361f3c5bc5"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "print(f'Mean absolute percentage error: {mape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute percentage error: 0.3273809523809524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh4wcy2Oe5pD"
      },
      "source": [
        "In Tensorflow, this metric is implemented with the [MeanAbsolutePercentageError](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsolutePercentageError) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acFCLs3Ne5pE",
        "outputId": "74ee9f6e-b4b9-42c1-db1f-ea8e664d7b38"
      },
      "source": [
        "mape = tf.keras.metrics.MeanAbsolutePercentageError()\n",
        "\n",
        "print(f'Mean absolute percentage error: {mape(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean absolute percentage error: 32.738094329833984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI8FdjIJ1_2F"
      },
      "source": [
        "### Median absolute error (MedAE) <a name=\"4.1.6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyieWJ2w2Awh"
      },
      "source": [
        "The median absolute error (MedAE) is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.\n",
        "\n",
        "$$\\text{MedAE}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}}) = \\text{median}(\\mid y^{(1)} - \\hat{y}^{(1)} \\mid, \\ldots, \\mid y^{(m)} - \\hat{y}^{(m)} \\mid)=\\text{median}(\\boldsymbol{|e|})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UpnitX8ZLEu",
        "outputId": "7d842b12-c667-4836-e05e-58403e90e1f8"
      },
      "source": [
        "medae = np.median(np.abs(residuals), axis=0)\n",
        "print(f'Median absolute error: {medae}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Median absolute error: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58IlGpMyZLbf"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [mean_absolute_percentage_error](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6oZjvSOZLlY",
        "outputId": "845e9919-a66e-4034-b31d-d67ce38df9eb"
      },
      "source": [
        "from sklearn.metrics import median_absolute_error\n",
        "\n",
        "medae = median_absolute_error(y_true, y_pred)\n",
        "print(f'Median absolute error: {medae}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Median absolute error: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oggQZuPb1NE"
      },
      "source": [
        "### Explained variance <a name=\"4.1.7\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJN9VxMTb1NF"
      },
      "source": [
        "The explained variance relates the variance of the residuals with the variance of the target variable:\n",
        "\n",
        "$$\\text{explained variance}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}}) = 1 - \\frac{Var\\{\\boldsymbol{y} - \\hat{\\boldsymbol{y}}\\}}{Var\\{\\boldsymbol{y}\\}}$$\n",
        "\n",
        "- The best possible score is $1$, lower values are worse.\n",
        "\n",
        "- If residuals are close to its mean (low sparsity or variability), then $Var\\{\\boldsymbol{y} - \\hat{\\boldsymbol{y}}\\} \\approx 0$ so the explained variance will be close to $1$.\n",
        "\n",
        "- Much sparsity of residuals, bigger $Var\\{\\boldsymbol{y} - \\hat{\\boldsymbol{y}}\\}$ and smaller explained variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD2ldskPb1NG",
        "outputId": "6dde6c70-7a5e-4135-c9b8-0cf6b396581a"
      },
      "source": [
        "residuals_var = np.var(residuals, axis=0)\n",
        "y_true_var = np.var(y_true, axis=0)\n",
        "explained_variance = 1 - residuals_var/y_true_var\n",
        "print(f'Explained variance: {explained_variance}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained variance: 0.9353099730458221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9uhBWtbb1NJ"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [explained_variance_score](https://scikit-learn.org/stable/modules/model_evaluation.html#explained-variance-score) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKnLneCrb1NL",
        "outputId": "bebb1066-b0c6-43da-ce61-7e45be9cc2e1"
      },
      "source": [
        "from sklearn.metrics import explained_variance_score\n",
        "\n",
        "explained_variance = explained_variance_score(y_true, y_pred)\n",
        "print(f'Explained variance: {explained_variance}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained variance: 0.9353099730458221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQr2OZJ3tYsZ"
      },
      "source": [
        "### Coefficient of determination (R²) <a name=\"4.1.8\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Yypgx1tg0d"
      },
      "source": [
        "It represents the proportion of variance of the target variable ($\\boldsymbol{y}$) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well-unseen samples are likely to be predicted by the model, though the proportion of explained variance.\n",
        "\n",
        "The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n",
        "\n",
        "$$R^2(\\boldsymbol{y}, \\hat{\\boldsymbol{y}}) = 1-\\frac{\\sum_{i=1}^{m} (e^{(i)})^2}{\\sum_{i=1}^{m} (y^{(i)} - \\bar{\\boldsymbol{y}})^2}  = 1 - \\frac{\\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^{m} (y^{(i)} - \\bar{\\boldsymbol{y}})^2}= 1 - \\frac{\\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2}{m\\cdot \\text{Var}(\\boldsymbol{y})}= 1 - \\frac{\\text{MSE}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\text{Var}(\\boldsymbol{y})}$$\n",
        "\n",
        "\n",
        "- If the squared error sum is smaller than $m\\text{Var}(\\boldsymbol{y})$, then $R^2>0$. Smaller squared error sum, bigger $R^2$.\n",
        "\n",
        "- If the squared error sum is equal to $m\\text{Var}(\\boldsymbol{y})$, then $R^2=1-1=0$. This happen, for example,  when the model is constant and predict the expected value of $\\boldsymbol{y}$, that is $\\hat{y}^{(i)}=\\bar{\\boldsymbol{y}}$ for all $i$.\n",
        "\n",
        "- If the squared error sum is bigger than $m\\text{Var}(\\boldsymbol{y})$, then $R^2<0$. Bigger squared error sum, smaller $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4LGCxvBZm3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c094b2c6-e193-4378-a608-73979c1e4392"
      },
      "source": [
        "r2 = 1-(np.average(residuals**2))/np.var(y_true)\n",
        "print(f'Coefficient of determination: {r2}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficient of determination: 0.9353099730458221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAAtKrbrZnEs"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [r2_score](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96QSjTTIZnL9",
        "outputId": "5b1218de-328c-4f1d-9c22-b906a5cc1090"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(f'Coefficient of determination: {r2}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficient of determination: 0.9353099730458221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYhZg1vZHuEb"
      },
      "source": [
        "## Cost functions <a name=\"4.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0gypZa-Hync"
      },
      "source": [
        "The cost function most used in regression tasks is the mean squared error (MSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Sj6URgPYru"
      },
      "source": [
        "### Huber <a name=\"4.2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg3axoYYPaF8"
      },
      "source": [
        "The Huber loss applies a linear loss to samples that are classified as outliers. A sample is classified as an outlier if the absolute error of that sample is lesser than a certain threshold. It does not ignore the effect of the outliers but gives a lesser weight to them.\n",
        "\n",
        "$$J(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})= \\frac{1}{m} \\sum_{i=1}^{m}L_{\\epsilon}(\\hat{y}^{(i)},y^{(i)})$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "L_{\\epsilon}(\\hat{y}^{(i)},y^{(i)})=\\begin{cases}\n",
        "       \\frac{1}{2}(\\hat{y}^{(i)} - y^{(i)})^2 ,&\\quad\\text{if }|\\hat{y}^{(i)} - y^{(i)}|\\le\\epsilon\\\\\n",
        "       \\epsilon|\\hat{y}^{(i)} - y^{(i)}|-\\frac{1}{2}\\epsilon ,&\\quad\\text{otherwhise}\\\\\n",
        "     \\end{cases}\n",
        "$$\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGXax1QKaPMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67286f3f-4c57-4491-9b76-497921bbe43a"
      },
      "source": [
        "delta = 1.\n",
        "losses = np.where(np.abs(residuals) < delta,.5*(residuals)**2 , \n",
        "                  delta*(np.abs(residuals)-0.5*delta))\n",
        "huber_loss = np.average(losses)\n",
        "print(f'Huber loss: {huber_loss}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7tQPxnubc_A"
      },
      "source": [
        "In Scikit-learn, the huber regression is implemented with the [`HuberRegressor`](https://scikit-learn.org/stable/modules/linear_model.html#huber-regression) model.\n",
        "\n",
        "In Tensorflow, this cost function is implemented with the [Huber](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber) loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR9DgSRAf6aR",
        "outputId": "2c5c0329-e889-42f8-b5c0-908481fc4bad"
      },
      "source": [
        "huber = tf.keras.losses.Huber()\n",
        "print(f'Huber loss: {huber(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Huber loss: 0.1875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6emcv4FL6X7"
      },
      "source": [
        "### Logcosh <a name=\"4.2.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyebX9QrMYgO"
      },
      "source": [
        "As we mentioned previously, with MSE, the error explodes when we have outliers in our dataset, substantially distorting the computed error. On the other hand, MAE is too soft when the residuals are small. Another loss function which attempts to combine the best of both worlds is the **Logcosh loss function**.\n",
        "\n",
        "$$\\text{Log cosh}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})= \\frac{1}{m}\\sum_{i=1}^{m} \\log{\\text{cosh}(e^{(i)})}$$\n",
        "\n",
        "$\\log{\\text{cosh}(x})$ is approximately equal to $\\frac{(x^2)}{2}$ for small $x$ and to $|x|-\\log{2}$ for large $x$. This means that the Logcosh loss works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect predictions that are likely caused by outlier samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU6xR5U_OiAq",
        "outputId": "b14c8b66-8ed4-4207-fd71-0c3a52f123a6"
      },
      "source": [
        "np.average(np.log(np.cosh(residuals)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1685024610998955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr9xerloN9rJ"
      },
      "source": [
        "In Tensorflow, this cost function is implemented with the [LogCosh](https://www.tensorflow.org/api_docs/python/tf/keras/losses/LogCosh) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HqJWL4TN9rK",
        "outputId": "fe60492f-f63a-47cc-d43c-2a7ec2ce1d77"
      },
      "source": [
        "tf.keras.losses.LogCosh\n",
        "\n",
        "log_cosh = tf.keras.losses.LogCosh()\n",
        "print(f'Log cosh loss: {log_cosh(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log cosh loss: 0.16850246489048004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RyQV6ceHqcd"
      },
      "source": [
        "# Classificarion <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGvlGEPU3pxq"
      },
      "source": [
        "In machine learning, classification algorithms attempt to estimate the mapping function ($f$) from the input variables ($\\boldsymbol{X}$) to discrete or categorical output variable ($\\boldsymbol{y}$), $\\hat{\\boldsymbol{y}}=f(\\boldsymbol{X})$.\n",
        "\n",
        "In this case, $\\boldsymbol{y}$ is a categorical variable where each element depicts the label on which the instance is assigned to. \n",
        "\n",
        "- When the target variable has two possible labels, the problem is called **binary classification**. \n",
        "\n",
        "- When the target variable has more than two possible labels, the problem is called **multiclass classification**. Note that the classes must be mutually exclusive (each example can only be assigned to one category).\n",
        "\n",
        "- When each example can be assigned to multiple categories, the problem is called **multi-label classification**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpZvjFiLHxYj"
      },
      "source": [
        "## Metrics <a name=\"5.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGsH-712hXLk"
      },
      "source": [
        "### Confusion matrix <a name=\"5.1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "816dm913ho5W"
      },
      "source": [
        "A **confusion matrix** (also called **error matrix**) is a specific table that allows visualization of the performance of a classifier. Each row of the matrix represents the instances in an actual/true class, while each column represents the instances in a predicted class (or vice versa, Wikipedia and other references may use a different convention for axes).\n",
        "\n",
        "The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.\n",
        "\n",
        "**Binary classification**\n",
        "\n",
        "![](https://i.ibb.co/sHpMv3Q/confusion-matrix.png)\n",
        "\n",
        "- True Positive (TP): the number of positive examples in the data that have been classified as positive.\n",
        "\n",
        "- True Negative (TN): the number of negative examples in the data that have been classified as negative.\n",
        "\n",
        "- False Negative (FN): the number of positive examples in the data that have been classified as negative.\n",
        "\n",
        "- False Positive (TP): the number of negative examples in the data that have been classified as positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJgN-_KnMfYe"
      },
      "source": [
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 1, 0, 0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrHqotegMti4"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [confusion_matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVjekqJqMuEj",
        "outputId": "0f3bba87-43ab-41cd-c9f7-514b3f10147b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0]\n",
            " [2 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FazHmMzbMy4G"
      },
      "source": [
        "In Tensorflow, this metric is implemented with the [confusion_matrix](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHTMda_xMy4P",
        "outputId": "50ce8b27-48cb-4e60-9e0c-f8cfc62a5f13"
      },
      "source": [
        "error_matrix = tf.math.confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0]\n",
            " [2 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEXyDpsLMf4v"
      },
      "source": [
        "**Multiclass classification**\n",
        "\n",
        "Generally, denoting the confusion matrix as $C :=(c_{ij})$, where $c_{ij}$ is the number of observations actually assigned to group $i$ in the data and predicted to be in group $j$, the confusion matrix with respect to one classification target $k \\in \\{1,2,...,N\\}$ can be represented as follow:\n",
        "\n",
        "![](https://i.ibb.co/b7n8ZBJ/multiclass-confusion-matrix.png)\n",
        "\n",
        "- True Positive (TP): the number of examples actually assigned to group $k$ in the data that have been classified to be in the group $k$.\n",
        "\n",
        "- True Negative (TN): the number of examples not actually assigned to group $k$ in the data that have not been classified to be in the group $k$.\n",
        "\n",
        "- False Negative (FN): the number of examples actually assigned to group $k$ in the data that have been classified to be in another group.\n",
        "\n",
        "- False Positive (TP): the number of examples not actually assigned to group $k$ in the data that have been classified to be in the group $k$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnLrUmf-Mhej",
        "outputId": "08d96d80-c00e-49ff-ac61-9b2cec207209"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = [2, 0, 2, 2, 0, 1]\n",
        "y_pred = [0, 0, 2, 2, 0, 2]\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "# error_matrix = tf.math.confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0 0]\n",
            " [0 0 1]\n",
            " [1 0 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmSVH5TZMhnL"
      },
      "source": [
        "**Multi-label classification**\n",
        "\n",
        "In this case, since an example can be labeled with more than one class, it's common to plot a \"binary\" confusion matrix for each class.\n",
        "\n",
        "In Scikit-learn, this metric is implemented with the [multilabel_confusion_matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#multi-label-confusion-matrix) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnjtQuP0MiA3",
        "outputId": "7a30424f-95bc-4f57-e245-a251c3021729"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "y_true = [[1, 0, 1], [0, 0, 1]]\n",
        "y_pred = [[1, 0, 0], [1, 0, 1]]\n",
        "\n",
        "error_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrices:\\n{error_matrices}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrices:\n",
            "[[[0 1]\n",
            "  [0 1]]\n",
            "\n",
            " [[2 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 0]\n",
            "  [1 1]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q49dTkr3g3pd"
      },
      "source": [
        "### Accuracy <a name=\"5.1.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJcWaZlUg-bt"
      },
      "source": [
        "The **accuracy** provides the amount of correctly classified examples by relating the number of correctly classified examples to the overall number of examples.\n",
        "\n",
        "$$\\text{Accuracy}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})=\\frac{\\sum_{i=1}^Nc_{ii}}{\\sum_{i=1}^N\\sum_{j=1}^Nc_{ij}} = \\frac{\\sum_{i=1}^Nc_{ii}}{m}$$\n",
        "\n",
        "Of all examples in the data, what fraction of them the classifier has predicted correctly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLsfgUR5zDCb"
      },
      "source": [
        "**Binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD1d6pswzW29",
        "outputId": "ad4e110d-8afb-492d-b977-6a00226f6e28"
      },
      "source": [
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 1, 0, 0]\n",
        "# 6 examples, 4 correctly classified\n",
        "m = len(y_true)\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "accuracy = sum(error_matrix.diagonal())/m\n",
        "# accuracy = np.average(np.equal(y_true, y_pred))\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0]\n",
            " [2 2]]\n",
            "Accuracy: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ZD_scZz_MX"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [accuracy_score](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MidbZ7TqzuKA",
        "outputId": "a9da7d06-10ed-4d36-f61f-5da9d7b3929d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IXGXvKB0KvA"
      },
      "source": [
        "In Tensorflow, this metric is implemented with the [Accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzreu-Lw0MuN",
        "outputId": "28bd4dff-3308-4b94-bc3f-48a819de09bc"
      },
      "source": [
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "print(f'Accuracy: {accuracy(y_true, y_pred).numpy()}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6666666865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rGpY3kV0bPl"
      },
      "source": [
        "The [`BinaryAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy) keras metric can be used to interpret probabilities (we can modify the `threshold` parameter, which is set `0.5` by default):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu4qY_3IzITL",
        "outputId": "241fc1a9-7f92-4dbe-b54e-5fd443f4d6b5"
      },
      "source": [
        "y_true = [1.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
        "y_pred = [0.4, 0.1, 1.0, 0.8, 0.1, 0.4]\n",
        "\n",
        "accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
        "# accuracy = np.average(np.equal(y_true, np.round(y_pred)))\n",
        "print(f'Binary accuracy score: {accuracy(y_true, y_pred).numpy()}')\n",
        "\n",
        "accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.3)\n",
        "print('Binary accuracy score with threshold=0.3'\n",
        "      f': {accuracy(y_true, y_pred).numpy()}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary accuracy score: 0.6666666865348816\n",
            "Binary accuracy score with threshold=0.3: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hb_D56Jne-Q"
      },
      "source": [
        "**Note**: When we use accuracy, we assign an equal cost to false positives and false negatives. When that **dataset is imbalanced**, say it has 90% of instances in one class and only 10 % in the other, predict that every instance belongs to the majority class, get an accuracy of 90%. So, for imbalanced datasets, accuracy is not a good metric. Let's see an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDZEIeUynLnA",
        "outputId": "074613d0-dad2-4202-8ad0-a962b617c078"
      },
      "source": [
        "y_true = [1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
        "y_pred = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[0 1]\n",
            " [0 9]]\n",
            "Accuracy: 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3kEbUX3zIal"
      },
      "source": [
        "**Multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyN2GylgR0Zb",
        "outputId": "7252fde5-cb02-4785-ee6b-38d600ce27a7"
      },
      "source": [
        "y_true = [2, 0, 2, 2, 0, 1]\n",
        "y_pred = [0, 0, 2, 2, 0, 2]\n",
        "# 6 examples, 4 correctly classified\n",
        "m = len(y_true)\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "accuracy = sum(error_matrix.diagonal())/m\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0 0]\n",
            " [0 0 1]\n",
            " [1 0 2]]\n",
            "Accuracy: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdpmOofnR2hI",
        "outputId": "44be7d21-cc21-480c-d07b-4186d4a1945f"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "print(f'Accuracy: {accuracy(y_true, y_pred).numpy()}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6666666666666666\n",
            "Accuracy: 0.6666666865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhaTX8MJnju-"
      },
      "source": [
        "The [`CategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy) Keras metric can be used to interpret probabilities.\n",
        "\n",
        "**Top-k accuracy score** is a generalization of accuracy score. The difference is that a prediction is considered correct as long as the true label is associated with one of the $k$ highest predicted scores. `accuracy_score` is the special case of $k = 1$. In Scikit-learn, this metric is implemented with the [top_k_accuracy_score](https://scikit-learn.org/stable/modules/model_evaluation.html#top-k-accuracy-score) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hUZuQVKWkE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1164eb0-e0cd-4e13-baff-cc1c9fcb9899"
      },
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "y_true = np.array([2, 0, 2, 2, 0, 1])\n",
        "y_score = np.array([[0.6, 0.1, 0.3],\n",
        "                    [0.7, 0.1, 0.2],\n",
        "                    [0.3, 0.1, 0.6],\n",
        "                    [0.1, 0.3, 0.6],\n",
        "                    [0.7, 0.3, 0.0],\n",
        "                    [0.1, 0.3, 0.6]])\n",
        "print(f'Top-1 accuracy score: {top_k_accuracy_score(y_true, y_score, k=1)}')\n",
        "print(f'Top-2 accuracy score: {top_k_accuracy_score(y_true, y_score, k=2)}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top-1 accuracy score: 0.6666666666666666\n",
            "Top-2 accuracy score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibTRkEOhtBDV"
      },
      "source": [
        "In Tensorflow, this metric is implemented with the [TopKCategoricalAccuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy) method (a generalization of the [`CategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy) metric). It requires one-hot labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOHtJx_JTf3z",
        "outputId": "c7dc4ac2-8974-4eb2-a54a-886c645148e9"
      },
      "source": [
        "# The CategoricalAccuracy keras metric requires y_true in one-hot\n",
        "classes_number = 3\n",
        "y_true_one_hot = tf.one_hot(y_true.tolist(), depth=classes_number)\n",
        "\n",
        "top_1_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=1)\n",
        "# top_1_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "# np.average(np.equal(y_true, np.argmax(y_score, axis=-1)))\n",
        "top_2_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=2)\n",
        "\n",
        "print(f'Top-1 accuracy score: {top_1_accuracy(y_true_one_hot, y_score)}')\n",
        "print(f'Top-2 accuracy score: {top_2_accuracy(y_true_one_hot, y_score)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top-1 accuracy score: 0.6666666865348816\n",
            "Top-2 accuracy score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1cJCeCiybZB"
      },
      "source": [
        "**Multi-label classification**\n",
        "\n",
        "In multi-label classification, the [accuracy_score](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score) Scikit-learn metric computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in `y_true`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1KxLeJm3k8w",
        "outputId": "54e29136-fccb-492b-9cbf-7f662af3548b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_true = np.array([[1, 0, 1], [0, 0, 1]])\n",
        "y_pred = np.array([[1, 0, 0], [1, 0, 1]])\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "y_true = np.array([[1, 0, 1], [1, 0, 1]])\n",
        "y_pred = np.array([[1, 0, 0], [1, 0, 1]])\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0\n",
            "Accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvkUAzTK0t-A"
      },
      "source": [
        "This metric has a disadvantage. If the set of labels predicted for a sample does not exactly match but nearly, the accuracy will be equal to a set of labels predicted for a sample which matchs poorly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ44WsZw1cH3",
        "outputId": "7bc318c2-6578-42a9-a203-de3ade0079e1"
      },
      "source": [
        "y_true = np.array([[1, 0, 1, 0, 1, 0, 0]])\n",
        "y_pred = np.array([[1, 0, 1, 0, 1, 0, 1]])\n",
        "print(f'Accuracy: {accuracy_score(y_true, y_pred)}')\n",
        "\n",
        "y_true = np.array([[1, 0, 1, 0, 1, 0, 0]])\n",
        "y_pred = np.array([[0, 1, 0, 1, 0, 1, 1]])\n",
        "print(f'Accuracy: {accuracy_score(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0\n",
            "Accuracy: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SySdnk5TF3Vt"
      },
      "source": [
        "The [top_k_accuracy_score](https://scikit-learn.org/stable/modules/model_evaluation.html#top-k-accuracy-score) Scikit-learn metric does not cover the multi-label case because it assumes that classes are mutually exclusive (multiclass classification). The same happens with the [`CategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy) Keras metric. We could use the `Accuracy` or the `BinaryAccuracy` Keras metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebgKiG464Tgv",
        "outputId": "a51f196e-6f93-4c46-c802-2a60aa485819"
      },
      "source": [
        "y_true = np.array([[1, 0, 1, 0, 1, 0, 0]])\n",
        "y_pred = np.array([[1, 0, 1, 0, 1, 0, 1]])\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "print(f'Accuracy score: {accuracy(y_true, y_pred).numpy()}')\n",
        "\n",
        "y_true = np.array([[1, 0, 1, 0, 1, 0, 0]])\n",
        "y_pred = np.array([[0, 1, 0, 1, 0, 1, 1]])\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "print(f'Accuracy score: {accuracy(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.8571428656578064\n",
            "Accuracy score: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMzzNmmPViV7",
        "outputId": "e9cd6510-1c38-4148-a869-716d4da0b752"
      },
      "source": [
        "y_true = np.array([[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]])\n",
        "y_pred = np.array([[0.9, 0.2, 0.8, 0.3, 0.7, 0.1, 0.9]])\n",
        "\n",
        "accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
        "# accuracy = np.average(np.equal(y_true, np.round(y_pred)))\n",
        "print(f'Binary accuracy score: {accuracy(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary accuracy score: 0.8571428656578064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSgVQq6CyCzU"
      },
      "source": [
        "However, this accuracy metric is not a good metric for multi-label classififcation tasks. Imagine we have 10 possible labels, since each example is assigned with a small set of these labels, we can get a high accuracy when our model predicts mostly zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cGhqe18y11J",
        "outputId": "5bc97937-7f54-42ed-9a91-5d2dbe37a1e6"
      },
      "source": [
        "y_true = np.array([\n",
        "    [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]]\n",
        ")\n",
        "y_pred = np.array([\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
        ")\n",
        "\n",
        "accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
        "print(f'Binary accuracy score: {accuracy(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary accuracy score: 0.8500000238418579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7icoN8ruZni"
      },
      "source": [
        "### Recall <a name=\"5.1.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cayZyqPRub7g"
      },
      "source": [
        "The **recall** (often called **sensitivity** or **true positive rate**) represents the classifier’s ability to correctly classify a given class. It is provided by the amount of truly positive predicted examples (TP) related to the number of examples where the class actually happens (TP + TN). \n",
        "\n",
        "$$\\text{Recall}_{\\text{class}}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})=\\frac{TP_{\\text{class}}}{TP_{\\text{class}}+FN_{\\text{class}}}=\\frac{c_{kk}}{\\sum_{j=1}^Nc_{kj}}$$\n",
        "\n",
        "Of all examples actually assigned to the class $k$ in the data, what fraction of them the classifier has predicted to be in group $k$?\n",
        "\n",
        "In Scikit-learn, this metric is implemented with the [`recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) for binary, multiclass and multilabel classification. In TensorFlow, this metric is implemented with the [Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall) method (it can be used to interpret probabilities but does not cover the multi-label case).\n",
        "\n",
        "**Binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrQJPRwyXTAK",
        "outputId": "1e6f2943-bf9d-4c02-caee-89908c2b1a02"
      },
      "source": [
        "from sklearn.metrics import recall_score, confusion_matrix\n",
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 1, 0, 0]\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "# If average=\"binary\" (default), it returns the score for the positive class (1)\n",
        "print(f'Recall of positive class: {recall_score(y_true, y_pred)}')\n",
        "print(f'Recall of negative class: {recall_score(y_true, y_pred, pos_label=0)}')\n",
        "# If average=None, it returns the scores for each class\n",
        "print(f'Recall per class: {recall_score(y_true, y_pred, average=None)}')\n",
        "\n",
        "recall = tf.keras.metrics.Recall()\n",
        "print(f'Recall of positive class: {recall(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0]\n",
            " [2 2]]\n",
            "Recall of positive class: 0.5\n",
            "Recall of negative class: 1.0\n",
            "Recall per class: [1.  0.5]\n",
            "Recall of positive class: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-_Cps69w30"
      },
      "source": [
        "**Multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWfjE1MrWwiv",
        "outputId": "b8a231e4-d370-455e-e557-91a9633df8b9"
      },
      "source": [
        "y_true = np.array([2, 0, 2, 2, 0, 1])\n",
        "y_pred = np.array([0, 0, 2, 2, 0, 2])\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "print(f'Recall per class: {recall_score(y_true, y_pred, average=None)}')\n",
        "# If average=\"macro\", it returns the mean of the recall of each label\n",
        "print(f'Mean of recall: {recall_score(y_true, y_pred, average=\"macro\")}')\n",
        "# np.average(recall_score(y_true, y_pred, average=None))\n",
        "# Set average=\"weighted\" for imbalance datasets\n",
        "print('Weighted mean of recall: '\n",
        "      f'{recall_score(y_true, y_pred, average=\"weighted\")}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0 0]\n",
            " [0 0 1]\n",
            " [1 0 2]]\n",
            "Recall per class: [1.         0.         0.66666667]\n",
            "Mean of recall: 0.5555555555555555\n",
            "Weighted mean of recall: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kemNlMMBdlYv",
        "outputId": "b8ba943d-a226-4f5c-dffe-c4e9e2a95374"
      },
      "source": [
        "# The Recall keras metric requires y_true in one-hot\n",
        "classes_number = len(np.unique(y_true))\n",
        "y_true_one_hot = tf.one_hot(y_true.tolist(), depth=classes_number)\n",
        "y_score = np.array([[0.6, 0.1, 0.3],\n",
        "                    [0.7, 0.1, 0.2],\n",
        "                    [0.3, 0.1, 0.6],\n",
        "                    [0.1, 0.3, 0.6],\n",
        "                    [0.7, 0.3, 0.0],\n",
        "                    [0.1, 0.3, 0.6]])\n",
        "recall = tf.keras.metrics.Recall(top_k=1, class_id=2)\n",
        "print(f'Recall of class 2: {recall(y_true_one_hot, y_score).numpy()}')\n",
        "\n",
        "recall = tf.keras.metrics.Recall(top_k=2, class_id=2)\n",
        "print(f'Top-2-recall of class 2: {recall(y_true_one_hot, y_score).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall of class 2: 0.6666666865348816\n",
            "Top-2-recall of class 2: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FBJG9Ua99hb"
      },
      "source": [
        "**Multi-label classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9U3oddpbjBw",
        "outputId": "b116b220-da56-4cae-f6ec-75f95e99b935"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "y_true = np.array([[1, 0, 1], [0, 0, 1]])\n",
        "y_pred = np.array([[1, 0, 0], [1, 0, 1]])\n",
        "\n",
        "error_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrices:\\n{error_matrices}')\n",
        "print('Recall per class: '\n",
        "      f'{recall_score(y_true, y_pred, average=None, zero_division=0)}')\n",
        "print('Mean of recall: '\n",
        "      f'{recall_score(y_true, y_pred, average=\"macro\", zero_division=0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrices:\n",
            "[[[0 1]\n",
            "  [0 1]]\n",
            "\n",
            " [[2 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 0]\n",
            "  [1 1]]]\n",
            "Recall per class: [1.  0.  0.5]\n",
            "Mean of recall: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9nbSPkR13n_"
      },
      "source": [
        "### Precision <a name=\"5.1.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HrvqkXu15js"
      },
      "source": [
        "The **precision** (also called **positive predictive value**) represents the classifier’s certainty of correctly predicting a given class. The precision relates the amount of truly positive predicted (TP) examples to the number of examples where the particular class was predicted (TP + FP). \n",
        "\n",
        "$$\\text{Precision}_{\\text{class}}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})=\\frac{TP_{\\text{class}}}{TP_{\\text{class}}+FP_{\\text{class}}}=\\frac{c_{kk}}{\\sum_{j=1}^Nc_{jk}}$$\n",
        "\n",
        "Of all examples predicted to be in the group $k$, what fraction of them actually are assigned to group $k$ in the data?\n",
        "\n",
        "In Scikit-learn, this metric is implemented with the [`precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) for binary, multiclass and multilabel classification. In TensorFlow, this metric is implemented with the [Precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision) method (it can be used to interpret probabilities but does not cover the multi-label case).\n",
        "\n",
        "**Binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2z2nTioHWgx",
        "outputId": "c2ee28f4-fe12-4f57-b066-139edec7015e"
      },
      "source": [
        "from sklearn.metrics import precision_score, confusion_matrix\n",
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 1, 0, 0]\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "# If average=\"binary\" (default), it returns the score for the positive class (1)\n",
        "print(f'Precision of positive class: {precision_score(y_true, y_pred)}')\n",
        "# If average=None, it returns the score for each class\n",
        "print(f'Precision per class: {precision_score(y_true, y_pred, average=None)}')\n",
        "\n",
        "recall = tf.keras.metrics.Precision()\n",
        "print(f'Precision of positive class: {recall(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0]\n",
            " [2 2]]\n",
            "Precision of positive class: 1.0\n",
            "Precision per class: [0.5 1. ]\n",
            "Precision of positive class: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRPC1g6vHpRZ"
      },
      "source": [
        "**Multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_0KgGLMHpRb",
        "outputId": "6aa8b55d-dd18-46a7-92bd-07846494ff2a"
      },
      "source": [
        "y_true = np.array([2, 0, 2, 2, 0, 1])\n",
        "y_pred = np.array([0, 0, 2, 2, 0, 2])\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "print('Precision per class: '\n",
        "      f'{precision_score(y_true, y_pred, average=None, zero_division=0)}')\n",
        "# If average=\"macro\", it returns the mean of the recall of each label\n",
        "print('Mean of precision: '\n",
        "      f'{precision_score(y_true, y_pred, average=\"macro\", zero_division=0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0 0]\n",
            " [0 0 1]\n",
            " [1 0 2]]\n",
            "Precision per class: [0.66666667 0.         0.66666667]\n",
            "Mean of precision: 0.4444444444444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5heV7irwHpRd",
        "outputId": "05db8525-7c3e-40f6-e118-c683643943ba"
      },
      "source": [
        "# The Precision keras metric requires y_true in one-hot\n",
        "y_true_one_hot = tf.one_hot(y_true.tolist(), depth=classes_number)\n",
        "y_score = np.array([[0.6, 0.1, 0.3],\n",
        "                    [0.7, 0.1, 0.2],\n",
        "                    [0.3, 0.1, 0.6],\n",
        "                    [0.1, 0.3, 0.6],\n",
        "                    [0.7, 0.3, 0.0],\n",
        "                    [0.1, 0.3, 0.6]])\n",
        "recall = tf.keras.metrics.Precision(top_k=1, class_id=2)\n",
        "print(f'Precision of class 2: {recall(y_true_one_hot, y_score).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision of class 2: 0.6666666865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzk0joBNIlvh"
      },
      "source": [
        "**Multi-label classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA3TYnw9Ilvj",
        "outputId": "1273480d-6064-43eb-d3b1-38d92a27174c"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "y_true = np.array([[1, 0, 1], [0, 0, 1]])\n",
        "y_pred = np.array([[1, 0, 0], [1, 0, 1]])\n",
        "\n",
        "error_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrices:\\n{error_matrices}')\n",
        "print('Precision per class: '\n",
        "      f'{precision_score(y_true, y_pred, average=None, zero_division=0)}')\n",
        "print('Mean of precision: '\n",
        "      f'{precision_score(y_true, y_pred, average=\"macro\", zero_division=0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrices:\n",
            "[[[0 1]\n",
            "  [0 1]]\n",
            "\n",
            " [[2 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 0]\n",
            "  [1 1]]]\n",
            "Precision per class: [0.5 0.  1. ]\n",
            "Mean of precision: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtQcdU6u3FJ8"
      },
      "source": [
        "### F1-Score <a name=\"5.1.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts3MQPO53GGa"
      },
      "source": [
        "The **F1-score** (or **F measure** or **balanced F-score**) is a measure that combines precision and recall. It indicates the fraction of correctly classified examples for each class within the dataset.\n",
        "\n",
        "\n",
        "$$\\text{F-score}_{\\text{class}}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})=2 \\cdot \\frac{\\text{Precision}_{\\text{class}}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}}) \\cdot \\text{Recall}_{\\text{class}}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})}{\\text{Precision}_{\\text{class}}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}}) + \\text{Recall}_{\\text{class}}(\\boldsymbol{\\boldsymbol{y},\\hat{\\boldsymbol{y}}})}$$\n",
        "\n",
        "Of all examples classified to be in the group $k$ and all examples actually assigned to group $k$ in the data, what fraction of them have been predicted correctly?\n",
        "\n",
        "In Scikit-learn, this metric is implemented with the [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) for binary, multiclass and multilabel classification. In Tensorflow Addons, this metric is implemented with the [F1Score](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score) method (it can be used to interpret probabilities, and we can modify the `threshold` parameter, which is set `0.5` by default).\n",
        "\n",
        "**Binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KmhAD3nJfWw",
        "outputId": "bb533bff-c59c-4420-8e03-874aa8f8f34f"
      },
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 1, 0, 0]\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "# If average=\"binary\" (default), it returns the score for the positive class (1)\n",
        "print(f'F1-score of positive class: {f1_score(y_true, y_pred)}')\n",
        "# If average=None, it returns the score for each class\n",
        "print(f'F1-score per class: {f1_score(y_true, y_pred, average=None)}')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0]\n",
            " [2 2]]\n",
            "F1-score of positive class: 0.6666666666666666\n",
            "F1-score per class: [0.66666667 0.66666667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCBxbi74JfXD"
      },
      "source": [
        "**Multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdQbhM1nJfXE",
        "outputId": "7e12f274-c9c8-49d9-e61a-c6cdbad8be81"
      },
      "source": [
        "y_true = np.array([2, 0, 2, 2, 0, 1])\n",
        "y_pred = np.array([0, 0, 2, 2, 0, 2])\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "print(f'F1-score per class: {f1_score(y_true, y_pred, average=None)}')\n",
        "# If average=\"macro\", it returns the mean of the f1-score of each label\n",
        "print(f'Mean of f1-scores: {f1_score(y_true, y_pred, average=\"macro\")}')\n",
        "# np.average(f1_score(y_true, y_pred, average=None))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0 0]\n",
            " [0 0 1]\n",
            " [1 0 2]]\n",
            "F1-score per class: [0.8        0.         0.66666667]\n",
            "Mean of f1-scores: 0.48888888888888893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R43zQEquTopg",
        "outputId": "5e1fc10d-24fd-4486-9581-acb3dd406513"
      },
      "source": [
        "print(f'Mean of f1-scores: {f1_score(y_true, y_pred, average=\"weighted\")}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of f1-scores: 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neW05WsDTl8X",
        "outputId": "66cdc102-b714-434f-aa25-5e659bb2089a"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "y_true_one_hot = tf.one_hot(y_true.tolist(), depth=3)\n",
        "y_score = np.array([[0.6, 0.1, 0.3],\n",
        "                    [0.7, 0.1, 0.2],\n",
        "                    [0.3, 0.1, 0.6],\n",
        "                    [0.1, 0.3, 0.6],\n",
        "                    [0.7, 0.3, 0.0],\n",
        "                    [0.1, 0.3, 0.6]])\n",
        "\n",
        "\n",
        "metric = tfa.metrics.F1Score(num_classes=3, threshold=0.5, average=\"macro\")\n",
        "metric(y_true_one_hot, y_score).numpy()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4888889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQhxxmoTJfXF"
      },
      "source": [
        "**Multi-label classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaEj1_83JfXF",
        "outputId": "af084117-5df1-48b9-9816-a96e590ddedc"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "y_true = np.array([[1, 0, 1], [0, 0, 1]])\n",
        "y_pred = np.array([[1, 0, 0], [1, 0, 1]])\n",
        "\n",
        "error_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrices:\\n{error_matrices}')\n",
        "print('F1-score per class: '\n",
        "      f'{f1_score(y_true, y_pred, average=None, zero_division=0)}')\n",
        "print('Mean of f1-scores: '\n",
        "      f'{f1_score(y_true, y_pred, average=\"macro\", zero_division=0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrices:\n",
            "[[[0 1]\n",
            "  [0 1]]\n",
            "\n",
            " [[2 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 0]\n",
            "  [1 1]]]\n",
            "F1-score per class: [0.66666667 0.         0.66666667]\n",
            "Mean of f1-scores: 0.4444444444444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EHFHMbrKk1U"
      },
      "source": [
        "### Classification report <a name=\"5.1.6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiegxYpiKvEx"
      },
      "source": [
        "The [**`classification_report`**](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report) Scikit-learn metric builds a text report showing the main classification metrics (accuracy, recall, precision, etc). It can be used for binary, multiclass, and multi-label classification.\n",
        "\n",
        "**Binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1duSc83KvEz",
        "outputId": "5e62e91a-4646-4388-92a4-9b256aa6ce7b"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 1, 0, 0]\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "\n",
        "print('Classification_report:\\n'\n",
        "      f'{classification_report(y_true, y_pred, zero_division=0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0]\n",
            " [2 2]]\n",
            "Classification_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         2\n",
            "           1       1.00      0.50      0.67         4\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.75      0.75      0.67         6\n",
            "weighted avg       0.83      0.67      0.67         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq7p5ClGK-uW"
      },
      "source": [
        "**Multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r1-2VUBLcBF",
        "outputId": "2cf67523-59cd-42a9-d12f-ae15e370e701"
      },
      "source": [
        "y_true = np.array([2, 0, 2, 2, 0, 1])\n",
        "y_pred = np.array([0, 0, 2, 2, 0, 2])\n",
        "\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "\n",
        "print('Classification_report:\\n'\n",
        "      f'{classification_report(y_true, y_pred, zero_division=0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[2 0 0]\n",
            " [0 0 1]\n",
            " [1 0 2]]\n",
            "Classification_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80         2\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.44      0.56      0.49         6\n",
            "weighted avg       0.56      0.67      0.60         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coXfZE3oLg8c"
      },
      "source": [
        "**Multi-label classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yflpmZTbLo0w",
        "outputId": "1c8016c4-b278-446f-8569-76a723a6039b"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "y_true = np.array([[1, 0, 1], [0, 0, 1]])\n",
        "y_pred = np.array([[1, 0, 0], [1, 0, 1]])\n",
        "\n",
        "error_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrices:\\n{error_matrices}')\n",
        "\n",
        "print('Classification_report:\\n'\n",
        "      f'{classification_report(y_true, y_pred, zero_division=0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrices:\n",
            "[[[0 1]\n",
            "  [0 1]]\n",
            "\n",
            " [[2 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 0]\n",
            "  [1 1]]]\n",
            "Classification_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         1\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       1.00      0.50      0.67         2\n",
            "\n",
            "   micro avg       0.67      0.67      0.67         3\n",
            "   macro avg       0.50      0.50      0.44         3\n",
            "weighted avg       0.83      0.67      0.67         3\n",
            " samples avg       0.75      0.75      0.67         3\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAPuyyGH5xOm"
      },
      "source": [
        "### ROC curve and AUC <a name=\"5.1.7\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcwBEqHY5y2e"
      },
      "source": [
        "A **receiver operating characteristic (ROC) curve**, is a plot which illustrates the performance of a binary classifier as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (recall) vs. the fraction of false positives out of the negatives (called false positive rate, $\\text{FPR}=\\frac{\\text{FP}}{\\text{FP}+\\text{TN}}$), at various threshold settings. FPR is one minus the specificity or true negative rate.\n",
        "\n",
        "In Scikit-learn, this metric is implemented with the [`roc_curve`](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) method for binary classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "qbHZLrhgSBX3",
        "outputId": "70f1198f-4a27-49dd-a7b8-471096489abc"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred_prob = [0.1, 0.1, 0.8, 0.9, 0.1, 0.4]\n",
        "fper, tper, thresholds = roc_curve(y_true, y_pred_prob, pos_label=1)\n",
        "\n",
        "def plot_roc_curve(fper, tper):  \n",
        "    plt.plot(fper, tper, color='orange', label='ROC')\n",
        "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "plot_roc_curve(fper, tper)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXqDuAw8WD5D"
      },
      "source": [
        "It's common to compute the **area under the ROC-curve (AUC)** as a way to summarize the ROC curve. \n",
        "\n",
        "In Scikit-learn, this metric is implemented with the [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) method (it can also be used for [multiclass](https://scikit-learn.org/stable/modules/model_evaluation.html#multi-class-case) and [multi-label](https://scikit-learn.org/stable/modules/model_evaluation.html#multi-label-case) classification). In TensorFlow, this metric is implemented with the [AUC](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC) method.\n",
        "\n",
        "**Binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG2vLOhGWY0D",
        "outputId": "3b5b7558-e33d-4ddc-dd3d-3d729005ce19"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "area_under_curve = roc_auc_score(y_true, y_pred_prob)\n",
        "print(f'Auc: {area_under_curve}')\n",
        "\n",
        "auc = tf.keras.metrics.AUC()\n",
        "area_under_curve = auc(y_true, y_pred_prob)\n",
        "print(f'Auc: {area_under_curve}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Auc: 0.875\n",
            "Auc: 0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ZNiYeFo6a7"
      },
      "source": [
        "**Multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF3s3Gomn24P",
        "outputId": "31331fee-33bb-4655-d25f-535fce2cafa6"
      },
      "source": [
        "y_true = np.array([2, 0, 2, 2, 0, 1])\n",
        "\n",
        "y_score = np.array([[0.6, 0.1, 0.3],\n",
        "                    [0.7, 0.1, 0.2],\n",
        "                    [0.3, 0.1, 0.6],\n",
        "                    [0.1, 0.3, 0.6],\n",
        "                    [0.7, 0.3, 0.0],\n",
        "                    [0.1, 0.3, 0.6]])\n",
        "area_under_curve = roc_auc_score(y_true, y_score, multi_class=\"ovr\")\n",
        "print(f'Auc: {area_under_curve}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Auc: 0.8592592592592593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVjc0ol7pPNa"
      },
      "source": [
        "**Multi-label classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_CJXrrpon7i",
        "outputId": "18bb0060-03c4-419c-85ff-2ea052620fa4"
      },
      "source": [
        "y_true = np.array([[1.0, 0.0, 1.0], [0.0, 1.0, 1.0], [1.0, 1.0, 0.0]])\n",
        "y_pred = np.array([[0.9, 0.1, 0.2], [0.4, 0.2, 0.8], [0.6, 0.7, 0.2]])\n",
        "\n",
        "area_under_curve = roc_auc_score(y_true, y_pred, average=None)\n",
        "print(f'Auc per label: {area_under_curve}')\n",
        "area_under_curve = roc_auc_score(y_true, y_pred, average=\"macro\")\n",
        "print(f'Macro Auc: {area_under_curve}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Auc per label: [1.   1.   0.75]\n",
            "Macro Auc: 0.9166666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUXVw7LyybPx",
        "outputId": "209e0617-dba5-48f4-c064-d1cd33093f48"
      },
      "source": [
        "auc = tf.keras.metrics.AUC(multi_label=True)\n",
        "area_under_curve = auc(y_true, y_pred)\n",
        "print(f'Macro Auc: {area_under_curve}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro Auc: 0.9166666865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nszg_ObtqWp",
        "outputId": "e9bce845-9b77-46fe-fed4-6bbdc3ee96a9"
      },
      "source": [
        "y_true = np.array([\n",
        "    [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
        "    [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]]\n",
        ")\n",
        "y_pred = np.array([\n",
        "    [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
        "    [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
        ")\n",
        "\n",
        "auc = tf.keras.metrics.AUC(multi_label=True)\n",
        "area_under_curve = auc(y_true, y_pred)\n",
        "print(f'Macro Auc: {area_under_curve}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro Auc: 0.800000011920929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Qm3umyHxYm"
      },
      "source": [
        "## Cost functions <a name=\"5.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6FNNLIGYBTQ"
      },
      "source": [
        "### Binary cross-entropy  <a name=\"5.2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI4WXZ6XYXe9"
      },
      "source": [
        "**Binary cross-entropy** (for binary classification) and **categorical cross-entropy** (the generalization for multi-class classification) are used to **evaluate the probability outputs** of a classifier instead of its discrete predictions.\n",
        "\n",
        "For a binary classification with $\\hat{p}^{(i)}$ the probability estimated that the target variable $y^{(i)}$ is equal to 1 given the input features $\\boldsymbol{x}^{(i)}$, this is, the estimation of $P(y^{(i)}=1|X=\\boldsymbol{x}^{(i)})$. The cost function is simply the average cost over all training instances (the average of loss functions).\n",
        "\n",
        "$$J(\\boldsymbol{\\hat{\\boldsymbol{p}},\\boldsymbol{y}})=\\frac{1}{m}\\sum_{i=1}^{m}c(\\hat{p}^{(i)},y^{(i)})$$\n",
        "\n",
        "where the loss function, in this case called **logistic loss**, **log loss**, computed per sample, is the negative log-likelihood (assuming that the target variable $\\boldsymbol{y}$ follow a binomial probability distribution) of the classifier given the true label:\n",
        "\n",
        "$$c(p^{(i)},y^{(i)})=-(y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)}))$$\n",
        "\n",
        "\n",
        "or equivalently,\n",
        "\n",
        "$$\n",
        "   c(p^{(i)},y^{(i)})  = \\begin{cases}\n",
        "               -log(\\hat{p}^{(i)}) & \\text{if } y^{(i)} = 1\\\\\n",
        "               -log(1-\\hat{p}^{(i)}) & \\text{if } y^{(i)}=0\n",
        "          \\end{cases}\n",
        "$$\n",
        "\n",
        "This loss function makes sense.\n",
        "\n",
        "- $-log(t)$ grows very large when $t$ approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, and it will also be very large if the model estimates a probability close to 1 for a negative instance. \n",
        "\n",
        "- On the other hand, $-log(t)$ is close to 0 when t is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.\n",
        "\n",
        "This cost function is convex, so Gradient Descent (explained [here](https://nbviewer.jupyter.org/github/victorviro/ML_algorithms_python/blob/master/Introduction_gradient_descent_algorithm.ipynb)) or any other optimization algorithm (such as [Newton–Raphson method](https://en.wikipedia.org/wiki/Newton%27s_method)) is guaranteed to find the global minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYEozIne5q9b",
        "outputId": "94047d11-8dd9-43ed-92d0-d676c4427698"
      },
      "source": [
        "y_true = np.array([0, 1, 1, 0])\n",
        "y_pred_prob = np.array([.9, .1, .2, .65])\n",
        "\n",
        "losses = -(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1-y_pred_prob))\n",
        "print(f'Binary cross entropy loss: {np.average(losses)}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary cross entropy loss: 1.8161075557302175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDlOvxq9330z"
      },
      "source": [
        "In Scikit-learn, this cost function is implemented with the [`log_loss`](https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss) method (it can also be used for multiclass classification). In TensorFlow, this cost function is implemented with the [BinaryCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH5mDg4z3i9t",
        "outputId": "d1ed32ce-ff7c-4608-a327-780d82f2b2b7"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "print(f'Binary cross entropy loss: {log_loss(y_true, y_pred_prob)}')\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "print(f'Binary cross entropy loss: {bce(y_true, y_pred_prob).numpy()}')\n",
        " "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary cross entropy loss: 1.8161075557302175\n",
            "Binary cross entropy loss: 1.816106915473938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwpax6Xebzbf"
      },
      "source": [
        "### Categorical cross-entropy  <a name=\"5.2.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Crpf8Ib2pQ"
      },
      "source": [
        "The objective is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). Minimizing the cost function called the *cross-entropy*, should lead to this objective because it penalizes the model when it estimates a low probability for a target class. \n",
        "\n",
        "$$J(\\boldsymbol{\\hat{\\boldsymbol{p}},\\boldsymbol{y}})= -\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{k=1}^{N} y^{(i)}_klog(\\hat{p}_k^{(i)})$$\n",
        "\n",
        "- $y^{(i)}_k$ is the target probability that the $i^{\\text{th}}$ instance belongs to class $k$. In general, it is either equal to 1 or 0, depending on whether the instance belongs to the class or not.\n",
        "\n",
        "\n",
        "Notice that when there are just two classes ($N = 2$), this cost function is equivalent to the binary cross-entropy cost function. For $N>2$ we just calculate a separate loss for each class label per observation and sum the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kN_nFlYHwdY",
        "outputId": "e1d96c48-4276-4199-c78f-6f68b669dfe5"
      },
      "source": [
        "losses = (-y_true_one_hot*np.log(y_score+1e-9))\n",
        "cce = np.average(np.sum(losses, axis=-1))\n",
        "print(f'Categorical cross entropy loss: {cce}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categorical cross entropy loss: 0.6904911398887634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc60yZJt2k6Q"
      },
      "source": [
        "In Scikit-learn, this cost function is implemented with the [`log_loss`](https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss) method (the same we used for binary classification). In TensorFlow, this cost function is implemented with the [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4udbM3JGx1s",
        "outputId": "ea605542-5a96-426b-b905-b16c72de1489"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "print(f'Categorical cross entropy loss: {log_loss(y_true, y_score)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categorical cross entropy loss: 0.69049112401022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4C37Et4GStF",
        "outputId": "465bdd6e-c7d1-48e3-d8f4-858c4708910d"
      },
      "source": [
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "print(f'Categorical cross entropy loss: {cce(y_true_one_hot, y_score).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categorical cross entropy loss: 0.6904911398887634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRe02pKHl4BD"
      },
      "source": [
        "### Hamming loss  <a name=\"5.2.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTW4UeAeKXH3"
      },
      "source": [
        "In information theory, the **Hamming distance** between two strings of equal length is the number of positions at which the corresponding symbols are different. The symbols may be letters, bits, or decimal digits, among other possibilities.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Fredrick_Ishengoma/publication/264978395/figure/fig1/AS:295895569584128@1447558409105/Example-of-Hamming-Distance.png)\n",
        "\n",
        "The hamming loss is the proportion of predictions that were incorrectly classified. \n",
        "\n",
        "- For binary and multiclass classification, the hamming loss is equal to $1-\\text{accuracy}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGl-g2PYMfxn",
        "outputId": "504ba551-ade5-4f0c-fbf4-8d465721c3a7"
      },
      "source": [
        "y_true = np.array([0, 0, 2, 2, 0, 2])\n",
        "y_pred = np.array([2, 0, 2, 2, 0, 1])\n",
        "# 6 examples, 4 correctly classified\n",
        "\n",
        "hamming_loss = np.average(np.not_equal(y_pred, y_true))\n",
        "print(f'Hamming loss: {hamming_loss}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hamming loss: 0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1YV0YB1LMxQ"
      },
      "source": [
        "In Scikit-learn, this metric is implemented with the [`hamming_loss`](https://scikit-learn.org/stable/modules/model_evaluation.html#hamming-loss) method which computes the average Hamming distance between two sets of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDK-QtzDLPp5",
        "outputId": "11712a33-642f-4a46-df04-520917635791"
      },
      "source": [
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "print(f'Hamming loss: {hamming_loss(y_true, y_pred)}')\n",
        "print(f'Accuracy: {accuracy_score(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hamming loss: 0.3333333333333333\n",
            "Accuracy: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JszvH4aDM0-8"
      },
      "source": [
        "In Tensorflow Addons, this metric is implemented with the [HammingLoss](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/HammingLoss) method (can be used to interpret probabilities, and we can modify the `threshold` parameter, which is set `0.5` by default)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM84xG8mNLcQ",
        "outputId": "483afc32-86b9-4658-9b1f-10e4b257b36f"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "y_true_one_hot = tf.one_hot(y_true.tolist(), depth=3)\n",
        "y_score = np.array([[0.3, 0.1, 0.6],\n",
        "                    [0.7, 0.1, 0.2],\n",
        "                    [0.3, 0.1, 0.6],\n",
        "                    [0.1, 0.3, 0.6],\n",
        "                    [0.7, 0.3, 0.0],\n",
        "                    [0.1, 0.6, 0.3]])\n",
        "\n",
        "hamming_loss = tfa.metrics.HammingLoss(mode='multiclass')\n",
        "print(f'Hamming loss: {hamming_loss(y_true_one_hot, y_score).numpy()}')\n",
        "\n",
        "hamming_loss = tfa.metrics.HammingLoss(mode='multiclass', threshold=0.2)\n",
        "print('Hamming loss with threshold=0.2: '\n",
        "      f'{hamming_loss(y_true_one_hot, y_score).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hamming loss: 0.3333333432674408\n",
            "Hamming loss with threshold=0.2: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD2h8RYYlwaS"
      },
      "source": [
        "**Note**: As occurs with accuracy metric, when the **dataset is imbalanced**, say it has 90% of instances in one class and only 10 % in the other, predict that every instance belongs to the majority class, get an accuracy of 0.9 (hamming loss of 0.1). So, for imbalanced datasets, the hamming loss is not a good cost function. Let's see an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-kWL-gKlwae",
        "outputId": "9eedadea-e3a3-4182-ec09-abe3d04651ba"
      },
      "source": [
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "\n",
        "y_true = [1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
        "y_pred = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "error_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion matrix:\\n{error_matrix}')\n",
        "print(f'Hamming loss: {hamming_loss(y_true, y_pred)}')\n",
        "print(f'Accuracy: {accuracy_score(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[0 1]\n",
            " [0 9]]\n",
            "Hamming loss: 0.1\n",
            "Accuracy: 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NY29t61ZlYC"
      },
      "source": [
        "**Multi-label classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69XBDBzsbFTM",
        "outputId": "7f6e0803-b536-4098-8cdd-8bad88c64fe2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, hamming_loss\n",
        "\n",
        "y_true = np.array([[1, 0, 1, 0, 1, 0, 0]])\n",
        "y_pred = np.array([[0, 1, 0, 1, 0, 1, 1]])\n",
        "\n",
        "hamming = tfa.metrics.HammingLoss(mode='multilabel')\n",
        "print(f'Hamming loss: {hamming(y_true, y_pred).numpy()}')\n",
        "print(f'Hamming loss: {hamming_loss(y_true, y_pred)}')\n",
        "\n",
        "y_true = np.array([[1, 0, 1, 0, 1, 0, 0]])\n",
        "y_pred = np.array([[1, 0, 1, 0, 1, 0, 1]])\n",
        "\n",
        "hamming = tfa.metrics.HammingLoss(mode='multilabel')\n",
        "print(f'Hamming loss: {hamming(y_true, y_pred).numpy()}')\n",
        "print(f'Hamming loss: {hamming_loss(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hamming loss: 1.0\n",
            "Hamming loss: 1.0\n",
            "Hamming loss: 0.1428571492433548\n",
            "Hamming loss: 0.14285714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmPqXX87pW_H"
      },
      "source": [
        "The [HammingLoss](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/HammingLoss) method can be used to interpret probabilities, and we can modify the `threshold` parameter, which is set `0.5` by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChRl928Kcelo",
        "outputId": "f2178780-1e32-40ca-8fad-69f030d7fc13"
      },
      "source": [
        "y_true = np.array([[1, 0, 1, 0],\n",
        "                   [0, 1, 0, 1],\n",
        "                   [0, 0, 0, 1]], dtype=np.int32)\n",
        "y_pred = np.array([[0.82, 0.5, 0.90, 0],\n",
        "                   [0, 1, 0.4, 0.98],\n",
        "                   [0.89, 0.79, 0, 0.3]], dtype=np.float32)\n",
        "\n",
        "hamming_loss = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.8)\n",
        "print('Hamming loss with threshold=0.8: '\n",
        "      f'{hamming_loss(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hamming loss with threshold=0.8: 0.1666666716337204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbyyO0rfqHrm"
      },
      "source": [
        "However, the hamming loss is not usually a good cost function for multi-label classififcation tasks. Imagine we have 10 possible labels, since each example is assigned with a small set of these labels, we can get a low hamming loss when our model predicts mostly zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHdXHXR6cGXj",
        "outputId": "28317466-d5a3-4991-a43e-e28f48aefe23"
      },
      "source": [
        "y_true = np.array([\n",
        "    [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]]\n",
        ")\n",
        "y_pred = np.array([\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
        ")\n",
        "\n",
        "hamming_loss = tfa.metrics.HammingLoss(mode='multilabel')\n",
        "print(f'Hamming loss: {hamming_loss(y_true, y_pred).numpy()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hamming loss: 0.15000000596046448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQn0_ccN5GE2"
      },
      "source": [
        "# References <a name=\"6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adA8RuCj5Hpi"
      },
      "source": [
        "- [Scikit-learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "\n",
        "- [TensorFlow metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)\n",
        "\n",
        "- [Confusion matrix Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
        "\n",
        "- [Why is accuracy not the best measure for assessing classification models?](https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models)\n",
        "\n",
        "- [How to use Logcosh with Keras?](https://www.machinecurve.com/index.php/2019/10/23/how-to-use-logcosh-with-keras/)\n",
        "\n",
        "\n",
        "- [How does Keras calculate accuracy?](https://datascience.stackexchange.com/questions/14415/how-does-keras-calculate-accuracy)\n",
        "\n",
        "\n",
        "- [Performance Metrics (Error Measures) in Machine Learning Regression](https://arxiv.org/abs/1809.03006)\n",
        "\n",
        "- [Metrics for Multi-Class Classification: an Overview](https://arxiv.org/abs/2008.05756)"
      ]
    }
  ]
}